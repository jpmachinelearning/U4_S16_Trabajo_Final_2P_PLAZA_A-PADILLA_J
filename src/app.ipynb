{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlXKdNdDdsEbpnjJG9xPbn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/blob/main/src/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FvUeZvpmVC3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4df3a831-3918-48c9-aa55-1ed12dea15db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyGithub\n",
            "  Downloading pygithub-2.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.32.4)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.5.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.4.0->PyGithub) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (2026.1.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->pynacl>=1.4.0->PyGithub) (3.0)\n",
            "Downloading pygithub-2.8.1-py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m432.7/432.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynacl-1.6.2-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynacl, PyGithub\n",
            "Successfully installed PyGithub-2.8.1 pynacl-1.6.2\n",
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: es-core-news-lg\n",
            "Successfully installed es-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 1. INSTALACION DE LIBRERIAS\n",
        "# ==========================================\n",
        "\n",
        "!pip install PyGithub\n",
        "!python -m spacy download es_core_news_lg\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================\n",
        "# 2. IMPORTACI√ìN DE LIBRERIAS Y VARIABLES GLOBALES\n",
        "# ================================================\n",
        "\n",
        "import emoji\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from google.colab import files, userdata\n",
        "from github import Github, Auth, GithubException\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Descargas NLTK (ejecutar una vez)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "STOP_ES = set(stopwords.words('spanish'))\n",
        "STOP_EN = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.blank(\"es\")\n",
        "nlp2 = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "INLINE_CODE = re.compile(r'`[^`]+`')\n",
        "FENCED_CODE = re.compile(r'```(.+?)```', re.DOTALL)\n",
        "CODE_KEYWORDS = re.compile(r'\\b(def|class|import|from|return|console\\.log|printf|#include|std::|->|=>)\\b')\n",
        "BRACES_SEMICOLON = re.compile(r'[{};=<>]')\n",
        "IMG_TAG = re.compile(r'<img\\b[^>]*>', re.IGNORECASE)\n",
        "HTML_TAG = re.compile(r'<\\/?\\w+[^>]*>', re.IGNORECASE)\n",
        "URL_LARGE = re.compile(r'https?://\\S{30,}', re.IGNORECASE)\n",
        "ASSET_DOMAINS = re.compile(r'(github\\.com|githubusercontent\\.com|assets/|cdn\\.)', re.IGNORECASE)\n",
        "\n",
        "MAPEO_LEMAS = {\n",
        "  \"hi\": \"hola\",\n",
        "  \"Ademas\": \"adem√°s\",\n",
        "  \"ademas\": \"adem√°s\",\n",
        "  \"qutarir\": \"quitar\",\n",
        "  \"qutacer\": \"quitar\",\n",
        "  \"qutaria\": \"quitar\",\n",
        "  \"qutar√≠a\": \"quitar\",\n",
        "  \"a√±adiais\": \"a√±adir\",\n",
        "  \"v√°lir\": \"v√°lido\",\n",
        "  \"Gracia\": \"gracias\",\n",
        "  \"gracia\": \"gracias\",\n",
        "  \"modul\": \"m√≥dulo\",\n",
        "  \"finar\": \"final\",\n",
        "  \"somar\": \"sumar\",\n",
        "  \"deveria\": \"deber\",\n",
        "  \"dever√≠a\": \"deber\",\n",
        "  \"estaria\": \"estar\",\n",
        "  \"est√°r√≠a\": \"estar\",\n",
        "  \"configurancion\": \"configuraci√≥n\",\n",
        "  \"temrinos\": \"t√©rminos\",\n",
        "  \"desp√©s\": \"despu√©s\",\n",
        "  \"propois\": \"propios\",\n",
        "  \"v√°lir\": \"valer\",\n",
        "  \"viudedad\": \"viudedad\",\n",
        "  \"reav\": \"revisar\",\n",
        "  \"qeu\": \"que\",\n",
        "  \"pusistar\": \"pusiste\",\n",
        "  \"modificaco\": \"modificado\",\n",
        "  \"desc√°rgatar\": \"descargar\",\n",
        "  \"fuentir\": \"fuente\",\n",
        "  \"invoizar\": \"facturar\",\n",
        "  \"informacion\": \"informaci√≥n\",\n",
        "  \"subsana\": \"subsanar\",\n",
        "  \"eskema\": \"esquema\",\n",
        "  }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HANBfslKmC-J",
        "outputId": "de4a901d-34dc-4456-b875-75a1ae15d5e1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================================\n",
        "# 3. CONEXI√ìN Y CLONADO DE REPOSITORIO DE GITHUB\n",
        "# ===============================================\n",
        "\n",
        "try:\n",
        "    TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    USER_GITHUB = \"jpmachinelearning\"\n",
        "    EMAIL_GITHUB = userdata.get('MY_EMAIL')\n",
        "\n",
        "    # Configurar identidad global de Git de una vez\n",
        "    !git config --global user.email \"{EMAIL_GITHUB}\"\n",
        "    !git config --global user.name \"{USER_GITHUB}\"\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"‚ùå Error: Crea el secreto 'GITHUB_TOKEN' en el panel lateral.\")\n",
        "\n",
        "# --- RUTAS Y NOMBRES ---\n",
        "DEST_REPO_NAME = 'U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J'\n",
        "REPO_URL = f\"https://{TOKEN}@github.com/{USER_GITHUB}/{DEST_REPO_NAME}.git\"\n",
        "ruta_repositorio_local = f\"/content/{DEST_REPO_NAME}\"\n",
        "ruta_local_data = os.path.join(ruta_repositorio_local, 'data')\n",
        "\n",
        "\n",
        "# --- FUNCI√ìN DE UTILIDAD PARA CLONADO/SINCRO ---\n",
        "def inicializar_repositorio():\n",
        "    %cd /content/\n",
        "    if not os.path.exists(DEST_REPO_NAME):\n",
        "        print(f\"üöÄ Clonando {DEST_REPO_NAME}...\")\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        print(f\"üîÑ Sincronizando repositorio...\")\n",
        "        %cd {DEST_REPO_NAME}\n",
        "        !git pull origin main\n",
        "        %cd /content/\n",
        "    os.makedirs(os.path.dirname(ruta_local_data), exist_ok=True)\n",
        "    print(f\"üìÇ Carpeta de datos lista en: {os.path.dirname(ruta_local_data)}\")\n",
        "\n",
        "# Inicializar carpeta si no existe\n",
        "inicializar_repositorio()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbdFWfFBYb8P",
        "outputId": "3258549e-211e-4360-9368-f0b50e9d0ee5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "üöÄ Clonando U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J...\n",
            "Cloning into 'U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J'...\n",
            "remote: Enumerating objects: 4073, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 4073 (delta 17), reused 25 (delta 12), pack-reused 4034 (from 2)\u001b[K\n",
            "Receiving objects: 100% (4073/4073), 46.15 MiB | 18.96 MiB/s, done.\n",
            "Resolving deltas: 100% (2287/2287), done.\n",
            "üìÇ Carpeta de datos lista en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 4. FUNCIONES\n",
        "# ==========================================\n",
        "\n",
        "### ---  Funciones de para prepeocesado ---\n",
        "\n",
        "def ensure_text_input(corpus):\n",
        "    if isinstance(corpus, str):\n",
        "        return corpus\n",
        "    # pandas Series o DataFrame cell\n",
        "    if pd is not None and isinstance(corpus, pd.Series):\n",
        "        # eliminar NaN y convertir a str\n",
        "        parts = [str(x) for x in corpus.dropna().astype(str).tolist()]\n",
        "        return \"\\n\\n\".join(parts)\n",
        "    # lista/tupla de strings\n",
        "    if isinstance(corpus, (list, tuple)):\n",
        "        parts = [str(x) for x in corpus if x is not None]\n",
        "        return \"\\n\\n\".join(parts)\n",
        "    # fallback: intentar str()\n",
        "    return str(corpus)\n",
        "\n",
        "\n",
        "def apply_mapeo_and_simple_grammar(text, corrections=None):\n",
        "    \"\"\"\n",
        "    Aplica MAPEO_LEMAS sobre tokens y corrige patrones gramaticales puntuales.\n",
        "    Si se pasa 'corrections' (lista), a√±ade tuplas (original, reemplazo).\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text\n",
        "    # Normalizar tokens simples usando MAPEO_LEMAS (mantener espacios)\n",
        "    def replace_token_match(m):\n",
        "        tok_orig = m.group(0)\n",
        "        tok = tok_orig.lower()\n",
        "        tok_repl = MAPEO_LEMAS.get(tok, tok)\n",
        "        # registrar si hubo cambio real y se pas√≥ lista\n",
        "        if corrections is not None and tok_repl != tok:\n",
        "            corrections.append((tok_orig, tok_repl))\n",
        "        return tok_repl\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in MAPEO_LEMAS.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    text = pattern.sub(lambda m: replace_token_match(m), text)\n",
        "    # Reglas gramaticales puntuales\n",
        "    def replace_ningun_excepcion(m):\n",
        "        orig = m.group(0)\n",
        "        repl = 'ninguna excepci√≥n'\n",
        "        if corrections is not None:\n",
        "            corrections.append((orig, repl))\n",
        "        return repl\n",
        "    text = re.sub(r'\\bning√∫n\\s+excepci√≥n\\b', replace_ningun_excepcion, text, flags=re.IGNORECASE)\n",
        "    # corregir dobles espacios\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# --- Sustituciones finales robustas: URLs -> 'url' y n√∫meros -> 'num' ---\n",
        "def apply_placeholders_preserve_punct(text: str, replacements=None) -> str:\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text\n",
        "    # URL pattern (prioriza detecci√≥n de URL completa)\n",
        "    url_pattern = re.compile(r'(?P<prefix>[\\(\\[\\{<\"\\']?)'r'(?P<url>(?:https?[:/]{0,3}|//|www\\.)'r'[A-Za-z0-9\\-]+'r'(?:\\.[A-Za-z0-9\\-]+)+'r'(?:[\\/?#][^\\s\\)\\]\\}\\>,.;:!?\"\\']*)?)'r'(?P<suffix>[\\)\\]\\}>,.;:!?\"\\']*)',flags=re.IGNORECASE)\n",
        "    def _url_sub(m):\n",
        "        orig = m.group('url')\n",
        "        if replacements is not None:\n",
        "            replacements.append((orig, 'url'))\n",
        "        return f\"{m.group('prefix')} {m.group('suffix')}\"\n",
        "    text = url_pattern.sub(_url_sub, text)\n",
        "    # N√∫mero puro: preservar prefijo/sufijo de puntuaci√≥n\n",
        "    number_pattern = re.compile(r'(?<![A-Za-z0-9])'r'(?P<prefix>[\\(\\[\\{<\"\\']?)'r'(?P<num>\\d+)'r'(?P<suffix>[\\)\\]\\}>,.;:!?\"\\']?)'r'(?![A-Za-z0-9])')\n",
        "    def _num_sub(m):\n",
        "        orig = m.group('num')\n",
        "        if replacements is not None:\n",
        "            replacements.append((orig, 'num'))\n",
        "        return f\"{m.group('prefix')} {m.group('suffix')}\"\n",
        "\n",
        "\n",
        "    text = number_pattern.sub(_num_sub, text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocesado(doc):\n",
        "  # Pipeline del prepocesado: normalizaci√≥n -> traducci√≥n de emoji -> clasificador (doble revisi√≥n 'und') / eliminaci√≥n segmentos 'en', 'und' y 'code' ->\n",
        "  # elimanci√≥n de typos -> cambio de numeros por placeholder 'num' y sitio web por 'url' ->\n",
        "  # eliminaci√≥n signos de puntuaci√≥n -> tokenizaci√≥n -> eliminaci√≥n de stopwords -> lematizaci√≥n\n",
        "\n",
        "\n",
        "  ### Normalizaci√≥n\n",
        "  #Normalizar entrada a texto (maneja None, NaN, Series, listas, etc.)\n",
        "  doc = ensure_text_input(doc) # Si queda vac√≠o, devolver resultado vac√≠o y log m√≠nimo\n",
        "  if not doc or str(doc).strip() == \"\":\n",
        "    return \"\", [[\"input_empty\"]]\n",
        "  doc = doc.lower()\n",
        "\n",
        "\n",
        "  ### Traducci√≥n de emoji\n",
        "  #doc = emoji.demojize(doc, language='es').replace(':', '').replace('_', ' ')\n",
        "\n",
        "  # Comprobamos si existen emojis en el documento mediante un condicional\n",
        "  if emoji.emoji_count(doc) > 0:\n",
        "\n",
        "      # Definimos una funci√≥n interna (callback) que solo procesa el emoji hallado\n",
        "      def procesar_emoji_individual(chars, data_dict):\n",
        "          # Traducimos el emoji a su nombre en espa√±ol (ej: :cara_sonriente:)\n",
        "          nombre_emoji = emoji.demojize(chars, language='es')\n",
        "\n",
        "          # Limpiamos los signos SOLO del nombre generado por demojize.\n",
        "          # Esto permite que \"cara_sonriente\" pase a ser \"cara sonriente\"\n",
        "          # para que el an√°lisis de sentimiento/TF-IDF lo entienda mejor,\n",
        "          # pero sin afectar a las URLs del resto del documento.\n",
        "          nombre_limpio = nombre_emoji.replace(':', '').replace('_', ' ')\n",
        "\n",
        "          # Devolvemos el texto con espacios para que no se pegue a otras palabras\n",
        "          return f\" {nombre_limpio} \"\n",
        "\n",
        "      # Aplicamos el reemplazo selectivo\n",
        "      doc = emoji.replace_emoji(doc, replace=procesar_emoji_individual)\n",
        "\n",
        "  ### Elimanci√≥n de typos\n",
        "  ortho_corrections = []\n",
        "  doc = apply_mapeo_and_simple_grammar(doc, corrections=ortho_corrections)\n",
        "\n",
        "\n",
        "  ### Clasificador (doble revision 'und') / eliminacion segmentos 'en', 'und' y 'code'\n",
        "  def looks_like_code_line(line):\n",
        "      if INLINE_CODE.search(line):\n",
        "          return True\n",
        "      if FENCED_CODE.search(line):\n",
        "          return True\n",
        "      if IMG_TAG.search(line) or HTML_TAG.search(line):\n",
        "          return True\n",
        "      if URL_LARGE.search(line) and ASSET_DOMAINS.search(line):\n",
        "          return True\n",
        "      if CODE_KEYWORDS.search(line):\n",
        "          return True\n",
        "      non_alpha = sum(1 for ch in line if not ch.isalpha() and not ch.isspace())\n",
        "      if len(line) > 0 and (non_alpha / len(line)) > 0.25:\n",
        "          return True\n",
        "      if BRACES_SEMICOLON.search(line):\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "  def split_by_blank_and_guess(segment):\n",
        "      parts = []\n",
        "      groups = re.split(r'\\n\\s*\\n', segment)\n",
        "      for g in groups:\n",
        "          lines = [ln for ln in g.splitlines() if ln.strip() != \"\"]\n",
        "          if not lines:\n",
        "              continue\n",
        "          # FORZAR code si hay etiquetas img/html o URLs de assets en las l√≠neas\n",
        "          # (requiere que IMG_TAG, HTML_TAG, URL_LARGE, ASSET_DOMAINS est√©n definidos globalmente)\n",
        "          if any(IMG_TAG.search(ln) or (URL_LARGE.search(ln) and ASSET_DOMAINS.search(ln)) or HTML_TAG.search(ln) for ln in lines):\n",
        "              parts.append(('code', \"\\n\".join(lines)))\n",
        "              continue\n",
        "          code_like_count = sum(1 for ln in lines if looks_like_code_line(ln) or ln.startswith('    ') or ln.startswith('\\t'))\n",
        "          prop = code_like_count / len(lines)\n",
        "          if prop >= 0.4:\n",
        "              code_lines = []\n",
        "              text_lines = []\n",
        "              for ln in lines:\n",
        "                  tokens = [t.lower() for t in word_tokenize(ln) if t.isalpha()]\n",
        "                  if len(tokens) >= 2:\n",
        "                      es_count = sum(1 for t in tokens if t in STOP_ES)\n",
        "                      if es_count / len(tokens) >= 0.25 or re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', ln):\n",
        "                          text_lines.append(ln)\n",
        "                          continue\n",
        "                  code_lines.append(ln)\n",
        "              if code_lines:\n",
        "                  parts.append(('code', \"\\n\".join(code_lines)))\n",
        "              if text_lines:\n",
        "                  parts.append(('text', \"\\n\".join(text_lines)))\n",
        "          else:\n",
        "              parts.append(('text', \"\\n\".join(lines)))\n",
        "      return parts\n",
        "\n",
        "  def split_into_blocks_by_indentation_and_fences(corpus):\n",
        "      \"\"\"\n",
        "      Acepta corpus de cualquier tipo; primero normaliza a string.\n",
        "      Extrae fenced code y agrupa el resto por bloques separados por l√≠nea en blanco.\n",
        "      \"\"\"\n",
        "      text = ensure_text_input(corpus)\n",
        "      # --- NORMALIZACI√ìN ADICIONAL: convertir separadores en \"punto y aparte\" ---\n",
        "      # [sep] o [ sep] -> punto y salto de l√≠nea doble\n",
        "      text = re.sub(r'\\[\\s*sep\\s*\\]', '.\\n\\n', text, flags=re.IGNORECASE)\n",
        "      # punto seguido (\". \") -> punto y salto de l√≠nea doble\n",
        "      text = re.sub(r'\\.\\s+', '.\\n\\n', text)\n",
        "      # ------------------------------------------------------------------------\n",
        "      blocks = []\n",
        "      cursor = 0\n",
        "      for m in FENCED_CODE.finditer(text):\n",
        "          pre = text[cursor:m.start()]\n",
        "          if pre.strip():\n",
        "              blocks.extend(split_by_blank_and_guess(pre))\n",
        "          code_block = m.group(0)\n",
        "          blocks.append(('code', code_block))\n",
        "          cursor = m.end()\n",
        "      tail = text[cursor:]\n",
        "      if tail.strip():\n",
        "          blocks.extend(split_by_blank_and_guess(tail))\n",
        "      return blocks\n",
        "\n",
        "  def detect_language_by_stopwords(text, min_tokens=1):\n",
        "      \"\"\"\n",
        "      Detecta 'es', 'en', 'bilingual' o 'und'.\n",
        "      Ajustes:\n",
        "        - tokenizaci√≥n robusta (quita prefijos/sufijos no alfab√©ticos)\n",
        "        - lista blanca ampliada (incluye 'finde')\n",
        "        - heur√≠stica para frases cortas sin stopwords\n",
        "        - filtra tokens de longitud 1 al contar stopwords\n",
        "        - prioriza 'es' cuando la se√±al espa√±ola es claramente mayor\n",
        "        - condici√≥n 'bilingual' m√°s estricta\n",
        "      \"\"\"\n",
        "      txt_stripped = text.strip()\n",
        "      # caso puntual muy corto (ej. \"Gracias\" o \"Gracias.\")\n",
        "      if txt_stripped.lower() in {'gracias', 'gracias.'}:\n",
        "          return 'es'\n",
        "      # tokenizaci√≥n robusta: limpiar prefijos/sufijos no alfab√©ticos\n",
        "      raw_tokens = word_tokenize(text)\n",
        "      tokens = []\n",
        "      for t in raw_tokens:\n",
        "          t_clean = re.sub(r'^[^A-Za-z√Å√â√ç√ì√ö√ë√°√©√≠√≥√∫√±]+|[^A-Za-z√Å√â√ç√ì√ö√ë√°√©√≠√≥√∫√±]+$', '', t)\n",
        "          if t_clean:\n",
        "              tokens.append(t_clean.lower())\n",
        "      # lista blanca para singletons y sufijos t√≠picos\n",
        "      SPANISH_SINGLETONS = {'gracias','hola','adjunto','captura','saludos','ok','listo','si','no','vale','finde'}\n",
        "      SPANISH_SUFFIXES = ('ci√≥n','dad','mente','ado','ada','ico','ica','oso','osa')\n",
        "      # single token common spanish\n",
        "      if len(tokens) == 1:\n",
        "          single = tokens[0]\n",
        "          if single in SPANISH_SINGLETONS or re.search(r'[√°√©√≠√≥√∫√±]', single):\n",
        "              return 'es'\n",
        "      # si hay pocos tokens, aplicar heur√≠sticas adicionales (fallback temprano)\n",
        "      if len(tokens) < min_tokens:\n",
        "          if re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', text):\n",
        "              return 'es'\n",
        "          low = text.lower()\n",
        "          for kw in ('que','para','con','por','como','ejemplo','funci√≥n','usar','gracias','alguna','novedad','adjunto','captura','finde'):\n",
        "              if kw in low:\n",
        "                  return 'es'\n",
        "          return 'und'\n",
        "      # filtrar tokens muy cortos para el conteo (evita ruido de 'a', 'I', etc.)\n",
        "      tokens_for_count = [t for t in tokens if len(t) > 1]\n",
        "      if not tokens_for_count:\n",
        "          tokens_for_count = tokens\n",
        "      # conteo de stopwords usando tokens filtrados\n",
        "      es_count = sum(1 for t in tokens_for_count if t in STOP_ES)\n",
        "      en_count = sum(1 for t in tokens_for_count if t in STOP_EN)\n",
        "      es_ratio = es_count / len(tokens_for_count) if tokens_for_count else 0\n",
        "      en_ratio = en_count / len(tokens_for_count) if tokens_for_count else 0\n",
        "      # --- NUEVA REGLA PUNTUAL: preferir 'en' si hay URL/asset y se√±al m√≠nima de ingl√©s ---\n",
        "      if (ASSET_DOMAINS.search(text) or re.search(r'https?://', text)) and en_ratio >= 0.05:\n",
        "          return 'en'\n",
        "      # ------------------------------------------------------------------------------\n",
        "      # heur√≠stica para frases cortas sin stopwords (casos como \"Adjunto captura.\")\n",
        "      if len(tokens) >= 2 and es_count == 0 and en_count == 0:\n",
        "          # aceptar si hay acento en alguno, sufijo t√≠pico espa√±ol o palabra en whitelist\n",
        "          if any(re.search(r'[√°√©√≠√≥√∫√±]', t) for t in tokens):\n",
        "              return 'es'\n",
        "          if any(t in SPANISH_SINGLETONS for t in tokens):\n",
        "              return 'es'\n",
        "          if any(t.endswith(SPANISH_SUFFIXES) for t in tokens):\n",
        "              return 'es'\n",
        "          # si no hay evidencia clara de ingl√©s, asumir espa√±ol de forma conservadora\n",
        "          return 'es'\n",
        "      # Priorizar espa√±ol o ingl√©s si la se√±al es claramente mayor\n",
        "      if es_ratio > en_ratio and es_ratio >= 0.12:\n",
        "          return 'es'\n",
        "      if en_ratio > es_ratio and en_ratio >= 0.12:\n",
        "          return 'en'\n",
        "      # marcar bilingual solo si hay evidencia s√≥lida en ambos idiomas\n",
        "      if len(tokens_for_count) >= 3 and es_ratio >= 0.12 and en_ratio >= 0.12:\n",
        "          return 'bilingual'\n",
        "      # decisi√≥n por proporci√≥n de stopwords (criterio secundario)\n",
        "      if es_ratio >= 0.12 and es_ratio > en_ratio:\n",
        "          return 'es'\n",
        "      if en_ratio >= 0.12 and en_ratio > es_ratio:\n",
        "          return 'en'\n",
        "      # fallback final por acentos\n",
        "      if re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', text):\n",
        "          return 'es'\n",
        "      return 'und'\n",
        "\n",
        "  def classify_and_extract_strict(corpus, recursividad, window=3):\n",
        "      \"\"\"\n",
        "      Versi√≥n ajustada:\n",
        "        - Imprime la salida (category, spanish_with_code, code_blocks, items, details)\n",
        "          antes de devolver el resultado.\n",
        "        - Reconstruye (recomposici√≥n) un nuevo texto que solo incluye secciones\n",
        "          consideradas espa√±olas (lang == 'es' o 'bilingual').\n",
        "        - Para elementos eliminados (lang in {'und','code','en'} o type == 'code'),\n",
        "          imprime \"Dato borrado:\" seguido del item.\n",
        "      \"\"\"\n",
        "      log = []\n",
        "      text = ensure_text_input(corpus)\n",
        "      blocks = split_into_blocks_by_indentation_and_fences(text)\n",
        "      items = []\n",
        "      for typ, content in blocks:\n",
        "          if typ == 'code':\n",
        "              m = FENCED_CODE.search(content)\n",
        "              inner = m.group(1) if m else content\n",
        "              items.append({'type': 'code', 'text': inner.strip(), 'lang': 'code'})\n",
        "          else:\n",
        "              doc = nlp(content)\n",
        "              for sent in doc.sents:\n",
        "                  s = sent.text.strip()\n",
        "                  if not s:\n",
        "                      continue\n",
        "                  lang = detect_language_by_stopwords(s)\n",
        "                  items.append({'type': 'text', 'text': s, 'lang': lang})\n",
        "      total = len(items)\n",
        "      code_count = sum(1 for it in items if it['type'] == 'code')\n",
        "      es_count = sum(1 for it in items if it['lang'] == 'es')\n",
        "      en_count = sum(1 for it in items if it['lang'] == 'en')\n",
        "      if total == 0:\n",
        "          category = \"texto ingles 100%\"\n",
        "      else:\n",
        "          prop_code = code_count / total\n",
        "          prop_es = es_count / total\n",
        "          prop_en = en_count / total\n",
        "          if prop_code >= 0.95:\n",
        "              category = \"texto codigo de programacion 100%\"\n",
        "          elif prop_es >= 0.95 and code_count == 0:\n",
        "              category = \"texto espa√±ol 100%\"\n",
        "          elif prop_en >= 0.95 and code_count == 0:\n",
        "              category = \"texto ingles 100%\"\n",
        "          elif prop_es > 0.5 and code_count > 0:\n",
        "              category = \"texto espa√±ol + codigo de programacion\"\n",
        "          else:\n",
        "              category = \"texto bilingue\"\n",
        "      spanish_related = []\n",
        "      for i, it in enumerate(items):\n",
        "          if it['type'] == 'code':\n",
        "              start = max(0, i - window)\n",
        "              end = min(len(items) - 1, i + window)\n",
        "              for j in range(start, end + 1):\n",
        "                  cand = items[j]\n",
        "                  if cand['type'] == 'text' and cand['lang'] == 'es':\n",
        "                      spanish_related.append(cand['text'])\n",
        "      if not spanish_related:\n",
        "          tech_keywords = {'funci√≥n','variable','clase','m√©todo','archivo','ejemplo','par√°metro','argumento','instalar','importar','usar'}\n",
        "          for it in items:\n",
        "              if it['type'] == 'text' and it['lang'] == 'es':\n",
        "                  tokens = set(t.lower() for t in word_tokenize(it['text']) if t.isalpha())\n",
        "                  if tokens & tech_keywords or re.search(r'[√°√©√≠√≥√∫√±]', it['text']):\n",
        "                      spanish_related.append(it['text'])\n",
        "      seen = set()\n",
        "      spanish_with_code = []\n",
        "      for s in spanish_related:\n",
        "          if s not in seen:\n",
        "              seen.add(s)\n",
        "              spanish_with_code.append(s)\n",
        "      code_blocks = [it['text'] for it in items if it['type'] == 'code']\n",
        "      for it in items:\n",
        "          log.append(f\"{it['type']} | {it['lang']} | {it['text']}\")\n",
        "      log.append(f\"Detalles: total_items: {total} | code_items: {code_count} | spanish_items: {es_count} | english_items: {en_count}\")\n",
        "      # --- Recomposici√≥n del texto original manteniendo solo espa√±ol ---\n",
        "      # Incluir items cuya etiqueta de idioma sea 'es' o 'bilingual' y que no sean c√≥digo.\n",
        "      recomposed_parts = []\n",
        "      for it in items:\n",
        "        # condici√≥n para conservar: tipo text y lang es o bilingual\n",
        "        if recursividad:\n",
        "          if it['type'] == 'text' and it['lang'] in {'es', 'bilingual'}:\n",
        "            recomposed_parts.append(it['text'])\n",
        "          else:\n",
        "            log.append(f\"Dato borrado: {it['type']} | {it['lang']} | {it['text']}\")\n",
        "        elif it['type'] == 'text' and it['lang'] in {'es', 'bilingual', 'und'}:\n",
        "          recomposed_parts.append(it['text'])\n",
        "        else:\n",
        "          # imprimir mensaje de borrado para los elementos no conservados\n",
        "          log.append(f\"Dato borrado: {it['type']} | {it['lang']} | {it['text']}\")\n",
        "      # unir con un espacio entre oraciones/secciones\n",
        "      recomposed_text = \" \".join(p.strip() for p in recomposed_parts if p.strip())\n",
        "      log.append(f\"recomposed_text: {recomposed_text}\")\n",
        "      # devolver la recomposici√≥n y el log\n",
        "      return recomposed_text, log\n",
        "  logs_for_flat =[]\n",
        "  res1, log1 = classify_and_extract_strict(doc, recursividad=False)\n",
        "  logs_for_flat.append(log1)\n",
        "  res2, log2 = classify_and_extract_strict(res1, recursividad=True)\n",
        "  res2 = (res2 or \"\").strip()\n",
        "  # normalizar res2 (recomposed_text)\n",
        "  if not res2:\n",
        "    res2 = \"\"\n",
        "\n",
        "\n",
        "  ### Cambio de numeros por placeholder 'num' y sitio web por 'url' (desactivado)\n",
        "  placeholder_changes = []\n",
        "  res2 = apply_placeholders_preserve_punct(res2, replacements=placeholder_changes)\n",
        "\n",
        "\n",
        "  if log2:\n",
        "    logs_for_flat.append(\"__Recursividad__\")\n",
        "  logs_for_flat.append(log2)\n",
        "  # aplanar (flatten) y asegurar que cada elemento sea string y sin saltos iniciales/finales\n",
        "  logs = []\n",
        "  for part in logs_for_flat:\n",
        "      if isinstance(part, (list, tuple)):\n",
        "          for line in part:\n",
        "              logs.append(str(line).strip())\n",
        "      else:\n",
        "          logs.append(str(part).strip())\n",
        "  # eliminar entradas vac√≠as si las hubiera\n",
        "  logs = [l for l in logs if l]\n",
        "  # --- Escribir logs, cambios placeholders y ortogr√°ficos en archivo log_file.txt ---\n",
        "  timestamp = datetime.datetime.utcnow().isoformat()\n",
        "  try:\n",
        "      ruta_log_file_txt = os.path.join(ruta_local_data, 'log_file.txt')\n",
        "      with open(ruta_log_file_txt, 'w', encoding='utf-8') as lf:\n",
        "          # Si hay logs generales ya calculados (variable logs existente), escribirlos\n",
        "          if logs:\n",
        "              lf.write(f\"# Entrada procesada {timestamp} UTC\\n\")\n",
        "              for line in logs:\n",
        "                  lf.write(line + \"\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "          # Correcciones ortogr√°ficas (si las hay)\n",
        "          if ortho_corrections:\n",
        "              lf.write(f\"__Corrector Ortogr√°fico__{timestamp}\\n\")\n",
        "              for orig, repl in ortho_corrections:\n",
        "                  lf.write(f\"'{orig}' se corrige por '{repl}'\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "          # Cambios por placeholders (si los hay)\n",
        "          if placeholder_changes:\n",
        "              lf.write(f\"__Cambio por Placeholder__{timestamp}\\n\")\n",
        "              for orig, repl in placeholder_changes:\n",
        "                  lf.write(f\"'{orig}' se cambio por '{repl}'\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "  except Exception:\n",
        "      # No interrumpir el pipeline por errores de logging\n",
        "      pass\n",
        "\n",
        "  def limpieza_selectiva_tecnica(doc):\n",
        "      \"\"\"\n",
        "      Limpia signos de puntuaci√≥n respetando:\n",
        "      - URLs completas (https://www.aeat.es)\n",
        "      - Versiones (16.0, 17.0.1)\n",
        "      - Guiones entre texto/n√∫meros (l10n-es, factura-e, v18-beta)\n",
        "      \"\"\"\n",
        "      # Normalizaci√≥n de espacios y saltos de l√≠nea\n",
        "      doc = doc.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "      # PROTEGER URLs\n",
        "      urls = re.findall(r'https?://\\S+', doc)\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(url, f' TOKEN_URL_{i} ')\n",
        "      # PROTEGER VERSIONES (Puntos entre n√∫meros)\n",
        "      versiones = re.findall(r'\\d+\\.\\d+(?:\\.\\d+)*', doc)\n",
        "      for i, ver in enumerate(versiones):\n",
        "          doc = doc.replace(ver, f' TOKEN_VER_{i} ')\n",
        "      # LIMPIEZA DE SIGNOS (Excepto el guion por ahora)\n",
        "      # Borramos: ¬° ! ¬ø ? [ ] ( ) { } | # * , ; : \" ' ¬ª ¬´ @\n",
        "      signos_a_borrar = r'[¬°!¬ø?\\[\\](){}|#*,\\;:\\\"\\'¬ª¬´@]'\n",
        "      doc = re.sub(signos_a_borrar, ' ', doc)\n",
        "      # TRATAMIENTO DE PUNTOS Y GUIONES RESTANTES\n",
        "      # Borrar puntos que no protegimos (puntos al final de frase)\n",
        "      doc = re.sub(r'\\.', ' ', doc)\n",
        "      # Borrar guiones SOLO si NO est√°n entre letras o n√∫meros\n",
        "      # (Borra guiones decorativos pero mantiene l10n-es o factura-e)\n",
        "      doc = re.sub(r'(?<![a-zA-Z0-9])-|-(?![a-zA-Z0-9])', ' ', doc)\n",
        "      # RESTAURAR URLs Y VERSIONES\n",
        "      for i, ver in enumerate(versiones):\n",
        "          doc = doc.replace(f' TOKEN_VER_{i} ', f' {ver} ')\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(f' TOKEN_URL_{i} ', f' {url} ')\n",
        "      # COLAPSAR ESPACIOS\n",
        "      doc = re.sub(r'\\s+', ' ', doc).strip()\n",
        "      return doc\n",
        "\n",
        "\n",
        "  ### Eliminaci√≥n signos de puntuaci√≥n\n",
        "  def limpieza_selectiva_tecnica(doc):\n",
        "      \"\"\"\n",
        "      Mantiene puntos en todo el texto (para extensiones y URLs).\n",
        "      Mantiene dos puntos (:) SOLO en URLs.\n",
        "      Elimina el resto de s√≠mbolos t√©cnicos innecesarios.\n",
        "      \"\"\"\n",
        "      if not doc:\n",
        "          return \"\"\n",
        "      # Normalizaci√≥n de espacios y reparaci√≥n de protocolo\n",
        "      doc = doc.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "      # Reparar URLs mal escritas (ej: https// -> https://)\n",
        "      doc = re.sub(r'(https?)(//)', r'\\1:\\2', doc)\n",
        "      # PROTEGER URLs (Capa de Seguridad M√°xima)\n",
        "      # Esto captura la URL completa con sus : / y .\n",
        "      urls = re.findall(r'https?://\\S+', doc)\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(url, f' TOKEN_URL_{i} ')\n",
        "      # PROTEGER N√öMEROS T√âCNICOS (Versiones y porcentajes)\n",
        "      # Ej: 16.0, 1.0.1, 10%\n",
        "      nums_tecnicos = re.findall(r'\\d+[.,]\\d+(?:\\.\\d+)*%?', doc)\n",
        "      for i, nt in enumerate(nums_tecnicos):\n",
        "          doc = doc.replace(nt, f' TOKEN_NUM_{i} ')\n",
        "      # LIMPIEZA SELECTIVA DE S√çMBOLOS\n",
        "      # Eliminamos los dos puntos (:) y la barra (/) aqu√≠ porque las URLs ya est√°n protegidas.\n",
        "      # MANTENEMOS el punto (.) por instrucci√≥n directa.\n",
        "      signos_a_borrar = r'[:/=<>\\\\[\\]{}()|#@*+;\\\"\\'¬´¬ª`‚Ä¢]'\n",
        "      doc = re.sub(signos_a_borrar, ' ', doc)\n",
        "      # TRATAMIENTO DE CONECTORES (Guion y Guion Bajo)\n",
        "      # Se mantienen solo si est√°n entre letras o n√∫meros (ej: l10n_es, factura-e)\n",
        "      # Se borran si son decorativos o est√°n sueltos.\n",
        "      doc = re.sub(r'(?<![a-zA-Z0-9])[-_]|[-_](?![a-zA-Z0-9])', ' ', doc)\n",
        "      # RESTAURACI√ìN\n",
        "      # Devolvemos las URLs y N√∫meros a su sitio original intactos\n",
        "      for i, nt in enumerate(nums_tecnicos):\n",
        "          doc = doc.replace(f' TOKEN_NUM_{i} ', f' {nt} ')\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(f' TOKEN_URL_{i} ', f' {url} ')\n",
        "      # COLAPSAR ESPACIOS\n",
        "      doc = re.sub(r'\\s+', ' ', doc).strip()\n",
        "      return doc\n",
        "\n",
        "  # Llamado de funci√≥n para quitar selectivamente signos de puntuaci√≥n\n",
        "  doc = limpieza_selectiva_tecnica(res2)\n",
        "\n",
        "\n",
        "  ### Tokenizaci√≥n\n",
        "  doc = nlp2(doc)\n",
        "\n",
        "\n",
        "  ### Eliminaci√≥n de stopwords\n",
        "  doc = [token for token in doc if not token.is_stop and not token.is_punct and not token.is_space] #[token for token in doc if not token.is_stop]\n",
        "\n",
        "\n",
        "  ### Lematizaci√≥n\n",
        "  doc = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "  return doc\n",
        "\n",
        "\n",
        "### --- FUNCI√ìN GLOBAL DE CARGA ---\n",
        "def sincronizar_con_github(repo_path, lista_archivos, mensaje_commit):\n",
        "    try:\n",
        "        if not os.path.exists(repo_path):\n",
        "            raise Exception(f\"La ruta del repositorio {repo_path} no existe.\")\n",
        "        %cd {repo_path}\n",
        "\n",
        "        # 1. Preparar archivos (Staging)\n",
        "        archivos_a√±adidos = 0\n",
        "        for archivo in lista_archivos:\n",
        "            # Verificamos que el archivo exista antes de intentar a√±adirlo\n",
        "            if os.path.exists(archivo):\n",
        "                !git add {archivo}\n",
        "                archivos_a√±adidos += 1\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Advertencia: No se encontr√≥ el archivo {archivo}\")\n",
        "\n",
        "        if archivos_a√±adidos == 0:\n",
        "            print(\"‚ùå No se encontraron archivos v√°lidos para subir.\")\n",
        "            return\n",
        "\n",
        "        # Commit con marca de tiempo autom√°tica si no se provee mensaje detallado\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "        full_message = f\"{mensaje_commit} ({timestamp})\"\n",
        "\n",
        "        # El comando commit devuelve error si no hay cambios, usamos try interno\n",
        "        !git commit -m \"{full_message}\"\n",
        "\n",
        "        # Pull y Push\n",
        "        print(f\"üöÄ Enviando {archivos_a√±adidos} archivo(s) a GitHub...\")\n",
        "        !git pull --rebase -X ours origin main\n",
        "        !git push origin main\n",
        "        print(f\"‚úÖ Sincronizaci√≥n exitosa.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la sincronizaci√≥n: {e}\")\n",
        "    finally:\n",
        "        %cd /content/\n"
      ],
      "metadata": {
        "id": "7e2s7yqa_4kc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. EXTRACCI√ìN Y SUBIDA DE DATASET\n",
        "# ==========================================\n",
        "\n",
        "REPO_NAME_EXTRACCION = 'OCA/l10n-spain'\n",
        "LIMITE_REGISTROS = 200\n",
        "datos_github_csv = 'datos_github.csv'\n",
        "ruta_datos_github_csv = os.path.join(ruta_local_data, datos_github_csv)\n",
        "\n",
        "if os.path.exists(ruta_datos_github_csv):\n",
        "    print(f\"‚úÖ El archivo '{datos_github_csv}' ya existe en el repositorio local.\")\n",
        "    print(\"üöÄ Se saltar√° la descarga para ahorrar recursos y tiempo.\")\n",
        "else:\n",
        "    try:\n",
        "        auth = Auth.Token(TOKEN)\n",
        "        g = Github(auth=auth)\n",
        "        repo = g.get_repo(REPO_NAME_EXTRACCION)\n",
        "        dataset = []\n",
        "\n",
        "        # --- EXTRACCI√ìN DE ISSUES (RF-01) ---\n",
        "\n",
        "        print(f\"\\n--- Extrayendo hasta {LIMITE_REGISTROS} Issues ---\")\n",
        "        issues = repo.get_issues(state='all', sort='created', direction='desc')\n",
        "        count = 0\n",
        "\n",
        "        for item in issues:\n",
        "            if count >= LIMITE_REGISTROS: break\n",
        "            if item.pull_request is not None: continue\n",
        "\n",
        "            print(f\"üì• Procesando Issue #{item.number}...\")\n",
        "\n",
        "            # Recolectar t√≠tulo, descripci√≥n y comentarios\n",
        "            titulo = item.title or \"\"\n",
        "            descripcion = item.body or \"\"\n",
        "            comments = [c.body for c in item.get_comments() if c.body]\n",
        "\n",
        "            # L√≥gica modificada: Una fila por cada comentario\n",
        "            if comments:\n",
        "                for comm in comments:\n",
        "                    dataset.append({\n",
        "                        'tipo': 'Issue',\n",
        "                        'numero': item.number,\n",
        "                        'titulo': titulo,\n",
        "                        'descripcion': descripcion,\n",
        "                        'comentarios': comm  # Comentario individual\n",
        "                    })\n",
        "            else:\n",
        "                # Si no hay comentarios, guardamos el Issue de todas formas\n",
        "                dataset.append({\n",
        "                    'tipo': 'Issue',\n",
        "                    'numero': item.number,\n",
        "                    'titulo': titulo,\n",
        "                    'descripcion': descripcion,\n",
        "                    'comentarios': \"\"\n",
        "                })\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # --- EXTRACCI√ìN DE PULL REQUESTS (RF-01) ---\n",
        "        print(f\"\\n--- Extrayendo hasta {LIMITE_REGISTROS} Pull Requests ---\")\n",
        "        pulls = repo.get_pulls(state='all', sort='created', direction='desc')\n",
        "        count = 0\n",
        "\n",
        "        for pr in pulls:\n",
        "            if count >= LIMITE_REGISTROS: break\n",
        "\n",
        "            print(f\"üì• Procesando PR #{pr.number}...\")\n",
        "\n",
        "            # Recolectar t√≠tulo, descripci√≥n y comentarios asociados\n",
        "            titulo = pr.title or \"\"\n",
        "            descripcion = pr.body or \"\"\n",
        "            comments = [c.body for c in pr.get_issue_comments() if c.body]\n",
        "\n",
        "            # L√≥gica modificada: Una fila por cada comentario\n",
        "            if comments:\n",
        "                for comm in comments:\n",
        "                    dataset.append({\n",
        "                        'tipo': 'Pull Request',\n",
        "                        'numero': pr.number,\n",
        "                        'titulo': titulo,\n",
        "                        'descripcion': descripcion,\n",
        "                        'comentarios': comm  # Comentario individual\n",
        "                    })\n",
        "            else:\n",
        "                # Si no hay comentarios, guardamos el PR de todas formas\n",
        "                dataset.append({\n",
        "                    'tipo': 'Pull Request',\n",
        "                    'numero': pr.number,\n",
        "                    'titulo': titulo,\n",
        "                    'descripcion': descripcion,\n",
        "                    'comentarios': \"\"\n",
        "                })\n",
        "\n",
        "            count += 1\n",
        "\n",
        "        # --- SECCI√ìN DE GUARDADO INTEGRADA ---\n",
        "        if dataset:\n",
        "            df = pd.DataFrame(dataset)\n",
        "\n",
        "            # Guardar localmente en la carpeta data del repo\n",
        "            df.to_csv(ruta_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "            print(f\"üìÑ Archivo generado en: {ruta_datos_github_csv}\")\n",
        "            # Sincronizaci√≥n con GitHub (Commit y Push)\n",
        "            sincronizar_con_github(ruta_local_data, [datos_github_csv], \"Actualizaci√≥n autom√°tica de extracci√≥n inicial\")\n",
        "            print(\"‚úÖ Proceso de extracci√≥n, guardado y push finalizado.\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è No se generaron datos para subir.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error en Celda de Extracci√≥n: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3T296idgcR0",
        "outputId": "4bd1002c-5f89-4d42-fe7f-cde0de4a01a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ El archivo 'datos_github.csv' ya existe en el repositorio local.\n",
            "üöÄ Se saltar√° la descarga para ahorrar recursos y tiempo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 6. CARGA Y REVISI√ìN DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "file_path = '/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/datos_github.csv'\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "print(data.count())\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        },
        "id": "uLgbpweddIYn",
        "outputId": "a259c413-4910-4957-fb71-d7422b70652d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tipo           1495\n",
            "numero         1495\n",
            "titulo         1495\n",
            "descripcion    1455\n",
            "comentarios    1451\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tipo  numero                                             titulo  \\\n",
              "0            Issue    4810  [15.0] [l10n_es_verifactu_oca] Bucle de reinte...   \n",
              "1            Issue    4793  MOD347 : Error E010330 Caracteres no v√°lidos '...   \n",
              "2            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "3            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "4            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "...            ...     ...                                                ...   \n",
              "1490  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1491  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1492  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1493  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1494  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "\n",
              "                                            descripcion  \\\n",
              "0     <!-- Provide a general summary of the issue in...   \n",
              "1     module: l10n_es_aeat_mod347\\nversion: 18.0\\n\\n...   \n",
              "2     Actualmente, al generar el fichero del modelo ...   \n",
              "3     Actualmente, al generar el fichero del modelo ...   \n",
              "4     Actualmente, al generar el fichero del modelo ...   \n",
              "...                                                 ...   \n",
              "1490  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1491  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1492  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1493  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1494  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "\n",
              "                                            comentarios  \n",
              "0                                                   NaN  \n",
              "1                              Siendo tratado en #4784   \n",
              "2     Adem√°s, hemos podido comprobar que el mismo pr...  \n",
              "3     S√≠, desde luego la soluci√≥n pasa por llevar es...  \n",
              "4     @pedrobaeza Gracias por la aclaraci√≥n.\\nDe acu...  \n",
              "...                                                 ...  \n",
              "1490  On my way to merge this fine PR!\\nPrepared bra...  \n",
              "1491  @pedrobaeza your merge command was aborted due...  \n",
              "1492                                /ocabot merge patch  \n",
              "1493  On my way to merge this fine PR!\\nPrepared bra...  \n",
              "1494  Congratulations, your PR was merged at 9f6c6d1...  \n",
              "\n",
              "[1495 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83f8ce6e-3222-438f-941e-5ecfb5b2106b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo</th>\n",
              "      <th>numero</th>\n",
              "      <th>titulo</th>\n",
              "      <th>descripcion</th>\n",
              "      <th>comentarios</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4810</td>\n",
              "      <td>[15.0] [l10n_es_verifactu_oca] Bucle de reinte...</td>\n",
              "      <td>&lt;!-- Provide a general summary of the issue in...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4793</td>\n",
              "      <td>MOD347 : Error E010330 Caracteres no v√°lidos '...</td>\n",
              "      <td>module: l10n_es_aeat_mod347\\nversion: 18.0\\n\\n...</td>\n",
              "      <td>Siendo tratado en #4784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>Adem√°s, hemos podido comprobar que el mismo pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>S√≠, desde luego la soluci√≥n pasa por llevar es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>@pedrobaeza Gracias por la aclaraci√≥n.\\nDe acu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>On my way to merge this fine PR!\\nPrepared bra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>@pedrobaeza your merge command was aborted due...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>/ocabot merge patch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>On my way to merge this fine PR!\\nPrepared bra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>Congratulations, your PR was merged at 9f6c6d1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1495 rows √ó 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83f8ce6e-3222-438f-941e-5ecfb5b2106b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-83f8ce6e-3222-438f-941e-5ecfb5b2106b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-83f8ce6e-3222-438f-941e-5ecfb5b2106b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_07e3f326-2f80-4f02-8d69-3d3240dbb7f3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_07e3f326-2f80-4f02-8d69-3d3240dbb7f3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1495,\n  \"fields\": [\n    {\n      \"column\": \"tipo\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pull Request\",\n          \"Issue\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"numero\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 424,\n        \"min\": 3449,\n        \"max\": 4811,\n        \"num_unique_values\": 400,\n        \"samples\": [\n          4801,\n          4727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titulo\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 399,\n        \"samples\": [\n          \"Persona f\\u00edsica - l10n_es_facturae - Odoo 14\",\n          \"[17.0][IMP] l10n_es_account_statement_import_n43: Add exclude pattern\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"descripcion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 383,\n        \"samples\": [\n          \"Antes de este fix, a la hora de a\\u00f1adir un socio destinatario durante la creaci\\u00f3n del reporte, saltaba un error que lo hac\\u00eda imposible.\\r\\n\\r\\nhttps://www.loom.com/share/1f1df1b6ed78452cb1c06c71594690c7\\r\\n\\r\\nMT-13290 @moduon\",\n          \"TT59873\\r\\n\\r\\n@Tecnativa @pedrobaeza @sergio-teruel @christian-ramos-tecnativa could you please review this?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comentarios\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1261,\n        \"samples\": [\n          \"> Buenas [@Joaco1980](https://github.com/Joaco1980) No entiendo muy bien como reproducir el error que comentas. Yo he realizado dos ventas en el POS sin seleccionar cliente y no me da ning\\u00fan error. Las ventas \\\"facturas simplificadas\\\" se env\\u00edan correctamente a TBAI.\\n\\nVale para replicarlo, crear un cliente nuevo \\\"PEPITO FLORES\\\" y d\\u00e9jale sin CP introducido, mete si quieres provincia, pa\\u00eds y todos los datos que consideres pero deja vac\\u00edo el CP. \\n\\nEs ahi donde da el error que comento, ademas si recargas la pagina y vuelves a crear el ticket se crea correctamente pero falla la transmisi\\u00f3n y da error cuando haces un ticket despu\\u00e9s de este error:\\n\\nES): El XML del fichero TicketBAI no cumple el esquema.[Linea:2 Columna:1706] Error:cvc-pattern-valid: Value 'aN-aN-NaN' is not facet-valid with respect to pattern '\\\\d{2,2}-\\\\d{2,2}-\\\\d{4,4}' for type 'FechaType'.(EU): TicketBAI fitxategiaren XMLak ez du eskema betetzen.[Linea:2 Columna:1706] Error:cvc-pattern-valid: Value 'aN-aN-NaN' is not facet-valid with respect to pattern '\\\\d{2,2}-\\\\d{2,2}-\\\\d{4,4}' for type 'FechaType'.\\n\\nConcretamente da un error de validaci\\u00f3n con la fecha de la factura anterior que la deja asi en el xml:\\n<FechaExpedicionFacturaAnterior>aN-aN-NaN</FechaExpedicionFacturaAnterior>\\n\\nYo estoy subiendo los tickets con error en la subida manualmente en el LROE poniendo esa fecha bien no se si es la manera correcta de hacerlo pero al menos se validan, porque el bot\\u00f3n de Cancelar y recrear no sirve en este caso concreto.\",\n          \"Seg\\u00fan el dise\\u00f1o de registro del Libro de IVA que se puede encontrar [aqu\\u00ed](https://sede.agenciatributaria.gob.es/Sede/iva/libros-registro.html) y descargar desde este enlace [Dise\\u00f1os de registro normalizados para los Libros Registro del IVA de personas jur\\u00eddicas no incluidas en SII en los formatos XLS y CSV (actualizado 18-12-2024)](https://sede.agenciatributaria.gob.es/static_files/AEAT/LSIJ.xlsx):\\n\\n> - En \\\"Cuota Deducible\\\" se consignar\\u00e1 el importe de la Cuota del IVA Soportado que sea deducible para cada tipo de IVA. Cuando toda la cuota soportada sea deducible, en la columna \\\"Cuota Deducible\\\" se consignar\\u00e1 el mismo contenido que figure en la columna \\\"Cuota IVA Soportado\\\". \\n\\nEs decir, que seg\\u00fan el ejemplo de @rafaelbn con una prorrata del 80% configurada en la compa\\u00f1\\u00eda entonces el valor mostrado en \\\"Cuota IVA Siportado\\\" deber\\u00eda ser diferente al valor mostrado en \\\"Cuota deducible\\\", concretamente un 80% en este caso. S\\u00f3lo deber\\u00edan coincidir cuando toda la cuota soportada sea deducible.\\n\\nActualmente con los dos m\\u00f3dulos instalados `l10n_es_vat_book` y `l10n_es_vat_prorate` el valor mostrado es id\\u00e9ntico en \\\"Cuota IVA Soportado\\\" y \\\"Cuota deducible\\\" dando a entender que toda la cuota soportada es deducible independientemente del valor que te hayas deducido. Tambi\\u00e9n se est\\u00e1 viendo afectado \\\"Total de factura\\\" pues el valor reflejado es la Base Imponible + Cuota Deducible, pareciendo que el IVA est\\u00e1 m\\u00e1l aplicado.\\n\\nEjemplo pr\\u00e1ctico de una factura de una compa\\u00f1ia con la Prorrata del IVA al 80% para este per\\u00edodo y con dos l\\u00edneas de factura, una l\\u00ednea con un importe de 100 \\u20ac y un impuesto del 21% y otra l\\u00ednea de 200\\u20ac con el mismo impuesto.\\n\\n![Image](https://github.com/user-attachments/assets/097d527b-11ad-48d2-b0a6-792d316bccbc)\\n\\n- El total de la factura son 363\\u20ac.\\n- La cuota de IVA Soportado es 63\\u20ac, el 21% de 100\\u20ac m\\u00e1s el 21% de 200\\u20ac.\\n- La cuota deducible es 50,40\\u20ac, el 80% de la cuota de IVA soportado.\\n\\n\\nSin enmbargo el libro de IVA est\\u00e1 haciendo un c\\u00e1lculo incorrecto, con este ejemplo sale:\\n\\n![Image](https://github.com/user-attachments/assets/bc39ab8f-904d-4010-a7e2-197267f3dc69)\\n\\nA todas luces se ve que el c\\u00e1lculo es incorrecto.\\n\\n\\u00bfCu\\u00e1l cree\\u00eds que es la soluci\\u00f3n correcta @pedrobaeza @HaraldPanten @etobella? \\u00bfCrear un m\\u00f3dulo glue, por ejemplo, `l10n_es_vat_book_prorate` o solventarlo directamente en `l10n_es_vat_book`?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. PREPROCESADO DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "pln_datos_github_csv = 'pln_datos_github.csv'\n",
        "ruta_pln_datos_github_csv = os.path.join(ruta_local_data, pln_datos_github_csv)\n",
        "\n",
        "coleccion = []\n",
        "datos_procesados = []\n",
        "c = 0\n",
        "\n",
        "# --- CONTROL DE SEGURIDAD: Verificar si el archivo ya existe ---\n",
        "if os.path.exists(ruta_pln_datos_github_csv):\n",
        "    print(f\"‚úÖ El archivo '{pln_datos_github_csv}' ya existe.\")\n",
        "    print(\"üì• Cargando datos procesados previos para reconstruir la colecci√≥n...\")\n",
        "\n",
        "    # Cargamos el archivo existente para recuperar la 'coleccion' necesaria para el M√≥dulo 8\n",
        "    pln_coleccion = pd.read_csv(ruta_pln_datos_github_csv)\n",
        "    coleccion = pln_coleccion['Comentario Preprocesado'].astype(str).tolist()\n",
        "    c = len(coleccion)\n",
        "    print(f\"‚úÖ Colecci√≥n reconstruida con {c} documentos.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚è≥ Iniciando preprocesado y estructuraci√≥n de datos...\")\n",
        "\n",
        "    # Usamos iterrows para tener acceso a la fila completa\n",
        "    for index, row in data.iterrows():\n",
        "        doc = row['comentarios']\n",
        "        # Filtro: Ignorar comentarios del bot\n",
        "        if isinstance(doc, str) and doc.strip().lower().startswith('/ocabot'):\n",
        "            continue\n",
        "        # Ejecuci√≥n del Preprocesado\n",
        "        res = preprocesado(doc)\n",
        "        # Validaci√≥n y Guardado\n",
        "        if isinstance(res, str) and res.strip():\n",
        "            # Guardar en colecci√≥n simple (para TF-IDF posterior)\n",
        "            coleccion.append(res)\n",
        "            c += 1\n",
        "            # Guardar fila completa para el nuevo CSV\n",
        "            datos_procesados.append({\n",
        "                'tipo': row['tipo'],\n",
        "                'numero': row['numero'],\n",
        "                'titulo': row['titulo'],\n",
        "                'descripcion': row['descripcion'],\n",
        "                'comentarios': doc,  # Original\n",
        "                'Comentario Preprocesado': res, # Salida de la funci√≥n\n",
        "                'Vector FT-IDF unigrama': \"\",\n",
        "                'Vector FT-IDF bigrama': \"\",\n",
        "                'Etiqueta: positivo(1) | neutral(0) | negativo(-1)': \"\",\n",
        "                'Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)': \"\",\n",
        "                'Etiqueta K-Means': \"\",\n",
        "                'Reetiqueta K-Means': \"\",\n",
        "            })\n",
        "\n",
        "    print(f'‚úÖ Se procesaron y estructuraron: {c} documentos')\n",
        "\n",
        "    # --- CREACI√ìN Y SUBIDA DEL NUEVO ARCHIVO CSV ---\n",
        "    if datos_procesados:\n",
        "        pln_coleccion = pd.DataFrame(datos_procesados)\n",
        "\n",
        "        # Guardar localmente en la ruta del repositorio\n",
        "        pln_coleccion.to_csv(ruta_pln_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "        print(f\"üìÑ Archivo '{pln_datos_github_csv}' generado exitosamente en: {ruta_pln_datos_github_csv}\")\n",
        "\n",
        "        # Sincronizamos a GitHub usando la funci√≥n modular\n",
        "        sincronizar_con_github(ruta_local_data, [pln_datos_github_csv, 'log_file.txt'], \"Guardar Pln Dataset y Logs (Preprocesado)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se generaron datos v√°lidos tras el preprocesado.\")\n",
        "\n",
        "# Validaci√≥n visual\n",
        "print(\"\\n--- Muestra de los primeros 5 elementos de la colecci√≥n ---\")\n",
        "print(coleccion[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VIBF2K4lPxy",
        "outputId": "092121b2-bce1-4c25-d7bd-d4b8b583a826"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ El archivo 'pln_datos_github.csv' ya existe.\n",
            "üì• Cargando datos procesados previos para reconstruir la colecci√≥n...\n",
            "‚úÖ Colecci√≥n reconstruida con 915 documentos.\n",
            "\n",
            "--- Muestra de los primeros 5 elementos de la colecci√≥n ---\n",
            "['tratar', 'poder comprobar problema reproducir odoo utilizar m√≥dulo l10n_es_aeat_mod190 rama generar fichero modelo odoo ejercicio sede electr√≥nico aeat devolver error similar relacionado falta campo obligatorio correspondiente prestaci√≥n jubilaci√≥n viudedad pensi√≥n incapacidad percepci√≥n asimilada requerir aeat ejercicio actualmente dicho campo parecer contemplado rama m√≥dulo provocar fichero generado v√°lir importaci√≥n campo incorporar rama considerar backport cambio permitir mantener compatibilidad modelo requisito aeat ejercicio gracias trabajo soporte', 'soluci√≥n pasar cambio ambos rama cuesti√≥n estar interesado financiar hacer esperar hacer funcionar software abierto contribuir voluntariamente ideal contribuir parche hacer versi√≥n posible exigir hacer parche poner versi√≥n intermedio soler pedir llevar versi√≥n superior mantener sincronizado caso rev√©s haber poner versi√≥n disponible estar versi√≥n anterior mayor√≠a trabajo faltar empuj√≥n anim√°is', 'pedrobaeza gracias aclaraci√≥n poner backport cambio rama cubrir requisito ejercicio intentar gracias cara sonreir ligeramente', 'hola agradecer alguien confirmar problema verifactu v17 buscar raz√≥n funcionar correctamente v16 migrar v17 aparecer error encontrar gracias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 8. VECTORIZACI√ìN TF-IDF\n",
        "# ==========================================\n",
        "\n",
        "tfidf_vectorizer_unigramas_joblib = \"tfidf_vectorizer_unigramas.joblib\"\n",
        "tfidf_vectorizer_bigramas_joblib = \"tfidf_vectorizer_bigramas.joblib\"\n",
        "\n",
        "ruta_tfidf_vectorizer_unigramas_joblib = os.path.join(ruta_local_data, tfidf_vectorizer_unigramas_joblib)\n",
        "ruta_tfidf_vectorizer_bigramas_joblib = os.path.join(ruta_local_data, tfidf_vectorizer_bigramas_joblib)\n",
        "\n",
        "# --- CONTROL DE SEGURIDAD: Verificar si los modelos ya existen ---\n",
        "if os.path.exists(ruta_tfidf_vectorizer_unigramas_joblib) and os.path.exists(ruta_tfidf_vectorizer_bigramas_joblib):\n",
        "    print(f\"‚úÖ Los modelos vectorizadores ya existen en la carpeta data.\")\n",
        "    print(\"üì• Cargando modelos persistidos en lugar de entrenar nuevos...\")\n",
        "\n",
        "    vectorizer_unigramas = joblib.load(ruta_tfidf_vectorizer_unigramas_joblib)\n",
        "    vectorizer_bigramas = joblib.load(ruta_tfidf_vectorizer_bigramas_joblib)\n",
        "\n",
        "    if coleccion:\n",
        "        X_tfidf_unigramas = vectorizer_unigramas.transform(coleccion)\n",
        "        X_tfidf_bigramas = vectorizer_bigramas.transform(coleccion)\n",
        "        print(\"‚úÖ Matrices TF-IDF reconstruidas desde los archivos cargados.\")\n",
        "else:\n",
        "    # Par√°metros recomendados para repos Git\n",
        "    vectorizer_unigramas = TfidfVectorizer(\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(1,1),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "        min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "        max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        norm=\"l2\"\n",
        "    )\n",
        "\n",
        "    vectorizer_bigramas = TfidfVectorizer(\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(2,2),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "        min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "        max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        norm=\"l2\"\n",
        "    )\n",
        "\n",
        "    if coleccion:\n",
        "        # Fit y transformar la colecci√≥n completa (IDF se calcula aqu√≠)\n",
        "        X_tfidf_unigramas = vectorizer_unigramas.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (unigramas)\n",
        "        print(\"Matriz TF-IDF_unigramas:\", X_tfidf_unigramas.shape)\n",
        "        X_tfidf_bigramas = vectorizer_bigramas.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (bigramas)\n",
        "        print(\"Matriz TF-IDF_bigramas:\", X_tfidf_bigramas.shape)\n",
        "\n",
        "        # AJUSTE: Guardado directo en la ruta local para compatibilidad modular\n",
        "        joblib.dump(vectorizer_unigramas, ruta_tfidf_vectorizer_unigramas_joblib) # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "        joblib.dump(vectorizer_bigramas, ruta_tfidf_vectorizer_bigramas_joblib) # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "\n",
        "        sincronizar_con_github(ruta_local_data, [tfidf_vectorizer_unigramas_joblib, tfidf_vectorizer_bigramas_joblib], \"Aggregar Modelos Vectorizados TF-IDF (Unigrams/Bigrams)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMJWRF3PnTCm",
        "outputId": "4466a4d3-6f04-4708-bb9d-1e669049d23d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Los modelos vectorizadores ya existen en la carpeta data.\n",
            "üì• Cargando modelos persistidos en lugar de entrenar nuevos...\n",
            "‚úÖ Matrices TF-IDF reconstruidas desde los archivos cargados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 9. VECTORIZACI√ìN DE LA COLECCI√ìN\n",
        "# ==========================================\n",
        "\n",
        "try:\n",
        "    print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "    # Sincronizar repositorio local\n",
        "    inicializar_repositorio()\n",
        "\n",
        "    # Definimos la ruta del repositorio como base √∫nica\n",
        "    pln_datos_github_csv = 'pln_datos_github.csv'\n",
        "\n",
        "    ruta_pln_datos_github_csv = os.path.join(ruta_local_data, pln_datos_github_csv)\n",
        "\n",
        "    # Cargar el CSV (Prioridad: Versi√≥n de GitHub para no perder datos previos)\n",
        "    if os.path.exists(ruta_pln_datos_github_csv):\n",
        "        pln_coleccion = pd.read_csv(ruta_pln_datos_github_csv)\n",
        "        print(\"   ...Cargando versi√≥n desde el repositorio local.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No se encontr√≥ el archivo '{pln_datos_github_csv}' en ninguna ubicaci√≥n.\")\n",
        "\n",
        "    # --- CONTROL DE SEGURIDAD: Verificar si las columnas ya est√°n llenas ---\n",
        "    col_uni = 'Vector FT-IDF unigrama'\n",
        "    col_bi = 'Vector FT-IDF bigrama'\n",
        "\n",
        "    # Verificamos si hay al menos un dato nulo en alguna de las dos columnas\n",
        "    if pln_coleccion[col_uni].notna().all() and pln_coleccion[col_bi].notna().all():\n",
        "        print(f\"‚úÖ Vectorizado de la coleccion ya realizado.\")\n",
        "    else:\n",
        "        # --- L√ìGICA DE EXTRACCI√ìN DE PALABRAS Y PESOS ---\n",
        "\n",
        "        vocab_uni = vectorizer_unigramas.get_feature_names_out()\n",
        "        vocab_bi = vectorizer_bigramas.get_feature_names_out()\n",
        "\n",
        "        def matriz_a_diccionario(matriz_fila, vocabulario):\n",
        "            \"\"\"Convierte una fila de matriz TF-IDF en un diccionario {palabra: peso} limpio\"\"\"\n",
        "            indices_no_cero = matriz_fila.nonzero()[1]\n",
        "            return {vocabulario[i]: round(float(matriz_fila[0, i]), 4) for i in indices_no_cero}\n",
        "\n",
        "        print(\"   ...Transformando matrices a formato {palabra: peso}\")\n",
        "\n",
        "        dict_unigramas = []\n",
        "        dict_bigramas = []\n",
        "\n",
        "        for i in range(len(coleccion)):\n",
        "            d_uni = matriz_a_diccionario(X_tfidf_unigramas[i], vocab_uni)\n",
        "            d_bi = matriz_a_diccionario(X_tfidf_bigramas[i], vocab_bi)\n",
        "\n",
        "            dict_unigramas.append(str(d_uni))\n",
        "            dict_bigramas.append(str(d_bi))\n",
        "\n",
        "        # Actualizaci√≥n de columnas\n",
        "        pln_coleccion[col_uni] = dict_unigramas\n",
        "        pln_coleccion[col_bi] = dict_bigramas\n",
        "\n",
        "        # GUARDADO Y SUBIDA (SOLUCI√ìN AL DUPLICADO)\n",
        "        # Guardamos EXCLUSIVAMENTE en la ruta del repositorio\n",
        "        pln_coleccion.to_csv(ruta_pln_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        # Definimos los archivos usando sus rutas completas dentro del repo\n",
        "        mensaje = \"Actualizaci√≥n: pln_datos_github.csv\"\n",
        "\n",
        "        # Llamada a la funci√≥n de sincronizaci√≥n\n",
        "        sincronizar_con_github(ruta_local_data, [pln_datos_github_csv], mensaje)\n",
        "\n",
        "        print(f\"‚úÖ Proceso completado. Archivo guardado en: {ruta_pln_datos_github_csv}\")\n",
        "        print(\"üöÄ Cambios enviados a GitHub sin duplicados en la ra√≠z.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en el M√≥dulo 10: {e}\")"
      ],
      "metadata": {
        "id": "tmMvffauWxTU",
        "outputId": "f8846a11-b6a1-4e4f-dc03-fc40c4ba96ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Preparando persistencia en el repositorio: U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "üìÇ Carpeta de datos lista en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "   ...Cargando versi√≥n desde el repositorio local.\n",
            "‚úÖ Vectorizado de la coleccion ya realizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================\n",
        "# 10. ETIQUETADO DE PARA AN√ÅLISIS DE SENTIMIENTOS\n",
        "# ================================================\n",
        "\n",
        "# Cargar el archivo como pln_coleccion\n",
        "ruta_pln_datos_github_csv = '/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/pln_datos_github.csv'\n",
        "pln_datos_github_csv = 'pln_datos_github.csv'\n",
        "pln_coleccion = pd.read_csv(ruta_pln_datos_github_csv)\n",
        "\n",
        "col_comentario = 'Comentario Preprocesado'\n",
        "col_etiqueta = 'Etiqueta: positivo(1) | neutral(0) | negativo(-1)'\n",
        "\n",
        "# Limpieza t√©cnica: Asegurar que los vac√≠os sean reconocidos como NaN\n",
        "# Esto soluciona el problema de si la primera fila tiene espacios invisibles\n",
        "pln_coleccion[col_etiqueta] = pd.to_numeric(pln_coleccion[col_etiqueta], errors='coerce')\n",
        "\n",
        "# Verificar pendientes\n",
        "pendientes_mask = pln_coleccion[col_etiqueta].isna()\n",
        "if not pendientes_mask.any():\n",
        "    print(\"================================\")\n",
        "    print(\"      Etiquetado Completado     \")\n",
        "    print(\"================================\")\n",
        "else:\n",
        "    print(f\"Iniciando... Filas pendientes: {pendientes_mask.sum()}\\n\")\n",
        "\n",
        "    # Bucle para recorrer la colecci√≥n\n",
        "    for index, row in pln_coleccion.iterrows():\n",
        "\n",
        "        # Validaci√≥n robusta: verifica si es NaN real\n",
        "        if pd.isna(row[col_etiqueta]):\n",
        "            pln_doc = row[col_comentario]\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"ID Fila: {index}\") # Esto te confirmar√° si empieza en 0 o 1\n",
        "            print(f\"Comentario actual: {pln_doc}\")\n",
        "\n",
        "            continuar_proceso = True\n",
        "            while True:\n",
        "                entrada = input(\"Etiqueta: positivo(1) | neutral(0) | negativo(-1) o '#' para salir: \").strip()\n",
        "\n",
        "                if entrada == '#':\n",
        "                    continuar_proceso = False\n",
        "                    break\n",
        "\n",
        "                if entrada in ['1', '0', '-1']:\n",
        "                    etiqueta_pln_doc = int(entrada)\n",
        "\n",
        "                    # --- GUARDADO DENTRO DEL WHILE ---\n",
        "                    pln_coleccion.at[index, col_etiqueta] = etiqueta_pln_doc\n",
        "                    pln_coleccion.to_csv(ruta_pln_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "                    mensaje = \"Etiqueta Actualizada\"\n",
        "\n",
        "                    sincronizar_con_github(ruta_local_data, [pln_datos_github_csv], mensaje)\n",
        "\n",
        "                    print(f\"‚úÖ Fila {index} sincronizada.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è Entrada inv√°lida.\")\n",
        "\n",
        "            if not continuar_proceso:\n",
        "                print(\"\\nProceso detenido.\")\n",
        "                break\n",
        "\n",
        "    if pln_coleccion[col_etiqueta].isna().sum() == 0:\n",
        "        print(\"\\n================================\")\n",
        "        print(\"      Etiquetado Completado     \")\n",
        "        print(\"================================\")"
      ],
      "metadata": {
        "id": "Xf_BriZISbSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "635bf544-cc5a-4712-9753-ddd1790e8c35"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================\n",
            "      Etiquetado Completado     \n",
            "================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 11. MODELO DE AN√ÅLISIS DE SENTIMIENTOS\n",
        "# ==========================================\n",
        "\n",
        "tfidf_vectorizer_ngram_1_2_joblib = \"tfidf_vectorizer_ngram_1_2.joblib\"\n",
        "ruta_tfidf_vectorizer_ngramas_1_2_joblib = os.path.join(ruta_local_data, tfidf_vectorizer_ngram_1_2_joblib)\n",
        "# --- CONTROL DE SEGURIDAD: Verificar si los modelos ya existen ---\n",
        "if os.path.exists(ruta_tfidf_vectorizer_ngramas_1_2_joblib):\n",
        "    print(f\"‚úÖ El modelo vectorizador ya existen en la carpeta data.\")\n",
        "    print(\"üì• Cargando modelo persistido en lugar de entrenar nuevos...\")\n",
        "    vectorizer_ngramas_1_2 = joblib.load(ruta_tfidf_vectorizer_ngramas_1_2_joblib)\n",
        "    if coleccion:\n",
        "        X_tfidf_ngramas_1_2 = vectorizer_ngramas_1_2.transform(coleccion)\n",
        "        print(\"‚úÖ Matriz TF-IDF reconstruida desde el archivo cargado.\")\n",
        "else:\n",
        "    # Par√°metros recomendados para repos Git\n",
        "    vectorizer_ngramas_1_2 = TfidfVectorizer(\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(1,2),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "        min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "        max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        norm=\"l2\"\n",
        "    )\n",
        "    if coleccion:\n",
        "        # Fit y transformar la colecci√≥n completa (IDF se calcula aqu√≠)\n",
        "        X_tfidf_ngramas_1_2 = vectorizer_ngramas_1_2.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (unigramas)\n",
        "        print(\"Matriz TF-IDF_unigramas:\", X_tfidf_ngramas_1_2.shape)\n",
        "        # AJUSTE: Guardado directo en la ruta local para compatibilidad modular\n",
        "        joblib.dump(vectorizer_ngramas_1_2, ruta_tfidf_vectorizer_ngramas_1_2_joblib) # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "        sincronizar_con_github(ruta_local_data, [tfidf_vectorizer_ngram_1_2_joblib], \"Agregar Model Vectorizador TF-IDF (1,2)\")\n",
        "\n",
        "print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "# Sincronizar repositorio local\n",
        "inicializar_repositorio()\n",
        "\n",
        "# Definimos la ruta del repositorio como base √∫nica\n",
        "pln_datos_github_csv = 'pln_datos_github.csv'\n",
        "\n",
        "ruta_pln_datos_github_csv = os.path.join(ruta_local_data, pln_datos_github_csv)\n",
        "\n",
        "if os.path.exists(ruta_pln_datos_github_csv):\n",
        "    pln_coleccion = pd.read_csv(ruta_pln_datos_github_csv)\n",
        "    print(\"   ...Cargando versi√≥n desde el repositorio local.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ el archivo '{pln_datos_github_csv}' en ninguna ubicaci√≥n.\")\n",
        "\n",
        "if ('Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)' in pln_coleccion.columns and\n",
        "        pln_coleccion['Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)'].notna().any()):\n",
        "        print(\"‚ö†Ô∏è Columnas ya existen y contienen datos, se omite la asignaci√≥n para evitar sobrescribir.\")\n",
        "        X = X_tfidf_ngramas_1_2\n",
        "        y = pln_coleccion['Etiqueta: positivo(1) | neutral(0) | negativo(-1)']\n",
        "\n",
        "        # Dividir en entrenamiento y prueba\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Crear modelo de regresi√≥n log√≠stica multiclase\n",
        "        reglog_multinominal_model  = LogisticRegression(\n",
        "            solver='lbfgs',\n",
        "            multi_class='multinomial',\n",
        "            max_iter=1000\n",
        "        )\n",
        "\n",
        "        # Entrenar\n",
        "        reglog_multinominal_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predecir\n",
        "        y_pred = reglog_multinominal_model.predict(X_test)\n",
        "\n",
        "        # Evaluar\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Accuracy expl√≠cito\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "else:\n",
        "    reglog_multinominal_model_joblib = \"reglog_multinominal_model.joblib\"\n",
        "    ruta_reglog_multinominal_model_joblib = os.path.join(ruta_local_data, reglog_multinominal_model_joblib)\n",
        "    # --- CONTROL DE SEGURIDAD: Verificar si los modelos ya existen ---\n",
        "    if os.path.exists(ruta_reglog_multinominal_model_joblib):\n",
        "        print(f\"‚úÖ El modelo vectorizador ya existen en la carpeta data.\")\n",
        "        print(\"üì• Cargando modelo persistido en lugar de entrenar nuevos...\")\n",
        "        reglog_multinominal_model = joblib.load(ruta_reglog_multinominal_model_joblib)\n",
        "\n",
        "    else:\n",
        "        X = X_tfidf_ngramas_1_2\n",
        "        y = pln_coleccion['Etiqueta: positivo(1) | neutral(0) | negativo(-1)']\n",
        "\n",
        "        # Dividir en entrenamiento y prueba\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "        # Crear modelo de regresi√≥n log√≠stica multiclase\n",
        "        reglog_multinominal_model  = LogisticRegression(\n",
        "            solver='lbfgs',\n",
        "            multi_class='multinomial',\n",
        "            max_iter=1000\n",
        "        )\n",
        "\n",
        "        # Entrenar\n",
        "        reglog_multinominal_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predecir\n",
        "        y_pred = reglog_multinominal_model.predict(X_test)\n",
        "\n",
        "        # Evaluar\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        # Accuracy expl√≠cito\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "        etiqueta_sentimental = reglog_multinominal_model.predict(X)\n",
        "        pln_coleccion['Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)'] = etiqueta_sentimental\n",
        "\n",
        "        pln_coleccion.to_csv(ruta_pln_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        joblib.dump(reglog_multinominal_model, ruta_reglog_multinominal_model_joblib)\n",
        "        mensaje = \"Creaci√≥n Modelo de An√°lisis de Sentimiento y Etiquetado de Sentimental\"\n",
        "        # Sincronizar con GitHub\n",
        "        # Nota: Se pasa la ruta del archivo o el nombre seg√∫n requiera tu funci√≥n\n",
        "        sincronizar_con_github(ruta_local_data, [pln_datos_github_csv, reglog_multinominal_model_joblib], mensaje)\n",
        "\n",
        "        print(\"‚úÖ Modelo de An√°lisis de Sentimiento regresi√≥n log√≠stica guardado en:\", ruta_reglog_multinominal_model_joblib)\n",
        "print(pln_coleccion[['Etiqueta: positivo(1) | neutral(0) | negativo(-1)', 'Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)']].head(10))"
      ],
      "metadata": {
        "id": "gvayYB8qrUo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79298507-8154-426c-e46b-ef3a67400091"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ El modelo vectorizador ya existen en la carpeta data.\n",
            "üì• Cargando modelo persistido en lugar de entrenar nuevos...\n",
            "‚úÖ Matriz TF-IDF reconstruida desde el archivo cargado.\n",
            "üìÇ Preparando persistencia en el repositorio: U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "üìÇ Carpeta de datos lista en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "   ...Cargando versi√≥n desde el repositorio local.\n",
            "‚ö†Ô∏è Columnas ya existen y contienen datos, se omite la asignaci√≥n para evitar sobrescribir.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        -1.0       0.58      0.35      0.43        52\n",
            "         0.0       0.60      0.72      0.65       111\n",
            "         1.0       0.72      0.71      0.71       112\n",
            "\n",
            "    accuracy                           0.64       275\n",
            "   macro avg       0.63      0.59      0.60       275\n",
            "weighted avg       0.64      0.64      0.64       275\n",
            "\n",
            "Accuracy: 0.6436\n",
            "   Etiqueta: positivo(1) | neutral(0) | negativo(-1)  \\\n",
            "0                                                0.0   \n",
            "1                                                0.0   \n",
            "2                                                1.0   \n",
            "3                                                1.0   \n",
            "4                                                0.0   \n",
            "5                                                1.0   \n",
            "6                                               -1.0   \n",
            "7                                                0.0   \n",
            "8                                                0.0   \n",
            "9                                                1.0   \n",
            "\n",
            "   Etiquetado Modelo: positivo(1) | neutral(0) | negativo(-1)  \n",
            "0                                                0.0           \n",
            "1                                                0.0           \n",
            "2                                                0.0           \n",
            "3                                                1.0           \n",
            "4                                                0.0           \n",
            "5                                                1.0           \n",
            "6                                               -1.0           \n",
            "7                                                1.0           \n",
            "8                                                0.0           \n",
            "9                                                1.0           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 12. MODELO K-Means\n",
        "# ==========================================\n",
        "\n",
        "tfidf_vectorizer_ngram_1_2_joblib = \"tfidf_vectorizer_ngram_1_2.joblib\"\n",
        "ruta_tfidf_vectorizer_ngramas_1_2_joblib = os.path.join(ruta_local_data, tfidf_vectorizer_ngram_1_2_joblib)\n",
        "# --- CONTROL DE SEGURIDAD: Verificar si los modelos ya existen ---\n",
        "\n",
        "if os.path.exists(ruta_tfidf_vectorizer_ngramas_1_2_joblib):\n",
        "    print(f\"‚úÖ El modelo vectorizador ya existen en la carpeta data.\")\n",
        "    print(\"üì• Cargando modelo persistido en lugar de entrenar nuevos...\")\n",
        "    vectorizer_ngramas_1_2 = joblib.load(ruta_tfidf_vectorizer_ngramas_1_2_joblib)\n",
        "    if coleccion:\n",
        "        X_tfidf_ngramas_1_2 = vectorizer_ngramas_1_2.transform(coleccion)\n",
        "        print(\"‚úÖ Matriz TF-IDF reconstruida desde el archivo cargado.\")\n",
        "else:\n",
        "    # Par√°metros recomendados para repos Git\n",
        "    vectorizer_ngramas_1_2 = TfidfVectorizer(\n",
        "        analyzer=\"word\",\n",
        "        ngram_range=(1,2),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "        min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "        max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "        use_idf=True,\n",
        "        smooth_idf=True,\n",
        "        norm=\"l2\"\n",
        "    )\n",
        "    if coleccion:\n",
        "        # Fit y transformar la colecci√≥n completa (IDF se calcula aqu√≠)\n",
        "        X_tfidf_ngramas_1_2 = vectorizer_ngramas_1_2.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (unigramas)\n",
        "        print(\"Matriz TF-IDF_unigramas:\", X_tfidf_ngramas_1_2.shape)\n",
        "        # AJUSTE: Guardado directo en la ruta local para compatibilidad modular\n",
        "        joblib.dump(vectorizer_ngramas_1_2, ruta_tfidf_vectorizer_ngramas_1_2_joblib) # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "        sincronizar_con_github(ruta_local_data, [tfidf_vectorizer_ngram_1_2_joblib], \"Agregar Model Vectorizador TF-IDF (1,2)\")\n",
        "\n",
        "tesauro = {\n",
        "    \"Soporte\": [\n",
        "        \"error\", \"bug\", \"fallo\", \"incidencia\", \"problema\", \"cron\",\n",
        "        \"neutralizar\", \"instalar\", \"actualizar\", \"corregir\", \"soluci√≥n\",\n",
        "        \"parche\", \"reproducir\", \"despliegue\"\n",
        "    ],\n",
        "    \"Normativa\": [\n",
        "        \"aeat\", \"verifactu\", \"sii\", \"ticketbai\", \"facturae\", \"obligatorio\",\n",
        "        \"reglamento\", \"normativa\", \"declaraci√≥n responsable\", \"cumplimiento\",\n",
        "        \"ejercicio\", \"modelo\", \"campo obligatorio\", \"legalidad\"\n",
        "    ],\n",
        "    \"Desarrollo\": [\n",
        "        \"backport\", \"migraci√≥n\", \"pr\", \"pull request\", \"refactorizaci√≥n\",\n",
        "        \"versi√≥n\", \"repositorio\", \"branch\", \"commit\", \"merge\", \"portear\",\n",
        "        \"runboat\", \"tester\"\n",
        "    ],\n",
        "    \"Configuraci√≥n\": [\n",
        "        \"diario\", \"contabilidad\", \"integraci√≥n\", \"conciliaci√≥n\", \"empresa\",\n",
        "        \"escenario\", \"facturaci√≥n\", \"importaci√≥n\", \"configuraci√≥n\",\n",
        "        \"multiempresa\", \"operativa\", \"rectificativa\", \"autofactura\"\n",
        "    ],\n",
        "    \"Comunidad\": [\n",
        "        \"gracias\", \"aportar\", \"contribuir\", \"voluntario\", \"financiar\",\n",
        "        \"oca\", \"colaboraci√≥n\", \"coordinaci√≥n\", \"ayuda\", \"agradecer\",\n",
        "        \"comentario\", \"issue\", \"cerrar\", \"abrir\"\n",
        "    ],\n",
        "    \"Facturaci√≥nElectronica\": [\n",
        "        \"facturae\", \"factoring\", \"literal legal\", \"qr\", \"hash\", \"m√≥dulo\",\n",
        "        \"l10n_es\", \"odoo\", \"instalaci√≥n\", \"dependencia\", \"firma\",\n",
        "        \"xml\", \"xsd\", \"validaci√≥n\"\n",
        "    ],\n",
        "    \"TicketBAI\": [\n",
        "        \"tbai\", \"bizkaia\", \"vizcaya\", \"cp\", \"destinatario\", \"nif\",\n",
        "        \"transmisi√≥n\", \"xml\", \"esquema\", \"agencia\", \"provincia\",\n",
        "        \"vasco\", \"obligatorio\", \"simplificada\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# --- Clustering no supervisado ---\n",
        "# Definir n√∫mero de clusters y semilla para reproducibilidad\n",
        "k = 7\n",
        "random_state = 42  # se puede cambiar, afecta a la semilla\n",
        "\n",
        "# Inicializar el modelo K-Means\n",
        "kmeans_model = KMeans(\n",
        "    n_clusters=k,\n",
        "    random_state=random_state,\n",
        "    n_init=10,        # n√∫mero de inicializaciones para mayor estabilidad\n",
        "    max_iter=300      # iteraciones m√°ximas\n",
        ")\n",
        "\n",
        "# Entrenar el modelo con la matriz TF-IDF\n",
        "kmeans_model.fit(X_tfidf_ngramas_1_2)\n",
        "\n",
        "# Obtener etiquetas de cluster para cada mensaje\n",
        "etiquetas = kmeans_model.labels_\n",
        "\n",
        "kmeans_model_joblib = 'kmeans_model.joblib'\n",
        "ruta_kmeans_model_joblib = os.path.join(ruta_local_data, kmeans_model_joblib) #Para carga\n",
        "\n",
        "# Guardar el modelo entrenado para reutilizarlo\n",
        "joblib.dump(kmeans_model, ruta_kmeans_model_joblib)\n",
        "\n",
        "sincronizar_con_github(ruta_local_data, [kmeans_model_joblib], \"Agregar Modelo K-Means\")\n",
        "\n",
        "print(\"‚úÖ Entrenamiento K-Means completado\")\n",
        "print(\"Clusters asignados:\", set(etiquetas))\n",
        "\n",
        "\n",
        "# --- Extracci√≥n de descriptores de cluster ---\n",
        "# Obtenemos los centroides del modelo K-Means\n",
        "centroides = kmeans_model.cluster_centers_\n",
        "\n",
        "# Obtenemos el vocabulario del vectorizador\n",
        "feature_names = vectorizer_ngramas_1_2.get_feature_names_out()\n",
        "\n",
        "# N√∫mero de t√©rminos m√°s representativos por cluster\n",
        "top_n = 15\n",
        "\n",
        "# Diccionario para guardar las palabras dominantes de cada cluster\n",
        "cluster_descriptores = {}\n",
        "\n",
        "for i in range(k):\n",
        "    # Ordenar t√©rminos por peso en el centroide\n",
        "    orden = centroides[i].argsort()[::-1][:top_n]\n",
        "    palabras_top = [feature_names[ind] for ind in orden]\n",
        "    cluster_descriptores[i] = palabras_top\n",
        "\n",
        "# Mostrar resultados\n",
        "for cluster_id, palabras in cluster_descriptores.items():\n",
        "    print(f\"\\nCluster {cluster_id}:\")\n",
        "    print(\", \".join(palabras))\n",
        "\n",
        "# --- Mapeo sem√°ntico con el tesauro ---\n",
        "cluster_categorias = {}\n",
        "\n",
        "for cluster_id, palabras in cluster_descriptores.items():\n",
        "    # Contador de coincidencias por categor√≠a\n",
        "    coincidencias = {categoria: 0 for categoria in tesauro.keys()}\n",
        "\n",
        "    for palabra in palabras:\n",
        "        for categoria, lista in tesauro.items():\n",
        "            if palabra in lista:\n",
        "                coincidencias[categoria] += 1\n",
        "\n",
        "    # Seleccionar la categor√≠a con m√°s coincidencias\n",
        "    categoria_asignada = max(coincidencias, key=coincidencias.get)\n",
        "    cluster_categorias[cluster_id] = categoria_asignada\n",
        "\n",
        "# Mostrar resultados\n",
        "for cluster_id, categoria in cluster_categorias.items():\n",
        "    print(f\"Cluster {cluster_id} ‚Üí Categor√≠a asignada: {categoria}\")\n",
        "\n",
        "\n",
        "# --- Clasificaci√≥n por reglas (fallback) ---\n",
        "etiqueta_final = []\n",
        "\n",
        "for idx, texto in enumerate(coleccion):\n",
        "    texto_lower = texto.lower()\n",
        "    categoria_detectada = None\n",
        "\n",
        "    # Buscar coincidencias directas con el tesauro\n",
        "    for categoria, lista in tesauro.items():\n",
        "        for palabra in lista:\n",
        "            if palabra in texto_lower:\n",
        "                categoria_detectada = categoria\n",
        "                break\n",
        "        if categoria_detectada:\n",
        "            break\n",
        "\n",
        "    # Si no se detecta nada fuerte, usar la categor√≠a del cluster\n",
        "    if categoria_detectada is None:\n",
        "        categoria_detectada = cluster_categorias[etiquetas[idx]]\n",
        "\n",
        "    etiqueta_final.append(categoria_detectada)\n",
        "\n",
        "print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "# Sincronizar repositorio local\n",
        "inicializar_repositorio()\n",
        "\n",
        "# Definimos la ruta del repositorio como base √∫nica\n",
        "pln_datos_github_csv = 'pln_datos_github.csv'\n",
        "\n",
        "ruta_pln_datos_github_csv = os.path.join(ruta_local_data, pln_datos_github_csv)\n",
        "\n",
        "if os.path.exists(ruta_pln_datos_github_csv):\n",
        "    pln_coleccion = pd.read_csv(ruta_pln_datos_github_csv)\n",
        "    print(\"   ...Cargando versi√≥n desde el repositorio local.\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"No se encontr√≥ el archivo '{pln_datos_github_csv}' en ninguna ubicaci√≥n.\")\n",
        "\n",
        "if ('Etiqueta K-Means' in pln_coleccion.columns and\n",
        "        'Reetiquetado K-Means' in pln_coleccion.columns and\n",
        "        pln_coleccion['Etiqueta K-Means'].notna().any() and\n",
        "        pln_coleccion['Reetiquetado K-Means'].notna().any()):\n",
        "\n",
        "        print(\"‚ö†Ô∏è Columnas ya existen y contienen datos, se omite la asignaci√≥n para evitar sobrescribir.\")\n",
        "else:\n",
        "    pln_coleccion['Etiqueta K-Means'] = etiquetas\n",
        "    pln_coleccion['Reetiquetado K-Means'] = etiqueta_final\n",
        "    print(pln_coleccion.head(10))\n",
        "\n",
        "    pln_coleccion.to_csv(ruta_pln_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    mensaje = \"Etiquetado y Reetiquetado con el Modelo K-Means\"\n",
        "    # Sincronizar con GitHub\n",
        "    # Nota: Se pasa la ruta del archivo o el nombre seg√∫n requiera tu funci√≥n\n",
        "    sincronizar_con_github(ruta_local_data, [pln_datos_github_csv], mensaje)\n",
        "\n",
        "    print(\"‚úÖ Proceso completado: Atualizaci√≥n archivo pln_datos_github.csv.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Hycfq1gzx1i-",
        "outputId": "77f7cedc-3b0c-4c78-9697-b91c0c4684eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ El modelo vectorizador ya existen en la carpeta data.\n",
            "üì• Cargando modelo persistido en lugar de entrenar nuevos...\n",
            "‚úÖ Matriz TF-IDF reconstruida desde el archivo cargado.\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data\n",
            "[main 7695939] Agregar Modelo K-Means (2026-02-12 02:10)\n",
            " 1 file changed, 0 insertions(+), 0 deletions(-)\n",
            "üöÄ Enviando 1 archivo(s) a GitHub...\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Current branch main is up to date.\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 16.16 KiB | 4.04 MiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J.git\n",
            "   90ee791..7695939  main -> main\n",
            "‚úÖ Sincronizaci√≥n exitosa.\n",
            "/content\n",
            "‚úÖ Entrenamiento K-Means completado\n",
            "Clusters asignados: {np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)}\n",
            "\n",
            "Cluster 0:\n",
            "hola, poder, problema, √©l, gracias, tener, favor, versi√≥n, cambio, haber, hacer, ok, aeat, caso, backport\n",
            "\n",
            "Cluster 1:\n",
            "gracia, odoo, pedro, probar, actualizar, gracia pedro, m√≥dulo, disculpa, pedrobaeza, problema, tener, mil gracia, pon, mil, valer\n",
            "\n",
            "Cluster 2:\n",
            "coraz√≥n, coraz√≥n rojo, rojo, error producir, error pos, error permitir, error nif, error m√≥dulo, error instalado, error implicar, error env√≠o, error docker, error c√≥digo, error ctroleof, error comentas\n",
            "\n",
            "Cluster 3:\n",
            "pr, cerrar, fusionar, issue, cerrar pr, √©l, poder, repositorio, falta, abrir, poner, haraldpantar, cerrar issue, comentado, problema\n",
            "\n",
            "Cluster 4:\n",
            "pedrobaeza, revisar, gracias, poder, poder revisar, pedrobaeza poder, haraldpantar pedrobaeza, gracias pedrobaeza, pedrobaeza almumu, almumu, haraldpantar, mergear, √©l, ping pedrobaeza, correcto\n",
            "\n",
            "Cluster 5:\n",
            "error, factura, enviar, cliente, √©l, pos, verifactu, subir, problema, campo, diario, odoo, poner, funcionar, sif\n",
            "\n",
            "Cluster 6:\n",
            "m√≥dulo, impuesto, fecha, cambiar, iva, caso, √©l, sii, operaci√≥n, factura, casilla, deber, fecha operaci√≥n, prorrata, empresa\n",
            "Cluster 0 ‚Üí Categor√≠a asignada: Desarrollo\n",
            "Cluster 1 ‚Üí Categor√≠a asignada: Soporte\n",
            "Cluster 2 ‚Üí Categor√≠a asignada: Soporte\n",
            "Cluster 3 ‚Üí Categor√≠a asignada: Comunidad\n",
            "Cluster 4 ‚Üí Categor√≠a asignada: Comunidad\n",
            "Cluster 5 ‚Üí Categor√≠a asignada: Soporte\n",
            "Cluster 6 ‚Üí Categor√≠a asignada: Normativa\n",
            "üìÇ Preparando persistencia en el repositorio: U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "üìÇ Carpeta de datos lista en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "   ...Cargando versi√≥n desde el repositorio local.\n",
            "‚ö†Ô∏è Columnas ya existen y contienen datos, se omite la asignaci√≥n para evitar sobrescribir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 13. REINICIAR CONEXI√ìN GTIHUB\n",
        "# ==========================================\n",
        "\n",
        "# Configurar la estrategia de reconciliaci√≥n\n",
        "!git -C /content/{DEST_REPO_NAME} config pull.rebase false\n",
        "\n",
        "# Forzar una limpieza y descarga de la versi√≥n actual de GitHub\n",
        "try:\n",
        "    print(\"üîÑ Sincronizando repositorio...\")\n",
        "    !git -C /content/{DEST_REPO_NAME} fetch origin\n",
        "    !git -C /content/{DEST_REPO_NAME} reset --hard origin/main\n",
        "    print(\"‚úÖ Repositorio sincronizado y limpio.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al sincronizar: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjCMjl3oZxgm",
        "outputId": "1af1d3cb-1122-4820-f9c9-aa089bc9ef8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Sincronizando repositorio...\n",
            "HEAD is now at 55bcf71 Etiqueta Actualizada (2026-02-11 21:44)\n",
            "‚úÖ Repositorio sincronizado y limpio.\n"
          ]
        }
      ]
    }
  ]
}