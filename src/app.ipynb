{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSwYTKsFxZADnV7b1GgI9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/blob/main/src/app.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FvUeZvpmVC3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "454b09ec-f3f5-41e2-8dfc-e19d809ff3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyGithub in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: pynacl>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (1.6.2)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.32.4)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (4.15.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from PyGithub) (2.5.0)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.3)\n",
            "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pynacl>=1.4.0->PyGithub) (2.0.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.14.0->PyGithub) (2026.1.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->pynacl>=1.4.0->PyGithub) (3.0)\n",
            "Collecting es-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_lg-3.8.0/es_core_news_lg-3.8.0-py3-none-any.whl (568.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.0/568.0 MB\u001b[0m \u001b[31m800.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.12/dist-packages (2.15.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ==========================================\n",
        "# 1. INSTALACION DE LIBRERIAS\n",
        "# ==========================================\n",
        "\n",
        "!pip install PyGithub\n",
        "!python -m spacy download es_core_news_lg\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================\n",
        "# 2. IMPORTACI√ìN DE LIBRERIAS Y VARIABLES GLOBALES\n",
        "# ================================================\n",
        "\n",
        "import emoji\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import datetime\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "from google.colab import files, userdata\n",
        "from github import Github, Auth, GithubException\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import joblib\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Descargas NLTK (ejecutar una vez)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "STOP_ES = set(stopwords.words('spanish'))\n",
        "STOP_EN = set(stopwords.words('english'))\n",
        "\n",
        "nlp = spacy.blank(\"es\")\n",
        "nlp2 = spacy.load(\"es_core_news_lg\")\n",
        "\n",
        "if \"sentencizer\" not in nlp.pipe_names:\n",
        "    nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "INLINE_CODE = re.compile(r'`[^`]+`')\n",
        "FENCED_CODE = re.compile(r'```(.+?)```', re.DOTALL)\n",
        "CODE_KEYWORDS = re.compile(r'\\b(def|class|import|from|return|console\\.log|printf|#include|std::|->|=>)\\b')\n",
        "BRACES_SEMICOLON = re.compile(r'[{};=<>]')\n",
        "IMG_TAG = re.compile(r'<img\\b[^>]*>', re.IGNORECASE)\n",
        "HTML_TAG = re.compile(r'<\\/?\\w+[^>]*>', re.IGNORECASE)\n",
        "URL_LARGE = re.compile(r'https?://\\S{30,}', re.IGNORECASE)\n",
        "ASSET_DOMAINS = re.compile(r'(github\\.com|githubusercontent\\.com|assets/|cdn\\.)', re.IGNORECASE)\n",
        "\n",
        "MAPEO_LEMAS = {\n",
        "  \"hi\": \"hola\",\n",
        "  \"Ademas\": \"adem√°s\",\n",
        "  \"ademas\": \"adem√°s\",\n",
        "  \"qutarir\": \"quitar\",\n",
        "  \"qutacer\": \"quitar\",\n",
        "  \"qutaria\": \"quitar\",\n",
        "  \"qutar√≠a\": \"quitar\",\n",
        "  \"a√±adiais\": \"a√±adir\",\n",
        "  \"v√°lir\": \"v√°lido\",\n",
        "  \"gracia\": \"gracias\",\n",
        "  \"modul\": \"m√≥dulo\",\n",
        "  \"finar\": \"final\",\n",
        "  \"somar\": \"sumar\",\n",
        "  \"deveria\": \"deber\",\n",
        "  \"dever√≠a\": \"deber\",\n",
        "  \"estaria\": \"estar\",\n",
        "  \"est√°r√≠a\": \"estar\",\n",
        "  \"configurancion\": \"configuraci√≥n\",\n",
        "  \"temrinos\": \"t√©rminos\",\n",
        "  \"desp√©s\": \"despu√©s\",\n",
        "  \"propois\": \"propios\",\n",
        "  \"v√°lir\": \"valer\",\n",
        "  \"viudedad\": \"viudedad\",\n",
        "  \"reav\": \"revisar\",\n",
        "  \"qeu\": \"que\",\n",
        "  \"pusistar\": \"pusiste\",\n",
        "  \"modificaco\": \"modificado\",\n",
        "  \"desc√°rgatar\": \"descargar\",\n",
        "  \"fuentir\": \"fuente\",\n",
        "  \"invoizar\": \"facturar\",\n",
        "  \"informacion\": \"informaci√≥n\",\n",
        "  \"subsana\": \"subsanar\",\n",
        "  \"eskema\": \"esquema\",\n",
        "  }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HANBfslKmC-J",
        "outputId": "1b1fe3b9-40ce-4852-df48-a8f99813751a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Archivo de log redireccionado a: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/log_file.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-156928019.py:39: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  _log_init.write(f\"Log iniciado: {datetime.datetime.utcnow().isoformat()} UTC\\n\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===============================================\n",
        "# 3. CONEXI√ìN Y CLONADO DE REPOSITORIO DE GITHUB\n",
        "# ===============================================\n",
        "\n",
        "try:\n",
        "    TOKEN = userdata.get('GITHUB_TOKEN')\n",
        "    USER_GITHUB = \"jpmachinelearning\"\n",
        "    EMAIL_GITHUB = userdata.get('MY_EMAIL')\n",
        "\n",
        "    # Configurar identidad global de Git de una vez\n",
        "    !git config --global user.email \"{EMAIL_GITHUB}\"\n",
        "    !git config --global user.name \"{USER_GITHUB}\"\n",
        "except userdata.SecretNotFoundError:\n",
        "    print(\"‚ùå Error: Crea el secreto 'GITHUB_TOKEN' en el panel lateral.\")\n",
        "\n",
        "# --- RUTAS Y NOMBRES ---\n",
        "DEST_REPO_NAME = 'U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J'\n",
        "REPO_URL = f\"https://{TOKEN}@github.com/{USER_GITHUB}/{DEST_REPO_NAME}.git\"\n",
        "ruta_repositorio_local = f\"/content/{DEST_REPO_NAME}\"\n",
        "ruta_local_data = os.path.join(ruta_repositorio_local, 'data')\n",
        "\n",
        "# Inicializar archivo de log (sobrescribe en cada ejecuci√≥n)\n",
        "ruta_log_file_txt = os.path.join(ruta_local_data, 'log_file.txt')\n",
        "with open(ruta_log_file_txt, 'w', encoding='utf-8') as _log_init:\n",
        "    _log_init.write(f\"Log iniciado: {datetime.datetime.utcnow().isoformat()} UTC\\n\")\n",
        "    _log_init.write(\"=\"*120 + \"\\n\\n\")\n",
        "\n",
        "print(f\"üìù Archivo de log redireccionado a: {ruta_log_file_txt}\")\n",
        "\n",
        "# --- FUNCI√ìN DE UTILIDAD PARA CLONADO/SINCRO ---\n",
        "def inicializar_repositorio():\n",
        "    %cd /content/\n",
        "    if not os.path.exists(DEST_REPO_NAME):\n",
        "        print(f\"üöÄ Clonando {DEST_REPO_NAME}...\")\n",
        "        !git clone {REPO_URL}\n",
        "    else:\n",
        "        print(f\"üîÑ Sincronizando repositorio...\")\n",
        "        %cd {DEST_REPO_NAME}\n",
        "        !git pull origin main\n",
        "        %cd /content/\n",
        "    os.makedirs(os.path.dirname(ruta_local_data), exist_ok=True)\n",
        "    print(f\"üìÇ Carpeta de datos lista en: {os.path.dirname(ruta_local_data)}\")\n",
        "\n",
        "# Inicializar carpeta si no existe\n",
        "inicializar_repositorio()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbdFWfFBYb8P",
        "outputId": "93dea52f-9b09-4165-a2ca-538581703dd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2949984326.py:25: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  _log_init.write(f\"Log iniciado: {datetime.datetime.utcnow().isoformat()} UTC\\n\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Archivo de log redireccionado a: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/log_file.txt\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "üìÇ Carpeta de datos lista en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 4. FUNCIONES\n",
        "# ==========================================\n",
        "\n",
        "### ---  Funciones de para prepeocesado ---\n",
        "\n",
        "def ensure_text_input(corpus):\n",
        "    if isinstance(corpus, str):\n",
        "        return corpus\n",
        "    # pandas Series o DataFrame cell\n",
        "    if pd is not None and isinstance(corpus, pd.Series):\n",
        "        # eliminar NaN y convertir a str\n",
        "        parts = [str(x) for x in corpus.dropna().astype(str).tolist()]\n",
        "        return \"\\n\\n\".join(parts)\n",
        "    # lista/tupla de strings\n",
        "    if isinstance(corpus, (list, tuple)):\n",
        "        parts = [str(x) for x in corpus if x is not None]\n",
        "        return \"\\n\\n\".join(parts)\n",
        "    # fallback: intentar str()\n",
        "    return str(corpus)\n",
        "\n",
        "\n",
        "def apply_mapeo_and_simple_grammar(text, corrections=None):\n",
        "    \"\"\"\n",
        "    Aplica MAPEO_LEMAS sobre tokens y corrige patrones gramaticales puntuales.\n",
        "    Si se pasa 'corrections' (lista), a√±ade tuplas (original, reemplazo).\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text\n",
        "    # 1) Normalizar tokens simples usando MAPEO_LEMAS (mantener espacios)\n",
        "    def replace_token_match(m):\n",
        "        tok_orig = m.group(0)\n",
        "        tok = tok_orig.lower()\n",
        "        tok_repl = MAPEO_LEMAS.get(tok, tok)\n",
        "        # registrar si hubo cambio real y se pas√≥ lista\n",
        "        if corrections is not None and tok_repl != tok:\n",
        "            corrections.append((tok_orig, tok_repl))\n",
        "        return tok_repl\n",
        "    pattern = re.compile(r'\\b(' + '|'.join(re.escape(k) for k in MAPEO_LEMAS.keys()) + r')\\b', flags=re.IGNORECASE)\n",
        "    text = pattern.sub(lambda m: replace_token_match(m), text)\n",
        "    # 2) Reglas gramaticales puntuales (ejemplos seguros y no intrusivos)\n",
        "    #    - \"ning√∫n excepci√≥n\" -> \"ninguna excepci√≥n\"\n",
        "    def replace_ningun_excepcion(m):\n",
        "        orig = m.group(0)\n",
        "        repl = 'ninguna excepci√≥n'\n",
        "        if corrections is not None:\n",
        "            corrections.append((orig, repl))\n",
        "        return repl\n",
        "    text = re.sub(r'\\bning√∫n\\s+excepci√≥n\\b', replace_ningun_excepcion, text, flags=re.IGNORECASE)\n",
        "    #    - corregir dobles espacios\n",
        "    text = re.sub(r'\\s{2,}', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# --- Sustituciones finales robustas: URLs -> 'url' y n√∫meros -> 'num' ---\n",
        "def apply_placeholders_preserve_punct(text: str, replacements=None) -> str:\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return text\n",
        "    # URL pattern (prioriza detecci√≥n de URL completa)\n",
        "    url_pattern = re.compile(r'(?P<prefix>[\\(\\[\\{<\"\\']?)'r'(?P<url>(?:https?[:/]{0,3}|//|www\\.)'r'[A-Za-z0-9\\-]+'r'(?:\\.[A-Za-z0-9\\-]+)+'r'(?:[\\/?#][^\\s\\)\\]\\}\\>,.;:!?\"\\']*)?)'r'(?P<suffix>[\\)\\]\\}>,.;:!?\"\\']*)',flags=re.IGNORECASE)\n",
        "    def _url_sub(m):\n",
        "        orig = m.group('url')\n",
        "        if replacements is not None:\n",
        "            replacements.append((orig, 'url'))\n",
        "        return f\"{m.group('prefix')} {m.group('suffix')}\"\n",
        "    text = url_pattern.sub(_url_sub, text)\n",
        "    # N√∫mero puro: preservar prefijo/sufijo de puntuaci√≥n\n",
        "    number_pattern = re.compile(r'(?<![A-Za-z0-9])'r'(?P<prefix>[\\(\\[\\{<\"\\']?)'r'(?P<num>\\d+)'r'(?P<suffix>[\\)\\]\\}>,.;:!?\"\\']?)'r'(?![A-Za-z0-9])')\n",
        "    def _num_sub(m):\n",
        "        orig = m.group('num')\n",
        "        if replacements is not None:\n",
        "            replacements.append((orig, 'num'))\n",
        "        return f\"{m.group('prefix')} {m.group('suffix')}\"\n",
        "\n",
        "\n",
        "    text = number_pattern.sub(_num_sub, text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocesado(doc):\n",
        "  # Pipeline del prepocesado: normalizaci√≥n -> traducci√≥n de emoji -> clasificador (doble revisi√≥n 'und') / eliminaci√≥n segmentos 'en', 'und' y 'code' ->\n",
        "  # elimanci√≥n de typos -> cambio de numeros por placeholder 'num' y sitio web por 'url' ->\n",
        "  # eliminaci√≥n signos de puntuaci√≥n -> tokenizaci√≥n -> eliminaci√≥n de stopwords -> lematizaci√≥n\n",
        "\n",
        "\n",
        "  ### Normalizaci√≥n\n",
        "  #Normalizar entrada a texto (maneja None, NaN, Series, listas, etc.)\n",
        "  doc = ensure_text_input(doc) # Si queda vac√≠o, devolver resultado vac√≠o y log m√≠nimo\n",
        "  if not doc or str(doc).strip() == \"\":\n",
        "    return \"\", [[\"input_empty\"]]\n",
        "  doc = doc.lower()\n",
        "\n",
        "\n",
        "  ### Traducci√≥n de emoji\n",
        "  #doc = emoji.demojize(doc, language='es').replace(':', '').replace('_', ' ')\n",
        "\n",
        "  # 1. Comprobamos si existen emojis en el documento mediante un condicional\n",
        "  if emoji.emoji_count(doc) > 0:\n",
        "\n",
        "      # 2. Definimos una funci√≥n interna (callback) que solo procesa el emoji hallado\n",
        "      def procesar_emoji_individual(chars, data_dict):\n",
        "          # Traducimos el emoji a su nombre en espa√±ol (ej: :cara_sonriente:)\n",
        "          nombre_emoji = emoji.demojize(chars, language='es')\n",
        "\n",
        "          # 3. Limpiamos los signos SOLO del nombre generado por demojize.\n",
        "          # Esto permite que \"cara_sonriente\" pase a ser \"cara sonriente\"\n",
        "          # para que el an√°lisis de sentimiento/TF-IDF lo entienda mejor,\n",
        "          # pero sin afectar a las URLs del resto del documento.\n",
        "          nombre_limpio = nombre_emoji.replace(':', '').replace('_', ' ')\n",
        "\n",
        "          # Devolvemos el texto con espacios para que no se pegue a otras palabras\n",
        "          return f\" {nombre_limpio} \"\n",
        "\n",
        "      # 4. Aplicamos el reemplazo selectivo\n",
        "      doc = emoji.replace_emoji(doc, replace=procesar_emoji_individual)\n",
        "\n",
        "  ### Elimanci√≥n de typos\n",
        "  ortho_corrections = []\n",
        "  doc = apply_mapeo_and_simple_grammar(doc, corrections=ortho_corrections)\n",
        "\n",
        "\n",
        "  ### Clasificador (doble revision 'und') / eliminacion segmentos 'en', 'und' y 'code'\n",
        "  def looks_like_code_line(line):\n",
        "      if INLINE_CODE.search(line):\n",
        "          return True\n",
        "      if FENCED_CODE.search(line):\n",
        "          return True\n",
        "      if IMG_TAG.search(line) or HTML_TAG.search(line):\n",
        "          return True\n",
        "      if URL_LARGE.search(line) and ASSET_DOMAINS.search(line):\n",
        "          return True\n",
        "      if CODE_KEYWORDS.search(line):\n",
        "          return True\n",
        "      non_alpha = sum(1 for ch in line if not ch.isalpha() and not ch.isspace())\n",
        "      if len(line) > 0 and (non_alpha / len(line)) > 0.25:\n",
        "          return True\n",
        "      if BRACES_SEMICOLON.search(line):\n",
        "          return True\n",
        "      return False\n",
        "\n",
        "  def split_by_blank_and_guess(segment):\n",
        "      parts = []\n",
        "      groups = re.split(r'\\n\\s*\\n', segment)\n",
        "      for g in groups:\n",
        "          lines = [ln for ln in g.splitlines() if ln.strip() != \"\"]\n",
        "          if not lines:\n",
        "              continue\n",
        "          #FORZAR code si hay etiquetas img/html o URLs de assets en las l√≠neas\n",
        "          # (requiere que IMG_TAG, HTML_TAG, URL_LARGE, ASSET_DOMAINS est√©n definidos globalmente)\n",
        "          if any(IMG_TAG.search(ln) or (URL_LARGE.search(ln) and ASSET_DOMAINS.search(ln)) or HTML_TAG.search(ln) for ln in lines):\n",
        "              parts.append(('code', \"\\n\".join(lines)))\n",
        "              continue\n",
        "          code_like_count = sum(1 for ln in lines if looks_like_code_line(ln) or ln.startswith('    ') or ln.startswith('\\t'))\n",
        "          prop = code_like_count / len(lines)\n",
        "          if prop >= 0.4:\n",
        "              code_lines = []\n",
        "              text_lines = []\n",
        "              for ln in lines:\n",
        "                  tokens = [t.lower() for t in word_tokenize(ln) if t.isalpha()]\n",
        "                  if len(tokens) >= 2:\n",
        "                      es_count = sum(1 for t in tokens if t in STOP_ES)\n",
        "                      if es_count / len(tokens) >= 0.25 or re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', ln):\n",
        "                          text_lines.append(ln)\n",
        "                          continue\n",
        "                  code_lines.append(ln)\n",
        "              if code_lines:\n",
        "                  parts.append(('code', \"\\n\".join(code_lines)))\n",
        "              if text_lines:\n",
        "                  parts.append(('text', \"\\n\".join(text_lines)))\n",
        "          else:\n",
        "              parts.append(('text', \"\\n\".join(lines)))\n",
        "      return parts\n",
        "\n",
        "  def split_into_blocks_by_indentation_and_fences(corpus):\n",
        "      \"\"\"\n",
        "      Acepta corpus de cualquier tipo; primero normaliza a string.\n",
        "      Extrae fenced code y agrupa el resto por bloques separados por l√≠nea en blanco.\n",
        "      \"\"\"\n",
        "      text = ensure_text_input(corpus)\n",
        "      # --- NORMALIZACI√ìN ADICIONAL: convertir separadores en \"punto y aparte\" ---\n",
        "      # 1) [sep] o [ sep] -> punto y salto de l√≠nea doble\n",
        "      text = re.sub(r'\\[\\s*sep\\s*\\]', '.\\n\\n', text, flags=re.IGNORECASE)\n",
        "      # 2) punto seguido (\". \") -> punto y salto de l√≠nea doble\n",
        "      text = re.sub(r'\\.\\s+', '.\\n\\n', text)\n",
        "      # ------------------------------------------------------------------------\n",
        "      blocks = []\n",
        "      cursor = 0\n",
        "      for m in FENCED_CODE.finditer(text):\n",
        "          pre = text[cursor:m.start()]\n",
        "          if pre.strip():\n",
        "              blocks.extend(split_by_blank_and_guess(pre))\n",
        "          code_block = m.group(0)\n",
        "          blocks.append(('code', code_block))\n",
        "          cursor = m.end()\n",
        "      tail = text[cursor:]\n",
        "      if tail.strip():\n",
        "          blocks.extend(split_by_blank_and_guess(tail))\n",
        "      return blocks\n",
        "\n",
        "  def detect_language_by_stopwords(text, min_tokens=1):\n",
        "      \"\"\"\n",
        "      Detecta 'es', 'en', 'bilingual' o 'und'.\n",
        "      Ajustes:\n",
        "        - tokenizaci√≥n robusta (quita prefijos/sufijos no alfab√©ticos)\n",
        "        - lista blanca ampliada (incluye 'finde')\n",
        "        - heur√≠stica para frases cortas sin stopwords\n",
        "        - filtra tokens de longitud 1 al contar stopwords\n",
        "        - prioriza 'es' cuando la se√±al espa√±ola es claramente mayor\n",
        "        - condici√≥n 'bilingual' m√°s estricta\n",
        "      \"\"\"\n",
        "      txt_stripped = text.strip()\n",
        "      # caso puntual muy corto (ej. \"Gracias\" o \"Gracias.\")\n",
        "      if txt_stripped.lower() in {'gracias', 'gracias.'}:\n",
        "          return 'es'\n",
        "      # tokenizaci√≥n robusta: limpiar prefijos/sufijos no alfab√©ticos\n",
        "      raw_tokens = word_tokenize(text)\n",
        "      tokens = []\n",
        "      for t in raw_tokens:\n",
        "          t_clean = re.sub(r'^[^A-Za-z√Å√â√ç√ì√ö√ë√°√©√≠√≥√∫√±]+|[^A-Za-z√Å√â√ç√ì√ö√ë√°√©√≠√≥√∫√±]+$', '', t)\n",
        "          if t_clean:\n",
        "              tokens.append(t_clean.lower())\n",
        "      # lista blanca para singletons y sufijos t√≠picos\n",
        "      SPANISH_SINGLETONS = {'gracias','hola','adjunto','captura','saludos','ok','listo','si','no','vale','finde'}\n",
        "      SPANISH_SUFFIXES = ('ci√≥n','dad','mente','ado','ada','ico','ica','oso','osa')\n",
        "      # 1) single token common spanish\n",
        "      if len(tokens) == 1:\n",
        "          single = tokens[0]\n",
        "          if single in SPANISH_SINGLETONS or re.search(r'[√°√©√≠√≥√∫√±]', single):\n",
        "              return 'es'\n",
        "      # si hay pocos tokens, aplicar heur√≠sticas adicionales (fallback temprano)\n",
        "      if len(tokens) < min_tokens:\n",
        "          if re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', text):\n",
        "              return 'es'\n",
        "          low = text.lower()\n",
        "          for kw in ('que','para','con','por','como','ejemplo','funci√≥n','usar','gracias','alguna','novedad','adjunto','captura','finde'):\n",
        "              if kw in low:\n",
        "                  return 'es'\n",
        "          return 'und'\n",
        "      # filtrar tokens muy cortos para el conteo (evita ruido de 'a', 'I', etc.)\n",
        "      tokens_for_count = [t for t in tokens if len(t) > 1]\n",
        "      if not tokens_for_count:\n",
        "          tokens_for_count = tokens\n",
        "      # conteo de stopwords usando tokens filtrados\n",
        "      es_count = sum(1 for t in tokens_for_count if t in STOP_ES)\n",
        "      en_count = sum(1 for t in tokens_for_count if t in STOP_EN)\n",
        "      es_ratio = es_count / len(tokens_for_count) if tokens_for_count else 0\n",
        "      en_ratio = en_count / len(tokens_for_count) if tokens_for_count else 0\n",
        "      # --- NUEVA REGLA PUNTUAL: preferir 'en' si hay URL/asset y se√±al m√≠nima de ingl√©s ---\n",
        "      if (ASSET_DOMAINS.search(text) or re.search(r'https?://', text)) and en_ratio >= 0.05:\n",
        "          return 'en'\n",
        "      # ------------------------------------------------------------------------------\n",
        "      # 2) heur√≠stica para frases cortas sin stopwords (casos como \"Adjunto captura.\")\n",
        "      if len(tokens) >= 2 and es_count == 0 and en_count == 0:\n",
        "          # aceptar si hay acento en alguno, sufijo t√≠pico espa√±ol o palabra en whitelist\n",
        "          if any(re.search(r'[√°√©√≠√≥√∫√±]', t) for t in tokens):\n",
        "              return 'es'\n",
        "          if any(t in SPANISH_SINGLETONS for t in tokens):\n",
        "              return 'es'\n",
        "          if any(t.endswith(SPANISH_SUFFIXES) for t in tokens):\n",
        "              return 'es'\n",
        "          # si no hay evidencia clara de ingl√©s, asumir espa√±ol de forma conservadora\n",
        "          return 'es'\n",
        "      # Priorizar espa√±ol o ingl√©s si la se√±al es claramente mayor\n",
        "      if es_ratio > en_ratio and es_ratio >= 0.12:\n",
        "          return 'es'\n",
        "      if en_ratio > es_ratio and en_ratio >= 0.12:\n",
        "          return 'en'\n",
        "      # marcar bilingual solo si hay evidencia s√≥lida en ambos idiomas\n",
        "      if len(tokens_for_count) >= 3 and es_ratio >= 0.12 and en_ratio >= 0.12:\n",
        "          return 'bilingual'\n",
        "      # decisi√≥n por proporci√≥n de stopwords (criterio secundario)\n",
        "      if es_ratio >= 0.12 and es_ratio > en_ratio:\n",
        "          return 'es'\n",
        "      if en_ratio >= 0.12 and en_ratio > es_ratio:\n",
        "          return 'en'\n",
        "      # fallback final por acentos\n",
        "      if re.search(r'[√°√©√≠√≥√∫√±√Å√â√ç√ì√ö√ë]', text):\n",
        "          return 'es'\n",
        "      return 'und'\n",
        "\n",
        "  def classify_and_extract_strict(corpus, recursividad, window=3):\n",
        "      \"\"\"\n",
        "      Versi√≥n ajustada:\n",
        "        - Imprime la salida (category, spanish_with_code, code_blocks, items, details)\n",
        "          antes de devolver el resultado.\n",
        "        - Reconstruye (recomposici√≥n) un nuevo texto que solo incluye secciones\n",
        "          consideradas espa√±olas (lang == 'es' o 'bilingual').\n",
        "        - Para elementos eliminados (lang in {'und','code','en'} o type == 'code'),\n",
        "          imprime \"Dato borrado:\" seguido del item.\n",
        "      \"\"\"\n",
        "      log = []\n",
        "      text = ensure_text_input(corpus)\n",
        "      blocks = split_into_blocks_by_indentation_and_fences(text)\n",
        "      items = []\n",
        "      for typ, content in blocks:\n",
        "          if typ == 'code':\n",
        "              m = FENCED_CODE.search(content)\n",
        "              inner = m.group(1) if m else content\n",
        "              items.append({'type': 'code', 'text': inner.strip(), 'lang': 'code'})\n",
        "          else:\n",
        "              doc = nlp(content)\n",
        "              for sent in doc.sents:\n",
        "                  s = sent.text.strip()\n",
        "                  if not s:\n",
        "                      continue\n",
        "                  lang = detect_language_by_stopwords(s)\n",
        "                  items.append({'type': 'text', 'text': s, 'lang': lang})\n",
        "      total = len(items)\n",
        "      code_count = sum(1 for it in items if it['type'] == 'code')\n",
        "      es_count = sum(1 for it in items if it['lang'] == 'es')\n",
        "      en_count = sum(1 for it in items if it['lang'] == 'en')\n",
        "      if total == 0:\n",
        "          category = \"texto ingles 100%\"\n",
        "      else:\n",
        "          prop_code = code_count / total\n",
        "          prop_es = es_count / total\n",
        "          prop_en = en_count / total\n",
        "          if prop_code >= 0.95:\n",
        "              category = \"texto codigo de programacion 100%\"\n",
        "          elif prop_es >= 0.95 and code_count == 0:\n",
        "              category = \"texto espa√±ol 100%\"\n",
        "          elif prop_en >= 0.95 and code_count == 0:\n",
        "              category = \"texto ingles 100%\"\n",
        "          elif prop_es > 0.5 and code_count > 0:\n",
        "              category = \"texto espa√±ol + codigo de programacion\"\n",
        "          else:\n",
        "              category = \"texto bilingue\"\n",
        "      spanish_related = []\n",
        "      for i, it in enumerate(items):\n",
        "          if it['type'] == 'code':\n",
        "              start = max(0, i - window)\n",
        "              end = min(len(items) - 1, i + window)\n",
        "              for j in range(start, end + 1):\n",
        "                  cand = items[j]\n",
        "                  if cand['type'] == 'text' and cand['lang'] == 'es':\n",
        "                      spanish_related.append(cand['text'])\n",
        "      if not spanish_related:\n",
        "          tech_keywords = {'funci√≥n','variable','clase','m√©todo','archivo','ejemplo','par√°metro','argumento','instalar','importar','usar'}\n",
        "          for it in items:\n",
        "              if it['type'] == 'text' and it['lang'] == 'es':\n",
        "                  tokens = set(t.lower() for t in word_tokenize(it['text']) if t.isalpha())\n",
        "                  if tokens & tech_keywords or re.search(r'[√°√©√≠√≥√∫√±]', it['text']):\n",
        "                      spanish_related.append(it['text'])\n",
        "      seen = set()\n",
        "      spanish_with_code = []\n",
        "      for s in spanish_related:\n",
        "          if s not in seen:\n",
        "              seen.add(s)\n",
        "              spanish_with_code.append(s)\n",
        "      code_blocks = [it['text'] for it in items if it['type'] == 'code']\n",
        "      for it in items:\n",
        "          log.append(f\"{it['type']} | {it['lang']} | {it['text']}\")\n",
        "      log.append(f\"Detalles: total_items: {total} | code_items: {code_count} | spanish_items: {es_count} | english_items: {en_count}\")\n",
        "      # --- Recomposici√≥n del texto original manteniendo solo espa√±ol ---\n",
        "      # Incluir items cuya etiqueta de idioma sea 'es' o 'bilingual' y que no sean c√≥digo.\n",
        "      recomposed_parts = []\n",
        "      for it in items:\n",
        "        # condici√≥n para conservar: tipo text y lang es o bilingual\n",
        "        if recursividad:\n",
        "          if it['type'] == 'text' and it['lang'] in {'es', 'bilingual'}:\n",
        "            recomposed_parts.append(it['text'])\n",
        "          else:\n",
        "            log.append(f\"Dato borrado: {it['type']} | {it['lang']} | {it['text']}\")\n",
        "        elif it['type'] == 'text' and it['lang'] in {'es', 'bilingual', 'und'}:\n",
        "          recomposed_parts.append(it['text'])\n",
        "        else:\n",
        "          # imprimir mensaje de borrado para los elementos no conservados\n",
        "          log.append(f\"Dato borrado: {it['type']} | {it['lang']} | {it['text']}\")\n",
        "      # unir con un espacio entre oraciones/secciones\n",
        "      recomposed_text = \" \".join(p.strip() for p in recomposed_parts if p.strip())\n",
        "      log.append(f\"recomposed_text: {recomposed_text}\")\n",
        "      # devolver la recomposici√≥n y el log\n",
        "      return recomposed_text, log\n",
        "  logs_for_flat =[]\n",
        "  res1, log1 = classify_and_extract_strict(doc, recursividad=False)\n",
        "  logs_for_flat.append(log1)\n",
        "  res2, log2 = classify_and_extract_strict(res1, recursividad=True)\n",
        "  res2 = (res2 or \"\").strip()\n",
        "  # normalizar res2 (recomposed_text)\n",
        "  if not res2:\n",
        "    res2 = \"\"\n",
        "\n",
        "\n",
        "  ### Cambio de numeros por placeholder 'num' y sitio web por 'url' (desactivado)\n",
        "  placeholder_changes = []\n",
        "  res2 = apply_placeholders_preserve_punct(res2, replacements=placeholder_changes)\n",
        "\n",
        "\n",
        "  if log2:\n",
        "    logs_for_flat.append(\"__Recursividad__\")\n",
        "  logs_for_flat.append(log2)\n",
        "  # aplanar (flatten) y asegurar que cada elemento sea string y sin saltos iniciales/finales\n",
        "  logs = []\n",
        "  for part in logs_for_flat:\n",
        "      if isinstance(part, (list, tuple)):\n",
        "          for line in part:\n",
        "              logs.append(str(line).strip())\n",
        "      else:\n",
        "          logs.append(str(part).strip())\n",
        "  # eliminar entradas vac√≠as si las hubiera\n",
        "  logs = [l for l in logs if l]\n",
        "  # --- Escribir logs, cambios placeholders y ortogr√°ficos en archivo log_file.txt ---\n",
        "  timestamp = datetime.datetime.utcnow().isoformat()\n",
        "  try:\n",
        "      #ruta_log_final = os.path.join(ruta_local_data, 'log_file.txt')\n",
        "      with open(ruta_log_file_txt, 'a', encoding='utf-8') as lf:\n",
        "          # si hay logs generales ya calculados (variable logs existente), escribirlos\n",
        "          if logs:\n",
        "              lf.write(f\"# Entrada procesada {timestamp} UTC\\n\")\n",
        "              for line in logs:\n",
        "                  lf.write(line + \"\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "          # 1) Correcciones ortogr√°ficas (si las hay)\n",
        "          if ortho_corrections:\n",
        "              lf.write(f\"__Corrector Ortogr√°fico__{timestamp}\\n\")\n",
        "              for orig, repl in ortho_corrections:\n",
        "                  lf.write(f\"'{orig}' se corrige por '{repl}'\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "          # 2) Cambios por placeholders (si los hay)\n",
        "          if placeholder_changes:\n",
        "              lf.write(f\"__Cambio por Placeholder__{timestamp}\\n\")\n",
        "              for orig, repl in placeholder_changes:\n",
        "                  lf.write(f\"'{orig}' se cambio por '{repl}'\\n\")\n",
        "              lf.write(\"\\n\")\n",
        "  except Exception:\n",
        "      # no interrumpir el pipeline por errores de logging\n",
        "      pass\n",
        "\n",
        "  def limpieza_selectiva_tecnica(doc):\n",
        "      \"\"\"\n",
        "      Limpia signos de puntuaci√≥n respetando:\n",
        "      - URLs completas (https://www.aeat.es)\n",
        "      - Versiones (16.0, 17.0.1)\n",
        "      - Guiones entre texto/n√∫meros (l10n-es, factura-e, v18-beta)\n",
        "      \"\"\"\n",
        "      # 1. Normalizaci√≥n de espacios y saltos de l√≠nea\n",
        "      doc = doc.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "      # 2. PROTEGER URLs\n",
        "      urls = re.findall(r'https?://\\S+', doc)\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(url, f' TOKEN_URL_{i} ')\n",
        "      # 3. PROTEGER VERSIONES (Puntos entre n√∫meros)\n",
        "      versiones = re.findall(r'\\d+\\.\\d+(?:\\.\\d+)*', doc)\n",
        "      for i, ver in enumerate(versiones):\n",
        "          doc = doc.replace(ver, f' TOKEN_VER_{i} ')\n",
        "      # 4. LIMPIEZA DE SIGNOS (Excepto el guion por ahora)\n",
        "      # Borramos: ¬° ! ¬ø ? [ ] ( ) { } | # * , ; : \" ' ¬ª ¬´ @\n",
        "      signos_a_borrar = r'[¬°!¬ø?\\[\\](){}|#*,\\;:\\\"\\'¬ª¬´@]'\n",
        "      doc = re.sub(signos_a_borrar, ' ', doc)\n",
        "      # 5. TRATAMIENTO DE PUNTOS Y GUIONES RESTANTES\n",
        "      # Borrar puntos que no protegimos (puntos al final de frase)\n",
        "      doc = re.sub(r'\\.', ' ', doc)\n",
        "      # Borrar guiones SOLO si NO est√°n entre letras o n√∫meros\n",
        "      # (Borra guiones decorativos pero mantiene l10n-es o factura-e)\n",
        "      doc = re.sub(r'(?<![a-zA-Z0-9])-|-(?![a-zA-Z0-9])', ' ', doc)\n",
        "      # 6. RESTAURAR URLs Y VERSIONES\n",
        "      for i, ver in enumerate(versiones):\n",
        "          doc = doc.replace(f' TOKEN_VER_{i} ', f' {ver} ')\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(f' TOKEN_URL_{i} ', f' {url} ')\n",
        "      # 7. COLAPSAR ESPACIOS\n",
        "      doc = re.sub(r'\\s+', ' ', doc).strip()\n",
        "      return doc\n",
        "\n",
        "\n",
        "  ### Eliminaci√≥n signos de puntuaci√≥n\n",
        "  def limpieza_selectiva_tecnica(doc):\n",
        "      \"\"\"\n",
        "      Mantiene puntos en todo el texto (para extensiones y URLs).\n",
        "      Mantiene dos puntos (:) SOLO en URLs.\n",
        "      Elimina el resto de s√≠mbolos t√©cnicos innecesarios.\n",
        "      \"\"\"\n",
        "      if not doc:\n",
        "          return \"\"\n",
        "      # 1. Normalizaci√≥n de espacios y reparaci√≥n de protocolo\n",
        "      doc = doc.replace('\\\\n', ' ').replace('\\n', ' ')\n",
        "      # Reparar URLs mal escritas (ej: https// -> https://)\n",
        "      doc = re.sub(r'(https?)(//)', r'\\1:\\2', doc)\n",
        "      # 2. PROTEGER URLs (Capa de Seguridad M√°xima)\n",
        "      # Esto captura la URL completa con sus : / y .\n",
        "      urls = re.findall(r'https?://\\S+', doc)\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(url, f' TOKEN_URL_{i} ')\n",
        "      # 3. PROTEGER N√öMEROS T√âCNICOS (Versiones y porcentajes)\n",
        "      # Ej: 16.0, 1.0.1, 10%\n",
        "      nums_tecnicos = re.findall(r'\\d+[.,]\\d+(?:\\.\\d+)*%?', doc)\n",
        "      for i, nt in enumerate(nums_tecnicos):\n",
        "          doc = doc.replace(nt, f' TOKEN_NUM_{i} ')\n",
        "      # 4. LIMPIEZA SELECTIVA DE S√çMBOLOS\n",
        "      # Eliminamos los dos puntos (:) y la barra (/) aqu√≠ porque las URLs ya est√°n protegidas.\n",
        "      # MANTENEMOS el punto (.) por instrucci√≥n directa.\n",
        "      signos_a_borrar = r'[:/=<>\\\\[\\]{}()|#@*+;\\\"\\'¬´¬ª`‚Ä¢]'\n",
        "      doc = re.sub(signos_a_borrar, ' ', doc)\n",
        "      # 5. TRATAMIENTO DE CONECTORES (Guion y Guion Bajo)\n",
        "      # Se mantienen solo si est√°n entre letras o n√∫meros (ej: l10n_es, factura-e)\n",
        "      # Se borran si son decorativos o est√°n sueltos.\n",
        "      doc = re.sub(r'(?<![a-zA-Z0-9])[-_]|[-_](?![a-zA-Z0-9])', ' ', doc)\n",
        "      # 6. RESTAURACI√ìN\n",
        "      # Devolvemos las URLs y N√∫meros a su sitio original intactos\n",
        "      for i, nt in enumerate(nums_tecnicos):\n",
        "          doc = doc.replace(f' TOKEN_NUM_{i} ', f' {nt} ')\n",
        "      for i, url in enumerate(urls):\n",
        "          doc = doc.replace(f' TOKEN_URL_{i} ', f' {url} ')\n",
        "      # 7. COLAPSAR ESPACIOS\n",
        "      doc = re.sub(r'\\s+', ' ', doc).strip()\n",
        "      return doc\n",
        "\n",
        "  #Llamado de funci√≥n para quitar selectivamente signos de puntuaci√≥n\n",
        "  doc = limpieza_selectiva_tecnica(res2)\n",
        "\n",
        "\n",
        "  ### Tokenizaci√≥n\n",
        "  doc = nlp2(doc)\n",
        "\n",
        "\n",
        "  ### Eliminaci√≥n de stopwords\n",
        "  doc = [token for token in doc if not token.is_stop and not token.is_punct and not token.is_space] #[token for token in doc if not token.is_stop]\n",
        "\n",
        "\n",
        "  ### Lematizaci√≥n\n",
        "  doc = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "  return doc\n",
        "\n",
        "\n",
        "### --- FUNCI√ìN GLOBAL DE CARGA ---\n",
        "def sincronizar_con_github(repo_path, lista_archivos, mensaje_commit):\n",
        "    \"\"\"\n",
        "    Sincroniza y sube archivos espec√≠ficos al repositorio de GitHub.\n",
        "    repo_path: Ruta local del repositorio (ej. /content/Nombre_Repo)\n",
        "    lista_archivos: Lista de nombres de archivos o rutas relativas dentro del repo.\n",
        "    mensaje_commit: Texto descriptivo del cambio.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(repo_path):\n",
        "            raise Exception(f\"La ruta del repositorio {repo_path} no existe.\")\n",
        "\n",
        "        %cd {repo_path}\n",
        "\n",
        "        # 1. Preparar archivos (Staging)\n",
        "        archivos_a√±adidos = 0\n",
        "        for archivo in lista_archivos:\n",
        "            # Si el usuario solo pasa el nombre, buscamos en data/\n",
        "            # Si pasa una ruta relativa, la respetamos.\n",
        "            ruta_git = archivo if '/' in archivo else f\"data/{archivo}\"\n",
        "\n",
        "            # Verificamos que el archivo exista antes de intentar a√±adirlo\n",
        "            if os.path.exists(os.path.join(repo_path, ruta_git)):\n",
        "                !git add {ruta_git}\n",
        "                archivos_a√±adidos += 1\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Advertencia: No se encontr√≥ el archivo {ruta_git}\")\n",
        "\n",
        "        if archivos_a√±adidos == 0:\n",
        "            print(\"‚ùå No se encontraron archivos v√°lidos para subir.\")\n",
        "            return\n",
        "\n",
        "        # 2. Commit con marca de tiempo autom√°tica si no se provee mensaje detallado\n",
        "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "        full_message = f\"{mensaje_commit} ({timestamp})\"\n",
        "\n",
        "        # El comando commit devuelve error si no hay cambios, usamos try interno\n",
        "        !git commit -m \"{full_message}\"\n",
        "\n",
        "        # 3. Pull y Push\n",
        "        print(f\"üöÄ Enviando {archivos_a√±adidos} archivo(s) a GitHub...\")\n",
        "        !git pull --rebase -X ours origin main\n",
        "        !git push origin main\n",
        "        print(f\"‚úÖ Sincronizaci√≥n exitosa.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante la sincronizaci√≥n: {e}\")\n",
        "    finally:\n",
        "        %cd /content/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### --- BORRAR ---\n",
        "def subir_archivos_a_github(lista_archivos, mensaje_commit):\n",
        "    \"\"\"\n",
        "    Recibe una lista de nombres de archivos en /content/ y los sube al repo.\n",
        "    Ejemplo: subir_archivos_a_github(['modelo.pkl', 'log.txt'], \"Update\")\n",
        "    \"\"\"\n",
        "    repo_path = f\"/content/{DEST_REPO_NAME}\"\n",
        "    data_folder = os.path.join(repo_path, \"data\")\n",
        "    os.makedirs(data_folder, exist_ok=True)\n",
        "\n",
        "    archivos_encontrados = []\n",
        "\n",
        "    # 1. Copiar archivos al repo local\n",
        "    for archivo in lista_archivos:\n",
        "        ruta_origen = f\"/content/{archivo}\"\n",
        "        if os.path.exists(ruta_origen):\n",
        "            !cp {ruta_origen} {data_folder}/\n",
        "            archivos_encontrados.append(f\"data/{archivo}\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Advertencia: No se encontr√≥ {archivo}\")\n",
        "\n",
        "    if not archivos_encontrados:\n",
        "        print(\"‚ùå Error: No hay archivos v√°lidos para subir.\")\n",
        "        return\n",
        "\n",
        "    # 2. Ejecutar Git\n",
        "    try:\n",
        "        %cd {repo_path}\n",
        "        for path_git in archivos_encontrados:\n",
        "            !git add {path_git}\n",
        "\n",
        "        # Evitar error si no hay cambios reales\n",
        "        !git commit -m \"{mensaje_commit}\"\n",
        "\n",
        "        print(f\"üîÑ Sincronizando y subiendo: {', '.join(archivos_encontrados)}...\")\n",
        "        !git pull --rebase -X ours origin main\n",
        "        !git push origin main\n",
        "        print(f\"‚úÖ ¬°√âxito! Archivos subidos a GitHub.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error durante el push: {e}\")\n",
        "    finally:\n",
        "        %cd /content/"
      ],
      "metadata": {
        "id": "7e2s7yqa_4kc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 5. EXTRACCI√ìN Y SUBIDA DE DATASET\n",
        "# ==========================================\n",
        "\n",
        "REPO_NAME_EXTRACCION = 'OCA/l10n-spain'\n",
        "LIMITE_REGISTROS = 200\n",
        "\n",
        "try:\n",
        "    auth = Auth.Token(TOKEN)\n",
        "    g = Github(auth=auth)\n",
        "    repo = g.get_repo(REPO_NAME_EXTRACCION)\n",
        "    dataset = []\n",
        "\n",
        "    # --- EXTRACCI√ìN DE ISSUES (RF-01) ---\n",
        "\n",
        "    print(f\"\\n--- Extrayendo hasta {LIMITE_REGISTROS} Issues ---\")\n",
        "    issues = repo.get_issues(state='all', sort='created', direction='desc')\n",
        "    count = 0\n",
        "\n",
        "    for item in issues:\n",
        "        if count >= LIMITE_REGISTROS: break\n",
        "        if item.pull_request is not None: continue\n",
        "\n",
        "        print(f\"üì• Procesando Issue #{item.number}...\")\n",
        "\n",
        "        # Recolectar t√≠tulo, descripci√≥n y comentarios\n",
        "        titulo = item.title or \"\"\n",
        "        descripcion = item.body or \"\"\n",
        "        comments = [c.body for c in item.get_comments() if c.body]\n",
        "\n",
        "        # L√≥gica modificada: Una fila por cada comentario\n",
        "        if comments:\n",
        "            for comm in comments:\n",
        "                dataset.append({\n",
        "                    'tipo': 'Issue',\n",
        "                    'numero': item.number,\n",
        "                    'titulo': titulo,\n",
        "                    'descripcion': descripcion,\n",
        "                    'comentarios': comm  # Comentario individual\n",
        "                })\n",
        "        else:\n",
        "            # Si no hay comentarios, guardamos el Issue de todas formas\n",
        "            dataset.append({\n",
        "                'tipo': 'Issue',\n",
        "                'numero': item.number,\n",
        "                'titulo': titulo,\n",
        "                'descripcion': descripcion,\n",
        "                'comentarios': \"\"\n",
        "            })\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # --- EXTRACCI√ìN DE PULL REQUESTS (RF-01) ---\n",
        "    print(f\"\\n--- Extrayendo hasta {LIMITE_REGISTROS} Pull Requests ---\")\n",
        "    pulls = repo.get_pulls(state='all', sort='created', direction='desc')\n",
        "    count = 0\n",
        "\n",
        "    for pr in pulls:\n",
        "        if count >= LIMITE_REGISTROS: break\n",
        "\n",
        "        print(f\"üì• Procesando PR #{pr.number}...\")\n",
        "\n",
        "        # Recolectar t√≠tulo, descripci√≥n y comentarios asociados\n",
        "        titulo = pr.title or \"\"\n",
        "        descripcion = pr.body or \"\"\n",
        "        comments = [c.body for c in pr.get_issue_comments() if c.body]\n",
        "\n",
        "        # L√≥gica modificada: Una fila por cada comentario\n",
        "        if comments:\n",
        "            for comm in comments:\n",
        "                dataset.append({\n",
        "                    'tipo': 'Pull Request',\n",
        "                    'numero': pr.number,\n",
        "                    'titulo': titulo,\n",
        "                    'descripcion': descripcion,\n",
        "                    'comentarios': comm  # Comentario individual\n",
        "                })\n",
        "        else:\n",
        "            # Si no hay comentarios, guardamos el PR de todas formas\n",
        "            dataset.append({\n",
        "                'tipo': 'Pull Request',\n",
        "                'numero': pr.number,\n",
        "                'titulo': titulo,\n",
        "                'descripcion': descripcion,\n",
        "                'comentarios': \"\"\n",
        "            })\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    # --- SECCI√ìN DE GUARDADO INTEGRADA ---\n",
        "    if dataset:\n",
        "        df = pd.DataFrame(dataset)\n",
        "        #Guardamos el archivo en la ra√≠z para que la funci√≥n modular lo reconozca\n",
        "        datos_github_csv = 'datos_github.csv'\n",
        "        ruta_datos_github_csv = os.path.join(ruta_local_data, datos_github_csv)\n",
        "        #Guardar localmente en la carpeta data del repo\n",
        "        df.to_csv(ruta_datos_github_csv, index=False, encoding='utf-8-sig')\n",
        "        print(f\"üìÑ Archivo generado en: {ruta_datos_github_csv}\")\n",
        "        #Sincronizaci√≥n con GitHub (Commit y Push)\n",
        "        sincronizar_con_github(ruta_local_data, [datos_github_csv], \"Actualizaci√≥n autom√°tica de extracci√≥n inicial\")\n",
        "        print(\"‚úÖ Proceso de extracci√≥n, guardado y push finalizado.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No se generaron datos para subir.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en Celda de Extracci√≥n: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3T296idgcR0",
        "outputId": "9bf707cc-3d9b-4766-f6f4-fd1dd3fdeb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Extrayendo hasta 200 Issues ---\n",
            "üì• Procesando Issue #4793...\n",
            "üì• Procesando Issue #4760...\n",
            "üì• Procesando Issue #4743...\n",
            "üì• Procesando Issue #4681...\n",
            "üì• Procesando Issue #4677...\n",
            "üì• Procesando Issue #4666...\n",
            "üì• Procesando Issue #4652...\n",
            "üì• Procesando Issue #4650...\n",
            "üì• Procesando Issue #4649...\n",
            "üì• Procesando Issue #4642...\n",
            "üì• Procesando Issue #4641...\n",
            "üì• Procesando Issue #4639...\n",
            "üì• Procesando Issue #4637...\n",
            "üì• Procesando Issue #4632...\n",
            "üì• Procesando Issue #4631...\n",
            "üì• Procesando Issue #4626...\n",
            "üì• Procesando Issue #4623...\n",
            "üì• Procesando Issue #4620...\n",
            "üì• Procesando Issue #4618...\n",
            "üì• Procesando Issue #4615...\n",
            "üì• Procesando Issue #4613...\n",
            "üì• Procesando Issue #4605...\n",
            "üì• Procesando Issue #4604...\n",
            "üì• Procesando Issue #4603...\n",
            "üì• Procesando Issue #4596...\n",
            "üì• Procesando Issue #4591...\n",
            "üì• Procesando Issue #4589...\n",
            "üì• Procesando Issue #4586...\n",
            "üì• Procesando Issue #4550...\n",
            "üì• Procesando Issue #4534...\n",
            "üì• Procesando Issue #4507...\n",
            "üì• Procesando Issue #4497...\n",
            "üì• Procesando Issue #4489...\n",
            "üì• Procesando Issue #4472...\n",
            "üì• Procesando Issue #4468...\n",
            "üì• Procesando Issue #4467...\n",
            "üì• Procesando Issue #4463...\n",
            "üì• Procesando Issue #4462...\n",
            "üì• Procesando Issue #4457...\n",
            "üì• Procesando Issue #4444...\n",
            "üì• Procesando Issue #4435...\n",
            "üì• Procesando Issue #4430...\n",
            "üì• Procesando Issue #4428...\n",
            "üì• Procesando Issue #4425...\n",
            "üì• Procesando Issue #4424...\n",
            "üì• Procesando Issue #4423...\n",
            "üì• Procesando Issue #4421...\n",
            "üì• Procesando Issue #4420...\n",
            "üì• Procesando Issue #4419...\n",
            "üì• Procesando Issue #4416...\n",
            "üì• Procesando Issue #4402...\n",
            "üì• Procesando Issue #4397...\n",
            "üì• Procesando Issue #4391...\n",
            "üì• Procesando Issue #4390...\n",
            "üì• Procesando Issue #4389...\n",
            "üì• Procesando Issue #4387...\n",
            "üì• Procesando Issue #4385...\n",
            "üì• Procesando Issue #4381...\n",
            "üì• Procesando Issue #4376...\n",
            "üì• Procesando Issue #4375...\n",
            "üì• Procesando Issue #4374...\n",
            "üì• Procesando Issue #4372...\n",
            "üì• Procesando Issue #4368...\n",
            "üì• Procesando Issue #4366...\n",
            "üì• Procesando Issue #4358...\n",
            "üì• Procesando Issue #4341...\n",
            "üì• Procesando Issue #4338...\n",
            "üì• Procesando Issue #4302...\n",
            "üì• Procesando Issue #4291...\n",
            "üì• Procesando Issue #4289...\n",
            "üì• Procesando Issue #4286...\n",
            "üì• Procesando Issue #4272...\n",
            "üì• Procesando Issue #4271...\n",
            "üì• Procesando Issue #4262...\n",
            "üì• Procesando Issue #4237...\n",
            "üì• Procesando Issue #4223...\n",
            "üì• Procesando Issue #4213...\n",
            "üì• Procesando Issue #4211...\n",
            "üì• Procesando Issue #4207...\n",
            "üì• Procesando Issue #4206...\n",
            "üì• Procesando Issue #4198...\n",
            "üì• Procesando Issue #4197...\n",
            "üì• Procesando Issue #4196...\n",
            "üì• Procesando Issue #4185...\n",
            "üì• Procesando Issue #4178...\n",
            "üì• Procesando Issue #4173...\n",
            "üì• Procesando Issue #4169...\n",
            "üì• Procesando Issue #4165...\n",
            "üì• Procesando Issue #4155...\n",
            "üì• Procesando Issue #4140...\n",
            "üì• Procesando Issue #4138...\n",
            "üì• Procesando Issue #4115...\n",
            "üì• Procesando Issue #4110...\n",
            "üì• Procesando Issue #4103...\n",
            "üì• Procesando Issue #4102...\n",
            "üì• Procesando Issue #4100...\n",
            "üì• Procesando Issue #4094...\n",
            "üì• Procesando Issue #4092...\n",
            "üì• Procesando Issue #4085...\n",
            "üì• Procesando Issue #4077...\n",
            "üì• Procesando Issue #4060...\n",
            "üì• Procesando Issue #4051...\n",
            "üì• Procesando Issue #4042...\n",
            "üì• Procesando Issue #4039...\n",
            "üì• Procesando Issue #4024...\n",
            "üì• Procesando Issue #4020...\n",
            "üì• Procesando Issue #4017...\n",
            "üì• Procesando Issue #4016...\n",
            "üì• Procesando Issue #4008...\n",
            "üì• Procesando Issue #3999...\n",
            "üì• Procesando Issue #3995...\n",
            "üì• Procesando Issue #3984...\n",
            "üì• Procesando Issue #3971...\n",
            "üì• Procesando Issue #3970...\n",
            "üì• Procesando Issue #3961...\n",
            "üì• Procesando Issue #3942...\n",
            "üì• Procesando Issue #3941...\n",
            "üì• Procesando Issue #3900...\n",
            "üì• Procesando Issue #3896...\n",
            "üì• Procesando Issue #3886...\n",
            "üì• Procesando Issue #3881...\n",
            "üì• Procesando Issue #3876...\n",
            "üì• Procesando Issue #3872...\n",
            "üì• Procesando Issue #3868...\n",
            "üì• Procesando Issue #3864...\n",
            "üì• Procesando Issue #3862...\n",
            "üì• Procesando Issue #3861...\n",
            "üì• Procesando Issue #3858...\n",
            "üì• Procesando Issue #3856...\n",
            "üì• Procesando Issue #3851...\n",
            "üì• Procesando Issue #3842...\n",
            "üì• Procesando Issue #3831...\n",
            "üì• Procesando Issue #3829...\n",
            "üì• Procesando Issue #3826...\n",
            "üì• Procesando Issue #3823...\n",
            "üì• Procesando Issue #3812...\n",
            "üì• Procesando Issue #3806...\n",
            "üì• Procesando Issue #3799...\n",
            "üì• Procesando Issue #3790...\n",
            "üì• Procesando Issue #3787...\n",
            "üì• Procesando Issue #3786...\n",
            "üì• Procesando Issue #3754...\n",
            "üì• Procesando Issue #3751...\n",
            "üì• Procesando Issue #3730...\n",
            "üì• Procesando Issue #3721...\n",
            "üì• Procesando Issue #3720...\n",
            "üì• Procesando Issue #3718...\n",
            "üì• Procesando Issue #3703...\n",
            "üì• Procesando Issue #3700...\n",
            "üì• Procesando Issue #3699...\n",
            "üì• Procesando Issue #3698...\n",
            "üì• Procesando Issue #3690...\n",
            "üì• Procesando Issue #3689...\n",
            "üì• Procesando Issue #3688...\n",
            "üì• Procesando Issue #3682...\n",
            "üì• Procesando Issue #3680...\n",
            "üì• Procesando Issue #3676...\n",
            "üì• Procesando Issue #3670...\n",
            "üì• Procesando Issue #3669...\n",
            "üì• Procesando Issue #3668...\n",
            "üì• Procesando Issue #3660...\n",
            "üì• Procesando Issue #3650...\n",
            "üì• Procesando Issue #3647...\n",
            "üì• Procesando Issue #3641...\n",
            "üì• Procesando Issue #3636...\n",
            "üì• Procesando Issue #3627...\n",
            "üì• Procesando Issue #3624...\n",
            "üì• Procesando Issue #3622...\n",
            "üì• Procesando Issue #3610...\n",
            "üì• Procesando Issue #3607...\n",
            "üì• Procesando Issue #3605...\n",
            "üì• Procesando Issue #3589...\n",
            "üì• Procesando Issue #3565...\n",
            "üì• Procesando Issue #3560...\n",
            "üì• Procesando Issue #3552...\n",
            "üì• Procesando Issue #3551...\n",
            "üì• Procesando Issue #3539...\n",
            "üì• Procesando Issue #3538...\n",
            "üì• Procesando Issue #3533...\n",
            "üì• Procesando Issue #3527...\n",
            "üì• Procesando Issue #3522...\n",
            "üì• Procesando Issue #3521...\n",
            "üì• Procesando Issue #3520...\n",
            "üì• Procesando Issue #3518...\n",
            "üì• Procesando Issue #3514...\n",
            "üì• Procesando Issue #3507...\n",
            "üì• Procesando Issue #3500...\n",
            "üì• Procesando Issue #3483...\n",
            "üì• Procesando Issue #3482...\n",
            "üì• Procesando Issue #3480...\n",
            "üì• Procesando Issue #3475...\n",
            "üì• Procesando Issue #3474...\n",
            "üì• Procesando Issue #3459...\n",
            "üì• Procesando Issue #3456...\n",
            "üì• Procesando Issue #3455...\n",
            "üì• Procesando Issue #3452...\n",
            "üì• Procesando Issue #3451...\n",
            "üì• Procesando Issue #3450...\n",
            "üì• Procesando Issue #3449...\n",
            "üì• Procesando Issue #3448...\n",
            "\n",
            "--- Extrayendo hasta 200 Pull Requests ---\n",
            "üì• Procesando PR #4809...\n",
            "üì• Procesando PR #4808...\n",
            "üì• Procesando PR #4807...\n",
            "üì• Procesando PR #4806...\n",
            "üì• Procesando PR #4805...\n",
            "üì• Procesando PR #4804...\n",
            "üì• Procesando PR #4803...\n",
            "üì• Procesando PR #4802...\n",
            "üì• Procesando PR #4801...\n",
            "üì• Procesando PR #4800...\n",
            "üì• Procesando PR #4799...\n",
            "üì• Procesando PR #4798...\n",
            "üì• Procesando PR #4797...\n",
            "üì• Procesando PR #4796...\n",
            "üì• Procesando PR #4795...\n",
            "üì• Procesando PR #4794...\n",
            "üì• Procesando PR #4792...\n",
            "üì• Procesando PR #4791...\n",
            "üì• Procesando PR #4790...\n",
            "üì• Procesando PR #4789...\n",
            "üì• Procesando PR #4788...\n",
            "üì• Procesando PR #4787...\n",
            "üì• Procesando PR #4786...\n",
            "üì• Procesando PR #4785...\n",
            "üì• Procesando PR #4784...\n",
            "üì• Procesando PR #4783...\n",
            "üì• Procesando PR #4782...\n",
            "üì• Procesando PR #4781...\n",
            "üì• Procesando PR #4780...\n",
            "üì• Procesando PR #4779...\n",
            "üì• Procesando PR #4778...\n",
            "üì• Procesando PR #4777...\n",
            "üì• Procesando PR #4776...\n",
            "üì• Procesando PR #4775...\n",
            "üì• Procesando PR #4774...\n",
            "üì• Procesando PR #4773...\n",
            "üì• Procesando PR #4772...\n",
            "üì• Procesando PR #4771...\n",
            "üì• Procesando PR #4770...\n",
            "üì• Procesando PR #4769...\n",
            "üì• Procesando PR #4768...\n",
            "üì• Procesando PR #4767...\n",
            "üì• Procesando PR #4766...\n",
            "üì• Procesando PR #4765...\n",
            "üì• Procesando PR #4764...\n",
            "üì• Procesando PR #4763...\n",
            "üì• Procesando PR #4762...\n",
            "üì• Procesando PR #4761...\n",
            "üì• Procesando PR #4759...\n",
            "üì• Procesando PR #4758...\n",
            "üì• Procesando PR #4757...\n",
            "üì• Procesando PR #4756...\n",
            "üì• Procesando PR #4755...\n",
            "üì• Procesando PR #4754...\n",
            "üì• Procesando PR #4753...\n",
            "üì• Procesando PR #4752...\n",
            "üì• Procesando PR #4751...\n",
            "üì• Procesando PR #4750...\n",
            "üì• Procesando PR #4749...\n",
            "üì• Procesando PR #4748...\n",
            "üì• Procesando PR #4747...\n",
            "üì• Procesando PR #4746...\n",
            "üì• Procesando PR #4745...\n",
            "üì• Procesando PR #4744...\n",
            "üì• Procesando PR #4742...\n",
            "üì• Procesando PR #4741...\n",
            "üì• Procesando PR #4740...\n",
            "üì• Procesando PR #4739...\n",
            "üì• Procesando PR #4738...\n",
            "üì• Procesando PR #4737...\n",
            "üì• Procesando PR #4736...\n",
            "üì• Procesando PR #4735...\n",
            "üì• Procesando PR #4734...\n",
            "üì• Procesando PR #4733...\n",
            "üì• Procesando PR #4732...\n",
            "üì• Procesando PR #4731...\n",
            "üì• Procesando PR #4730...\n",
            "üì• Procesando PR #4729...\n",
            "üì• Procesando PR #4728...\n",
            "üì• Procesando PR #4727...\n",
            "üì• Procesando PR #4726...\n",
            "üì• Procesando PR #4725...\n",
            "üì• Procesando PR #4724...\n",
            "üì• Procesando PR #4723...\n",
            "üì• Procesando PR #4722...\n",
            "üì• Procesando PR #4721...\n",
            "üì• Procesando PR #4720...\n",
            "üì• Procesando PR #4719...\n",
            "üì• Procesando PR #4718...\n",
            "üì• Procesando PR #4717...\n",
            "üì• Procesando PR #4716...\n",
            "üì• Procesando PR #4715...\n",
            "üì• Procesando PR #4714...\n",
            "üì• Procesando PR #4713...\n",
            "üì• Procesando PR #4712...\n",
            "üì• Procesando PR #4711...\n",
            "üì• Procesando PR #4710...\n",
            "üì• Procesando PR #4709...\n",
            "üì• Procesando PR #4708...\n",
            "üì• Procesando PR #4707...\n",
            "üì• Procesando PR #4706...\n",
            "üì• Procesando PR #4705...\n",
            "üì• Procesando PR #4704...\n",
            "üì• Procesando PR #4703...\n",
            "üì• Procesando PR #4702...\n",
            "üì• Procesando PR #4701...\n",
            "üì• Procesando PR #4700...\n",
            "üì• Procesando PR #4699...\n",
            "üì• Procesando PR #4698...\n",
            "üì• Procesando PR #4697...\n",
            "üì• Procesando PR #4696...\n",
            "üì• Procesando PR #4695...\n",
            "üì• Procesando PR #4694...\n",
            "üì• Procesando PR #4693...\n",
            "üì• Procesando PR #4692...\n",
            "üì• Procesando PR #4691...\n",
            "üì• Procesando PR #4690...\n",
            "üì• Procesando PR #4689...\n",
            "üì• Procesando PR #4688...\n",
            "üì• Procesando PR #4687...\n",
            "üì• Procesando PR #4686...\n",
            "üì• Procesando PR #4685...\n",
            "üì• Procesando PR #4684...\n",
            "üì• Procesando PR #4683...\n",
            "üì• Procesando PR #4682...\n",
            "üì• Procesando PR #4680...\n",
            "üì• Procesando PR #4679...\n",
            "üì• Procesando PR #4676...\n",
            "üì• Procesando PR #4675...\n",
            "üì• Procesando PR #4674...\n",
            "üì• Procesando PR #4673...\n",
            "üì• Procesando PR #4672...\n",
            "üì• Procesando PR #4671...\n",
            "üì• Procesando PR #4670...\n",
            "üì• Procesando PR #4669...\n",
            "üì• Procesando PR #4668...\n",
            "üì• Procesando PR #4667...\n",
            "üì• Procesando PR #4665...\n",
            "üì• Procesando PR #4664...\n",
            "üì• Procesando PR #4663...\n",
            "üì• Procesando PR #4662...\n",
            "üì• Procesando PR #4661...\n",
            "üì• Procesando PR #4660...\n",
            "üì• Procesando PR #4659...\n",
            "üì• Procesando PR #4658...\n",
            "üì• Procesando PR #4656...\n",
            "üì• Procesando PR #4655...\n",
            "üì• Procesando PR #4654...\n",
            "üì• Procesando PR #4653...\n",
            "üì• Procesando PR #4651...\n",
            "üì• Procesando PR #4648...\n",
            "üì• Procesando PR #4647...\n",
            "üì• Procesando PR #4646...\n",
            "üì• Procesando PR #4645...\n",
            "üì• Procesando PR #4644...\n",
            "üì• Procesando PR #4640...\n",
            "üì• Procesando PR #4638...\n",
            "üì• Procesando PR #4636...\n",
            "üì• Procesando PR #4635...\n",
            "üì• Procesando PR #4634...\n",
            "üì• Procesando PR #4633...\n",
            "üì• Procesando PR #4630...\n",
            "üì• Procesando PR #4629...\n",
            "üì• Procesando PR #4627...\n",
            "üì• Procesando PR #4625...\n",
            "üì• Procesando PR #4624...\n",
            "üì• Procesando PR #4622...\n",
            "üì• Procesando PR #4619...\n",
            "üì• Procesando PR #4617...\n",
            "üì• Procesando PR #4616...\n",
            "üì• Procesando PR #4614...\n",
            "üì• Procesando PR #4612...\n",
            "üì• Procesando PR #4611...\n",
            "üì• Procesando PR #4610...\n",
            "üì• Procesando PR #4609...\n",
            "üì• Procesando PR #4608...\n",
            "üì• Procesando PR #4607...\n",
            "üì• Procesando PR #4606...\n",
            "üì• Procesando PR #4602...\n",
            "üì• Procesando PR #4601...\n",
            "üì• Procesando PR #4600...\n",
            "üì• Procesando PR #4599...\n",
            "üì• Procesando PR #4598...\n",
            "üì• Procesando PR #4595...\n",
            "üì• Procesando PR #4594...\n",
            "üì• Procesando PR #4593...\n",
            "üì• Procesando PR #4592...\n",
            "üì• Procesando PR #4590...\n",
            "üì• Procesando PR #4588...\n",
            "üì• Procesando PR #4587...\n",
            "üì• Procesando PR #4585...\n",
            "üì• Procesando PR #4584...\n",
            "üì• Procesando PR #4583...\n",
            "üì• Procesando PR #4582...\n",
            "üì• Procesando PR #4581...\n",
            "üì• Procesando PR #4580...\n",
            "üì• Procesando PR #4579...\n",
            "üì• Procesando PR #4577...\n",
            "üì• Procesando PR #4576...\n",
            "üì• Procesando PR #4575...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "[main 6f20901] Auto-update dataset: 2026-02-10 03:09\n",
            " 1 file changed, 24221 insertions(+), 6219 deletions(-)\n",
            "üîÑ Sincronizando y subiendo: data/datos_github.csv...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 1.10 KiB | 102.00 KiB/s, done.\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   9174bc7..e73aa99  main       -> origin/main\n",
            "\u001b[KSuccessfully rebased and updated refs/heads/main.\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 257.05 KiB | 3.38 MiB/s, done.\n",
            "Total 4 (delta 1), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J.git\n",
            "   e73aa99..6afd312  main -> main\n",
            "‚úÖ ¬°√âxito! Archivos subidos a GitHub.\n",
            "/content\n",
            "‚úÖ Proceso de extracci√≥n y carga finalizado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 6. CARGA Y REVISI√ìN DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "file_path = '/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/datos_github.csv'\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "print(data.count())\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "uLgbpweddIYn",
        "outputId": "9d2cfc49-a2ec-4ae3-8cac-366785792d60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tipo           1494\n",
            "numero         1494\n",
            "titulo         1494\n",
            "descripcion    1454\n",
            "comentarios    1451\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              tipo  numero                                             titulo  \\\n",
              "0            Issue    4793  MOD347 : Error E010330 Caracteres no v√°lidos '...   \n",
              "1            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "2            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "3            Issue    4760                 Actualizar modelo 190 en rama 16.0   \n",
              "4            Issue    4743  [IMP][16.0] l10n_es_igic IGIC: Nuevo Tipo (1%)...   \n",
              "...            ...     ...                                                ...   \n",
              "1489  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1490  Pull Request    4576  [18.0][FIX] l10n_es_aeat: Find properly the XM...   \n",
              "1491  Pull Request    4575  [18.0][FIX] l10n_es_payment_order_confirming_a...   \n",
              "1492  Pull Request    4575  [18.0][FIX] l10n_es_payment_order_confirming_a...   \n",
              "1493  Pull Request    4575  [18.0][FIX] l10n_es_payment_order_confirming_a...   \n",
              "\n",
              "                                            descripcion  \\\n",
              "0     module: l10n_es_aeat_mod347\\nversion: 18.0\\n\\n...   \n",
              "1     Actualmente, al generar el fichero del modelo ...   \n",
              "2     Actualmente, al generar el fichero del modelo ...   \n",
              "3     Actualmente, al generar el fichero del modelo ...   \n",
              "4     Seg√∫n https://www.gobiernodecanarias.org/boc/2...   \n",
              "...                                                 ...   \n",
              "1489  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1490  Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...   \n",
              "1491  Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...   \n",
              "1492  Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...   \n",
              "1493  Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...   \n",
              "\n",
              "                                            comentarios  \n",
              "0                              Siendo tratado en #4784   \n",
              "1     Adem√°s, hemos podido comprobar que el mismo pr...  \n",
              "2     S√≠, desde luego la soluci√≥n pasa por llevar es...  \n",
              "3     @pedrobaeza Gracias por la aclaraci√≥n.\\nDe acu...  \n",
              "4                                                   NaN  \n",
              "...                                                 ...  \n",
              "1489  On my way to merge this fine PR!\\nPrepared bra...  \n",
              "1490  Congratulations, your PR was merged at 9f6c6d1...  \n",
              "1491                                /ocabot merge patch  \n",
              "1492  This PR looks fantastic, let's merge it!\\nPrep...  \n",
              "1493  Congratulations, your PR was merged at 704c5b1...  \n",
              "\n",
              "[1494 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7659bea-7043-456c-bebe-60e5eeb7db42\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tipo</th>\n",
              "      <th>numero</th>\n",
              "      <th>titulo</th>\n",
              "      <th>descripcion</th>\n",
              "      <th>comentarios</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4793</td>\n",
              "      <td>MOD347 : Error E010330 Caracteres no v√°lidos '...</td>\n",
              "      <td>module: l10n_es_aeat_mod347\\nversion: 18.0\\n\\n...</td>\n",
              "      <td>Siendo tratado en #4784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>Adem√°s, hemos podido comprobar que el mismo pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>S√≠, desde luego la soluci√≥n pasa por llevar es...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4760</td>\n",
              "      <td>Actualizar modelo 190 en rama 16.0</td>\n",
              "      <td>Actualmente, al generar el fichero del modelo ...</td>\n",
              "      <td>@pedrobaeza Gracias por la aclaraci√≥n.\\nDe acu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Issue</td>\n",
              "      <td>4743</td>\n",
              "      <td>[IMP][16.0] l10n_es_igic IGIC: Nuevo Tipo (1%)...</td>\n",
              "      <td>Seg√∫n https://www.gobiernodecanarias.org/boc/2...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>On my way to merge this fine PR!\\nPrepared bra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4576</td>\n",
              "      <td>[18.0][FIX] l10n_es_aeat: Find properly the XM...</td>\n",
              "      <td>Forward-port of #4573 \\r\\n\\r\\nThe tax groups a...</td>\n",
              "      <td>Congratulations, your PR was merged at 9f6c6d1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4575</td>\n",
              "      <td>[18.0][FIX] l10n_es_payment_order_confirming_a...</td>\n",
              "      <td>Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...</td>\n",
              "      <td>/ocabot merge patch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4575</td>\n",
              "      <td>[18.0][FIX] l10n_es_payment_order_confirming_a...</td>\n",
              "      <td>Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...</td>\n",
              "      <td>This PR looks fantastic, let's merge it!\\nPrep...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>Pull Request</td>\n",
              "      <td>4575</td>\n",
              "      <td>[18.0][FIX] l10n_es_payment_order_confirming_a...</td>\n",
              "      <td>Forward-port of #4571, #4572, #4574\\r\\n\\r\\nIf ...</td>\n",
              "      <td>Congratulations, your PR was merged at 704c5b1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1494 rows √ó 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7659bea-7043-456c-bebe-60e5eeb7db42')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7659bea-7043-456c-bebe-60e5eeb7db42 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7659bea-7043-456c-bebe-60e5eeb7db42');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_20feaa9a-2396-49ae-93c3-0180adb4f9ea\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_20feaa9a-2396-49ae-93c3-0180adb4f9ea button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1494,\n  \"fields\": [\n    {\n      \"column\": \"tipo\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Pull Request\",\n          \"Issue\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"numero\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 426,\n        \"min\": 3448,\n        \"max\": 4809,\n        \"num_unique_values\": 400,\n        \"samples\": [\n          4800,\n          4726\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"titulo\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 399,\n        \"samples\": [\n          \"Nuevo m\\u00f3dulo l10n_es_ticketbai_zuzendu para subsanaci\\u00f3n de facturas ticketbai\",\n          \"[19.0][MIG] : l10n_es_account_banking_sepa_fsdd Migration to 19.0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"descripcion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 383,\n        \"samples\": [\n          \"The lines with type \\\"Num\\\" have not sign according to https://sede.agenciatributaria.gob.es/static_files/Sede/Disenyo_registro/DR_200_299/archivos_24/216e2024.xlsx\\r\\n\\r\\n<img width=\\\"981\\\" height=\\\"37\\\" alt=\\\"image\\\" src=\\\"https://github.com/user-attachments/assets/c5547dff-8faf-4b1d-82e0-5e8cb9042870\\\" />\\r\\n\\r\\nTodos son de tipo \\\"Num\\\" por lo que no deber\\u00eda aplicarse signo\\r\\n\\r\\n<img width=\\\"957\\\" height=\\\"594\\\" alt=\\\"image\\\" src=\\\"https://github.com/user-attachments/assets/20988eed-98ae-484e-8ef8-a89ed0ff54c4\\\" />\\r\\n\\r\\nAdem\\u00e1s, hab\\u00eda un fallo en el final de la primera p\\u00e1gina.\\r\\n\\r\\nSubido archivo al pre-portal de AEAT, antes daba este error \\r\\n<img width=\\\"1868\\\" height=\\\"462\\\" alt=\\\"image\\\" src=\\\"https://github.com/user-attachments/assets/8a518867-87b2-4ee4-97a9-577020c61731\\\" />\\r\\n\\r\\nY ahora se valida sin ning\\u00fan error.\\r\\n\\r\\nMT-13343 @moduon\",\n          \"The module was not working correctly in multi-company environments. The _calc_ingresos_gastos_retenciones method only searched for accounting entries in self.company_id.id, ignoring the allowed_company_ids context.\\r\\n\\r\\nThis caused calculations to return 0.00 when users switched companies in the UI.\\r\\n\\r\\nChanged the domain filter to use 'in' operator with allowed_company_ids from context, falling back to company_id for backward compatibility.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"comentarios\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1260,\n        \"samples\": [\n          \"Cierro porque ya se est\\u00e1 tratando en el PR y esto a\\u00f1ade ruido.\",\n          \"@HaraldPanten this PR has passed functional and technical review. Could you please review and merge it?\\r\\nThanks\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7. PREPROCESADO DE DATOS\n",
        "# ==========================================\n",
        "\n",
        "coleccion = []       # Mantenemos esta lista para el vectorizador (Secci√≥n 8)\n",
        "datos_procesados = [] # Nueva lista para estructurar el CSV extendido\n",
        "c = 0\n",
        "\n",
        "print(\"‚è≥ Iniciando preprocesado y estructuraci√≥n de datos...\")\n",
        "\n",
        "# Usamos iterrows para tener acceso a la fila completa (t√≠tulo, n√∫mero, etc.)\n",
        "for index, row in data.iterrows():\n",
        "    doc = row['comentarios']\n",
        "\n",
        "    # 1. Filtro: Ignorar comentarios del bot\n",
        "    if isinstance(doc, str) and doc.strip().lower().startswith('/ocabot'):\n",
        "        continue\n",
        "\n",
        "    # 2. Ejecuci√≥n del Preprocesado\n",
        "    res = preprocesado(doc)\n",
        "\n",
        "    # 3. Validaci√≥n y Guardado\n",
        "    if isinstance(res, str) and res.strip():\n",
        "        # A) Guardar en colecci√≥n simple (para TF-IDF posterior en el c√≥digo)\n",
        "        coleccion.append(res)\n",
        "        c += 1\n",
        "\n",
        "        # B) Guardar fila completa para el nuevo CSV\n",
        "        datos_procesados.append({\n",
        "            'tipo': row['tipo'],\n",
        "            'numero': row['numero'],\n",
        "            'titulo': row['titulo'],\n",
        "            'descripcion': row['descripcion'],\n",
        "            'comentarios': doc,  # Original\n",
        "            'Comentario Preprocesado': res, # Salida de la funci√≥n\n",
        "            'Vector FT-IDF unigrama': \"\", # Columna placeholder solicitada\n",
        "            'Vector FT-IDF bigrama': \"\", # Columna placeholder solicitada\n",
        "            'Etiqueta: positivo(1) | neutral(0) | negativo(-1)': \"\" # Columna placeholder solicitada\n",
        "        })\n",
        "\n",
        "print(f'‚úÖ Se procesaron y estructuraron: {c} documentos')\n",
        "\n",
        "# --- CREACI√ìN Y SUBIDA DEL NUEVO ARCHIVO CSV ---\n",
        "\n",
        "if datos_procesados:\n",
        "    # Crear DataFrame con los datos procesados\n",
        "    df_pln = pd.DataFrame(datos_procesados)\n",
        "\n",
        "    # Definir nombre del archivo\n",
        "    nombre_csv_pln = 'pln_datos_github.csv'\n",
        "\n",
        "    # Guardar localmente en /content/\n",
        "    df_pln.to_csv(f\"/content/{nombre_csv_pln}\", index=False, encoding='utf-8-sig')\n",
        "    print(f\"üìÑ Archivo '{nombre_csv_pln}' generado exitosamente en local.\")\n",
        "\n",
        "    # Subir a GitHub usando la funci√≥n modular existente\n",
        "    # Nota: Se usa datetime.datetime porque datetime fue importado en la secci√≥n 2\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    subir_archivos_a_github([nombre_csv_pln], f\"Add NLP Dataset (Preprocessed): {timestamp}\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No se generaron datos v√°lidos tras el preprocesado.\")\n",
        "\n",
        "# Validaci√≥n visual (imprime los primeros 5 elementos de la colecci√≥n para verificar)\n",
        "print(\"\\n--- Muestra de los primeros 5 elementos preprocesados ---\")\n",
        "print(coleccion[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VIBF2K4lPxy",
        "outputId": "66689172-90f3-4d7c-9f37-a3594fcf163b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Iniciando preprocesado y estructuraci√≥n de datos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1624110909.py:404: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.datetime.utcnow().isoformat()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Se procesaron y estructuraron: 914 documentos\n",
            "üìÑ Archivo 'pln_datos_github.csv' generado exitosamente en local.\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "üîÑ Sincronizando y subiendo: data/pln_datos_github.csv...\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Everything up-to-date\n",
            "‚úÖ ¬°√âxito! Archivos subidos a GitHub.\n",
            "/content\n",
            "\n",
            "--- Muestra de los primeros 5 elementos preprocesados ---\n",
            "['tratar', 'poder comprobar problema reproducir odoo utilizar m√≥dulo l10n_es_aeat_mod190 rama generar fichero modelo odoo ejercicio sede electr√≥nico aeat devolver error similar relacionado falta campo obligatorio correspondiente prestaci√≥n jubilaci√≥n viudedad pensi√≥n incapacidad percepci√≥n asimilada requerir aeat ejercicio actualmente dicho campo parecer contemplado rama m√≥dulo provocar fichero generado v√°lir importaci√≥n campo incorporar rama considerar backport cambio permitir mantener compatibilidad modelo requisito aeat ejercicio gracias trabajo soporte', 'soluci√≥n pasar cambio ambos rama cuesti√≥n estar interesado financiar hacer esperar hacer funcionar software abierto contribuir voluntariamente ideal contribuir parche hacer versi√≥n posible exigir hacer parche poner versi√≥n intermedio soler pedir llevar versi√≥n superior mantener sincronizado caso rev√©s haber poner versi√≥n disponible estar versi√≥n anterior mayor√≠a trabajo faltar empuj√≥n anim√°is', 'pedrobaeza gracias aclaraci√≥n poner backport cambio rama cubrir requisito ejercicio intentar gracias cara sonreir ligeramente', 'hola agradecer alguien confirmar problema verifactu v17 buscar raz√≥n funcionar correctamente v16 migrar v17 aparecer error encontrar gracias']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 8. VECTORIZACI√ìN TF-IDF\n",
        "# ==========================================\n",
        "\n",
        "# Par√°metros recomendados para repos Git\n",
        "vectorizer_unigramas = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(1,1),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "    min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "    max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    norm=\"l2\"\n",
        ")\n",
        "\n",
        "vectorizer_bigramas = TfidfVectorizer(\n",
        "    analyzer=\"word\",\n",
        "    ngram_range=(2,2),   # unigramas + bigramas; usar (1,1) si solo unigramas\n",
        "    min_df=2,            # ignora t√©rminos que aparecen en menos de 2 documentos\n",
        "    max_df=0.9,          # ignora t√©rminos que aparecen en >90% de docs\n",
        "    use_idf=True,\n",
        "    smooth_idf=True,\n",
        "    norm=\"l2\"\n",
        ")\n",
        "\n",
        "# Fit y transformar la colecci√≥n completa (IDF se calcula aqu√≠)\n",
        "X_tfidf_unigramas = vectorizer_unigramas.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (unigramas)\n",
        "print(\"Matriz TF-IDF_unigramas:\", X_tfidf_unigramas.shape)\n",
        "X_tfidf_bigramas = vectorizer_bigramas.fit_transform(coleccion)  # --- RF‚Äì03 Representaci√≥n del texto --- el modelo TF‚ÄìIDF (bigramas)\n",
        "print(\"Matriz TF-IDF_unigramas:\", X_tfidf_bigramas.shape)\n",
        "\n",
        "# AJUSTE: Guardado directo en /content/ para compatibilidad modular\n",
        "joblib.dump(vectorizer_unigramas, \"/content/tfidf_vectorizer_unigramas.joblib\") # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "# M√°s tarde: vectorizer = joblib.load(\"/content/tfidf_vectorizer_unigramas.joblib\")\n",
        "joblib.dump(vectorizer_bigramas, \"/content/tfidf_vectorizer_bigramas.joblib\") # --- RF‚Äì03 Representaci√≥n del texto --- matrices dispersas adecuadas para tareas de PLN.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMJWRF3PnTCm",
        "outputId": "c1656e72-99c9-4c1c-e51b-c8ba4660e06c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matriz TF-IDF_unigramas: (914, 1541)\n",
            "Matriz TF-IDF_unigramas: (914, 1574)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/tfidf_vectorizer_bigramas.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 9. GUARDADO DE LOGS Y MODELOS (ARTEFACTOS)\n",
        "# ==========================================\n",
        "\n",
        "archivos_a_subir = ['log_file.txt', 'tfidf_vectorizer_unigramas.joblib', 'tfidf_vectorizer_bigramas.joblib']\n",
        "mensaje = \"Guardado de log y vectorizadores joblib\"\n",
        "\n",
        "subir_archivos_a_github(archivos_a_subir, mensaje)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-KXEW4jntLk",
        "outputId": "c245e7f7-0c4f-4f89-f695-e0b580a060f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "[main 46388c6] Guardado de log y vectorizadores joblib\n",
            " 1 file changed, 1737 insertions(+), 1737 deletions(-)\n",
            "üîÑ Sincronizando y subiendo: data/log_file.txt, data/tfidf_vectorizer_unigramas.joblib, data/tfidf_vectorizer_bigramas.joblib...\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Current branch main is up to date.\n",
            "Enumerating objects: 7, done.\n",
            "Counting objects: 100% (7/7), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (4/4), done.\n",
            "Writing objects: 100% (4/4), 26.11 KiB | 810.00 KiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J.git\n",
            "   139230b..46388c6  main -> main\n",
            "‚úÖ ¬°√âxito! Archivos subidos a GitHub.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 10. VECTORIZACI√ìN DE LA COLECCI√ìN\n",
        "# ==========================================\n",
        "\n",
        "try:\n",
        "    print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "    # 1. Sincronizar repositorio local\n",
        "    inicializar_repositorio()\n",
        "\n",
        "    # Definimos la ruta del repositorio como base √∫nica\n",
        "    ruta_repo_local = f\"/content/{DEST_REPO_NAME}\"\n",
        "    nombre_csv_pln = 'pln_datos_github.csv'\n",
        "\n",
        "    # RUTA √öNICA: Todo lo que sea para GitHub debe ir aqu√≠\n",
        "    ruta_final_csv = os.path.join(ruta_repo_local, nombre_csv_pln)\n",
        "\n",
        "    # 2. Cargar el CSV (Prioridad: Versi√≥n de GitHub para no perder datos previos)\n",
        "    if os.path.exists(ruta_final_csv):\n",
        "        df_pln = pd.read_csv(ruta_final_csv)\n",
        "        print(\"   ...Cargando versi√≥n desde el repositorio local.\")\n",
        "    elif os.path.exists(f\"/content/{nombre_csv_pln}\"):\n",
        "        df_pln = pd.read_csv(f\"/content/{nombre_csv_pln}\")\n",
        "        print(\"   ...Cargando versi√≥n desde ra√≠z (migrando al repo).\")\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"No se encontr√≥ el archivo '{nombre_csv_pln}' en ninguna ubicaci√≥n.\")\n",
        "\n",
        "    # --- L√ìGICA DE EXTRACCI√ìN DE PALABRAS Y PESOS ---\n",
        "\n",
        "    vocab_uni = vectorizer_unigramas.get_feature_names_out()\n",
        "    vocab_bi = vectorizer_bigramas.get_feature_names_out()\n",
        "\n",
        "    def matriz_a_diccionario(matriz_fila, vocabulario):\n",
        "        \"\"\"Convierte una fila de matriz TF-IDF en un diccionario {palabra: peso} limpio\"\"\"\n",
        "        indices_no_cero = matriz_fila.nonzero()[1]\n",
        "        return {vocabulario[i]: round(float(matriz_fila[0, i]), 4) for i in indices_no_cero}\n",
        "\n",
        "    print(\"   ...Transformando matrices a formato {palabra: peso}\")\n",
        "\n",
        "    dict_unigramas = []\n",
        "    dict_bigramas = []\n",
        "\n",
        "    for i in range(len(coleccion)):\n",
        "        d_uni = matriz_a_diccionario(X_tfidf_unigramas[i], vocab_uni)\n",
        "        d_bi = matriz_a_diccionario(X_tfidf_bigramas[i], vocab_bi)\n",
        "\n",
        "        dict_unigramas.append(str(d_uni))\n",
        "        dict_bigramas.append(str(d_bi))\n",
        "\n",
        "    # 4. Actualizaci√≥n de columnas\n",
        "    df_pln['Vector FT-IDF unigrama'] = dict_unigramas\n",
        "    df_pln['Vector FT-IDF bigrama'] = dict_bigramas\n",
        "\n",
        "    # 5. GUARDADO Y SUBIDA (SOLUCI√ìN AL DUPLICADO)\n",
        "    # Guardamos EXCLUSIVAMENTE en la ruta del repositorio\n",
        "    df_pln.to_csv(ruta_final_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    # Definimos los archivos usando sus rutas completas dentro del repo\n",
        "    # Esto asegura que la funci√≥n de subida sepa exactamente qu√© archivos enviar\n",
        "    archivos_a_subir = [\n",
        "        ruta_final_csv,\n",
        "        os.path.join(ruta_repo_local, 'tfidf_vectorizer_unigramas.joblib'),\n",
        "        os.path.join(ruta_repo_local, 'tfidf_vectorizer_bigramas.joblib')\n",
        "    ]\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    mensaje = f\"NLP Update: Vectores legibles {timestamp}\"\n",
        "\n",
        "    # Llamada a la funci√≥n de subida\n",
        "    subir_archivos_a_github(archivos_a_subir, mensaje)\n",
        "\n",
        "    print(f\"‚úÖ Proceso completado. Archivo guardado en: {ruta_final_csv}\")\n",
        "    print(\"üöÄ Cambios enviados a GitHub sin duplicados en la ra√≠z.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en el M√≥dulo 10: {e}\")"
      ],
      "metadata": {
        "id": "tmMvffauWxTU",
        "outputId": "defa38e9-5fb4-4321-d994-1f0bd4970fa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Preparando persistencia en el repositorio: U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "   ...Cargando versi√≥n desde ra√≠z (migrando al repo).\n",
            "   ...Transformando matrices a formato {palabra: peso}\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/pln_datos_github.csv\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/tfidf_vectorizer_unigramas.joblib\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/tfidf_vectorizer_bigramas.joblib\n",
            "‚ùå Error: No hay archivos v√°lidos para subir.\n",
            "‚úÖ Proceso completado. Archivo guardado en: /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/pln_datos_github.csv\n",
            "üöÄ Cambios enviados a GitHub sin duplicados en la ra√≠z.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Funcional pero con error no ejecutar\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "# 10. VECTORIZACI√ìN DE LA COLECCI√ìN\n",
        "\n",
        "# ==========================================\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "\n",
        "    print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "\n",
        "\n",
        "    # 1. Sincronizar repositorio local\n",
        "\n",
        "    inicializar_repositorio()\n",
        "\n",
        "\n",
        "\n",
        "    ruta_repo_local = f\"/content/{DEST_REPO_NAME}\"\n",
        "\n",
        "    nombre_csv_pln = 'pln_datos_github.csv'\n",
        "\n",
        "    ruta_final_csv = os.path.join(ruta_repo_local, nombre_csv_pln)\n",
        "\n",
        "\n",
        "\n",
        "    # 2. Cargar el CSV (Prioridad: Versi√≥n de GitHub)\n",
        "\n",
        "    if os.path.exists(ruta_final_csv):\n",
        "\n",
        "        df_pln = pd.read_csv(ruta_final_csv)\n",
        "\n",
        "    elif os.path.exists(f\"/content/{nombre_csv_pln}\"):\n",
        "\n",
        "        df_pln = pd.read_csv(f\"/content/{nombre_csv_pln}\")\n",
        "\n",
        "    else:\n",
        "\n",
        "        raise FileNotFoundError(\"No se encontr√≥ el archivo base 'pln_datos_github.csv'\")\n",
        "\n",
        "\n",
        "\n",
        "    # --- L√ìGICA DE EXTRACCI√ìN DE PALABRAS Y PESOS ---\n",
        "\n",
        "\n",
        "\n",
        "    # Obtenemos los nombres de las palabras (features) de los vectorizadores del M√≥dulo 8\n",
        "\n",
        "    vocab_uni = vectorizer_unigramas.get_feature_names_out()\n",
        "\n",
        "    vocab_bi = vectorizer_bigramas.get_feature_names_out()\n",
        "\n",
        "\n",
        "\n",
        "    def matriz_a_diccionario(matriz_fila, vocabulario):\n",
        "\n",
        "        \"\"\"Convierte una fila de matriz TF-IDF en un diccionario {palabra: peso} limpio\"\"\"\n",
        "\n",
        "        # Obtenemos los √≠ndices de las columnas que NO son cero\n",
        "\n",
        "        indices_no_cero = matriz_fila.nonzero()[1]\n",
        "\n",
        "\n",
        "\n",
        "        # Aplicamos float() al peso para eliminar el rastro de np.float64\n",
        "\n",
        "        return {vocabulario[i]: round(float(matriz_fila[0, i]), 4) for i in indices_no_cero}\n",
        "\n",
        "\n",
        "\n",
        "    print(\"   ...Transformando matrices a formato {palabra: peso}\")\n",
        "\n",
        "\n",
        "\n",
        "    dict_unigramas = []\n",
        "\n",
        "    dict_bigramas = []\n",
        "\n",
        "\n",
        "\n",
        "    # Iteramos sobre cada documento procesado en la Secci√≥n 8\n",
        "\n",
        "    for i in range(len(coleccion)):\n",
        "\n",
        "        # Procesar fila i de unigramas y bigramas\n",
        "\n",
        "        d_uni = matriz_a_diccionario(X_tfidf_unigramas[i], vocab_uni)\n",
        "\n",
        "        d_bi = matriz_a_diccionario(X_tfidf_bigramas[i], vocab_bi)\n",
        "\n",
        "\n",
        "\n",
        "        dict_unigramas.append(str(d_uni)) # Guardamos como string para el CSV\n",
        "\n",
        "        dict_bigramas.append(str(d_bi))\n",
        "\n",
        "\n",
        "\n",
        "    # 4. Actualizaci√≥n de columnas con el nuevo formato\n",
        "\n",
        "    df_pln['Vector FT-IDF unigrama'] = dict_unigramas\n",
        "\n",
        "    df_pln['Vector FT-IDF bigrama'] = dict_bigramas\n",
        "\n",
        "\n",
        "\n",
        "    # 5. Guardado y Subida (Sin tocar tu l√≥gica funcional de guardado)\n",
        "\n",
        "    df_pln.to_csv(f\"/content/{nombre_csv_pln}\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "    df_pln.to_csv(ruta_final_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "\n",
        "    archivos_a_subir = [nombre_csv_pln, 'tfidf_vectorizer_unigramas.joblib', 'tfidf_vectorizer_bigramas.joblib']\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "\n",
        "    mensaje = f\"NLP Update: Vectores legibles {timestamp}\"\n",
        "\n",
        "\n",
        "\n",
        "    subir_archivos_a_github(archivos_a_subir, mensaje)\n",
        "\n",
        "    print(\"‚úÖ Proceso completado. Los vectores ahora son legibles: {'palabra': peso}\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "\n",
        "    print(f\"‚ùå Error en el M√≥dulo 10: {e}\")"
      ],
      "metadata": {
        "id": "7CmfCO5bly-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Presenta error.\n",
        "\n",
        "# ==========================================\n",
        "# 10. VECTORIZACI√ìN DE LA COLECCI√ìN\n",
        "# ==========================================\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "try:\n",
        "    print(f\"üìÇ Preparando persistencia en el repositorio: {DEST_REPO_NAME}\")\n",
        "\n",
        "    # 1. Sincronizar repositorio local\n",
        "    inicializar_repositorio()\n",
        "\n",
        "    ruta_repo_local = f\"/content/{DEST_REPO_NAME}\"\n",
        "    ruta_data = os.path.join(ruta_repo_local, \"data\")\n",
        "    nombre_csv_pln = 'pln_datos_github.csv'\n",
        "    ruta_final_csv = os.path.join(ruta_data, nombre_csv_pln)\n",
        "\n",
        "    # Asegurar que la carpeta 'data' exista\n",
        "    os.makedirs(ruta_data, exist_ok=True)\n",
        "\n",
        "    # 2. Cargar el CSV (Prioridad: Versi√≥n de GitHub)\n",
        "    if os.path.exists(ruta_final_csv):\n",
        "        df_pln = pd.read_csv(ruta_final_csv)\n",
        "        print(\"   ...Cargando versi√≥n desde carpeta 'data' del repo.\")\n",
        "    elif os.path.exists(f\"/content/{nombre_csv_pln}\"):\n",
        "        df_pln = pd.read_csv(f\"/content/{nombre_csv_pln}\")\n",
        "        print(\"   ...Cargando versi√≥n desde ra√≠z.\")\n",
        "    else:\n",
        "        print(\"   ...Usando DataFrame actual en memoria.\")\n",
        "\n",
        "    # --- L√ìGICA DE EXTRACCI√ìN DE PALABRAS Y PESOS ---\n",
        "    vocab_uni = vectorizer_unigramas.get_feature_names_out()\n",
        "    vocab_bi = vectorizer_bigramas.get_feature_names_out()\n",
        "\n",
        "    def matriz_a_diccionario(matriz_fila, vocabulario):\n",
        "        indices_no_cero = matriz_fila.nonzero()[1]\n",
        "        return {vocabulario[i]: round(float(matriz_fila[0, i]), 4) for i in indices_no_cero}\n",
        "\n",
        "    print(\"   ...Transformando matrices a formato {palabra: peso}\")\n",
        "    df_pln['Vector FT-IDF unigrama'] = [str(matriz_a_diccionario(X_tfidf_unigramas[i], vocab_uni)) for i in range(len(coleccion))]\n",
        "    df_pln['Vector FT-IDF bigrama'] = [str(matriz_a_diccionario(X_tfidf_bigramas[i], vocab_bi)) for i in range(len(coleccion))]\n",
        "\n",
        "    # 3. GUARDADO F√çSICO Y COPIADO\n",
        "    df_pln.to_csv(ruta_final_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "    modelos = ['tfidf_vectorizer_unigramas.joblib', 'tfidf_vectorizer_bigramas.joblib']\n",
        "    for modelo in modelos:\n",
        "        ruta_origen = f\"/content/{modelo}\"\n",
        "        ruta_destino = os.path.join(ruta_data, modelo)\n",
        "        if os.path.exists(ruta_origen):\n",
        "            shutil.copy2(ruta_origen, ruta_destino) # copy2 preserva metadatos\n",
        "            print(f\"   ...Modelo {modelo} copiado a carpeta data.\")\n",
        "\n",
        "    # 4. SINCRONIZACI√ìN DEL SISTEMA DE ARCHIVOS\n",
        "    # Peque√±a pausa para asegurar que el SO confirme la escritura de archivos\n",
        "    time.sleep(1)\n",
        "\n",
        "    # 5. SUBIDA A GITHUB\n",
        "    # IMPORTANTE: Enviamos las rutas RELATIVAS a la carpeta 'data'\n",
        "    # Si tu funci√≥n 'subir_archivos_a_github' hace un 'git add .', esto funcionar√° mejor.\n",
        "    archivos_a_subir = [\n",
        "        f\"data/{nombre_csv_pln}\",\n",
        "        f\"data/tfidf_vectorizer_unigramas.joblib\",\n",
        "        f\"data/tfidf_vectorizer_bigramas.joblib\"\n",
        "    ]\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    mensaje = f\"NLP Update: Datos en carpeta data {timestamp}\"\n",
        "\n",
        "    print(\"   ...Iniciando proceso de subida a GitHub.\")\n",
        "    subir_archivos_a_github(archivos_a_subir, mensaje)\n",
        "\n",
        "    print(f\"‚úÖ Proceso completado exitosamente.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en el M√≥dulo 10: {e}\")"
      ],
      "metadata": {
        "id": "TrWG8Rhnih7G",
        "outputId": "a2e9a37b-f020-4505-b087-781164435302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Preparando persistencia en el repositorio: U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "/content\n",
            "üîÑ Sincronizando repositorio...\n",
            "/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            " * branch            main       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "/content\n",
            "   ...Cargando versi√≥n desde carpeta 'data' del repo.\n",
            "   ...Transformando matrices a formato {palabra: peso}\n",
            "   ...Modelo tfidf_vectorizer_unigramas.joblib copiado a carpeta data.\n",
            "   ...Modelo tfidf_vectorizer_bigramas.joblib copiado a carpeta data.\n",
            "   ...Iniciando proceso de subida a GitHub.\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ data/pln_datos_github.csv\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ data/tfidf_vectorizer_unigramas.joblib\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ data/tfidf_vectorizer_bigramas.joblib\n",
            "‚ùå Error: No hay archivos v√°lidos para subir.\n",
            "‚úÖ Proceso completado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================\n",
        "# 11. ETIQUETADO DE PARA AN√ÅLISIS DE SENTIMIENTOS\n",
        "# ================================================\n",
        "\n",
        "# 1. Cargar el archivo como pln_coleccion\n",
        "pln_file_path = '/content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/pln_datos_github.csv'\n",
        "pln_coleccion = pd.read_csv(pln_file_path)\n",
        "\n",
        "col_comentario = 'Comentario Preprocesado'\n",
        "col_etiqueta = 'Etiqueta: positivo(1) | neutral(0) | negativo(-1)'\n",
        "\n",
        "# Limpieza t√©cnica: Asegurar que los vac√≠os sean reconocidos como NaN\n",
        "# Esto soluciona el problema de si la primera fila tiene espacios invisibles\n",
        "pln_coleccion[col_etiqueta] = pd.to_numeric(pln_coleccion[col_etiqueta], errors='coerce')\n",
        "\n",
        "# 2. Verificar pendientes\n",
        "pendientes_mask = pln_coleccion[col_etiqueta].isna()\n",
        "if not pendientes_mask.any():\n",
        "    print(\"================================\")\n",
        "    print(\"      Etiquetado Completado     \")\n",
        "    print(\"================================\")\n",
        "else:\n",
        "    print(f\"Iniciando... Filas pendientes: {pendientes_mask.sum()}\\n\")\n",
        "\n",
        "    # 3. Bucle para recorrer la colecci√≥n\n",
        "    for index, row in pln_coleccion.iterrows():\n",
        "\n",
        "        # Validaci√≥n robusta: verifica si es NaN real\n",
        "        if pd.isna(row[col_etiqueta]):\n",
        "            pln_doc = row[col_comentario]\n",
        "\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"ID Fila: {index}\") # Esto te confirmar√° si empieza en 0 o 1\n",
        "            print(f\"Comentario actual: {pln_doc}\")\n",
        "\n",
        "            continuar_proceso = True\n",
        "            while True:\n",
        "                entrada = input(\"Ingrese Etiqueta (1, 0, -1) o '#' para salir: \").strip()\n",
        "\n",
        "                if entrada == '#':\n",
        "                    continuar_proceso = False\n",
        "                    break\n",
        "\n",
        "                if entrada in ['1', '0', '-1']:\n",
        "                    etiqueta_pln_doc = int(entrada)\n",
        "\n",
        "                    # --- GUARDADO DENTRO DEL WHILE ---\n",
        "                    pln_coleccion.at[index, col_etiqueta] = etiqueta_pln_doc\n",
        "                    pln_coleccion.to_csv(pln_file_path, index=False)\n",
        "\n",
        "                    archivos_a_subir = [pln_file_path]\n",
        "                    mensaje_github = f\"Etiquetado manual fila {index}: valor {etiqueta_pln_doc}\"\n",
        "                    subir_archivos_a_github(archivos_a_subir, mensaje_github)\n",
        "\n",
        "                    print(f\"‚úÖ Fila {index} sincronizada.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"‚ö†Ô∏è Entrada inv√°lida.\")\n",
        "\n",
        "            if not continuar_proceso:\n",
        "                print(\"\\nProceso detenido.\")\n",
        "                break\n",
        "\n",
        "    if pln_coleccion[col_etiqueta].isna().sum() == 0:\n",
        "        print(\"\\n================================\")\n",
        "        print(\"      Etiquetado Completado     \")\n",
        "        print(\"================================\")"
      ],
      "metadata": {
        "id": "Xf_BriZISbSb",
        "outputId": "6da4b121-da26-48c6-d2ac-a5c673d3509f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontraron 914 filas pendientes. Iniciando...\n",
            "Presione '#' y Enter para guardar y salir en cualquier momento.\n",
            "\n",
            "\n",
            "[Fila 0]\n",
            "Contenido: tratar\n",
            "Ingrese Etiqueta: positivo(1) | neutral(0) | negativo(-1): 0\n",
            "‚ö†Ô∏è Advertencia: No se encontr√≥ /content/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J/data/pln_datos_github.csv\n",
            "‚ùå Error: No hay archivos v√°lidos para subir.\n",
            "‚úÖ Fila 0 sincronizada en GitHub.\n",
            "\n",
            "[Fila 1]\n",
            "Contenido: poder comprobar problema reproducir odoo utilizar m√≥dulo l10n_es_aeat_mod190 rama generar fichero modelo odoo ejercicio sede electr√≥nico aeat devolver error similar relacionado falta campo obligatorio correspondiente prestaci√≥n jubilaci√≥n viudedad pensi√≥n incapacidad percepci√≥n asimilada requerir aeat ejercicio actualmente dicho campo parecer contemplado rama m√≥dulo provocar fichero generado v√°lir importaci√≥n campo incorporar rama considerar backport cambio permitir mantener compatibilidad modelo requisito aeat ejercicio gracias trabajo soporte\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1121218861.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# 4. Ciclo de validaci√≥n y entrada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mentrada\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ingrese Etiqueta: positivo(1) | neutral(0) | negativo(-1): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mentrada\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'#'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 12. SECCI√ìN DE PRUEBAS\n",
        "# ==========================================\n",
        "\n",
        "\n",
        "# Vector del documento i (sparse row)\n",
        "i = 1\n",
        "vec_i = X_tfidf_unigramas[i]  # scipy.sparse.csr_matrix (1, n_features)\n",
        "\n",
        "# Obtener nombres de features (t√©rminos / n-gramas)\n",
        "features = vectorizer_unigramas.get_feature_names_out()\n",
        "\n",
        "# Para ver los top-k t√©rminos con mayor peso en doc i:\n",
        "k = 10\n",
        "row = vec_i.tocoo()\n",
        "top_idx = np.argsort(row.data)[-k:][::-1]  # √≠ndices en row.data ordenados por peso\n",
        "top_terms = [(features[row.col[idx]], row.data[idx]) for idx in top_idx]\n",
        "print(\"Top terms doc (unigramas)\", i, top_terms)\n",
        "\n",
        "# 1. Definir la ruta del modelo dentro del repositorio clonado\n",
        "# DEST_REPO_NAME viene de tu Celda de Configuraci√≥n Com√∫n\n",
        "ruta_modelo_github = f\"/content/{DEST_REPO_NAME}/data/tfidf_vectorizer_unigramas.joblib\"\n",
        "\n",
        "# 2. Cargar el vectorizador desde GitHub (repo local)\n",
        "try:\n",
        "    if os.path.exists(ruta_modelo_github):\n",
        "        vectorizer = joblib.load(ruta_modelo_github)\n",
        "        print(\"‚úÖ Modelo cargado exitosamente desde GitHub.\")\n",
        "    else:\n",
        "        print(\"‚ùå Error: El modelo no se encuentra en la ruta del repositorio. ¬øYa hiciste el push?\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al cargar el modelo: {e}\")\n",
        "\n",
        "# 3. Uso del modelo cargado\n",
        "nuevo_doc = \"error al ejecutar test de integraci√≥n en CI\"\n",
        "nuevo_pre = preprocesado(nuevo_doc)  # usar tu funci√≥n\n",
        "\n",
        "# Transformar usando el vectorizador cargado\n",
        "vec_nuevo = vectorizer.transform([nuevo_pre])  # no fit, solo transform\n",
        "print(\"Matriz TF-IDF del nuevo doc:\", vec_nuevo.shape)\n",
        "print(vec_nuevo)\n",
        "\n",
        "# Obtener los nombres de las palabras\n",
        "nombres = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Ver qu√© palabras corresponden a los √≠ndices 388 y 635\n",
        "print(f\"Palabra en 388: {nombres[388]}\")\n",
        "print(f\"Palabra en 635: {nombres[635]}\")\n",
        "\n",
        "# 1. Buscar el √≠ndice de la palabra \"error\" en el vocabulario\n",
        "vocabulario = vectorizer.get_feature_names_out()\n",
        "palabra_a_buscar = \"error\"\n",
        "\n",
        "if palabra_a_buscar in vocabulario:\n",
        "    # Obtener el √≠ndice (columna)\n",
        "    indice = list(vocabulario).index(palabra_a_buscar)\n",
        "    print(f\"‚úÖ La palabra '{palabra_a_buscar}' est√° en el vocabulario en el √≠ndice: {indice}\")\n",
        "\n",
        "    # 2. Ver el peso de esa palabra en el 'nuevo_doc' que transformaste antes\n",
        "    # vec_nuevo es la matriz (1, 1238) que obtuviste en el paso anterior\n",
        "    peso_tfidf = vec_nuevo[0, indice]\n",
        "\n",
        "    if peso_tfidf > 0:\n",
        "        print(f\"üìä Peso TF-IDF de '{palabra_a_buscar}' en el nuevo doc: {peso_tfidf:.4f}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è La palabra existe en el modelo, pero no tiene peso en este documento espec√≠fico (quiz√°s fue filtrada o no estaba en el string original).\")\n",
        "else:\n",
        "    print(f\"‚ùå La palabra '{palabra_a_buscar}' NO existe en el vocabulario del modelo.\")"
      ],
      "metadata": {
        "id": "Hycfq1gzx1i-",
        "outputId": "1ab0321c-3fc2-4f97-987d-7d8bc3430bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top terms doc (unigramas) 1 [('ejercicio', np.float64(0.3760118860761082)), ('rama', np.float64(0.36829554210209065)), ('aeat', np.float64(0.2643283775347077)), ('campo', np.float64(0.24623013387705966)), ('fichero', np.float64(0.2177532537713971)), ('modelo', np.float64(0.18498330508422126)), ('odoo', np.float64(0.1474602982086119)), ('prestaci√≥n', np.float64(0.14047405943799587)), ('sede', np.float64(0.14047405943799587)), ('generado', np.float64(0.13560110981518958))]\n",
            "‚úÖ Modelo cargado exitosamente desde GitHub.\n",
            "Matriz TF-IDF del nuevo doc: (1, 1541)\n",
            "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
            "\twith 5 stored elements and shape (1, 1541)>\n",
            "  Coords\tValues\n",
            "  (0, 215)\t0.4877058348683418\n",
            "  (0, 479)\t0.46064560040232533\n",
            "  (0, 526)\t0.28017024345813835\n",
            "  (0, 782)\t0.5710506569384148\n",
            "  (0, 1417)\t0.38125376241575293\n",
            "Palabra en 388: dec√≠s\n",
            "Palabra en 635: flexibilidad\n",
            "‚úÖ La palabra 'error' est√° en el vocabulario en el √≠ndice: 526\n",
            "üìä Peso TF-IDF de 'error' en el nuevo doc: 0.2802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1624110909.py:404: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  timestamp = datetime.datetime.utcnow().isoformat()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ==========================================\n",
        "# 13. REINICIAR CONEXI√ìN GTIHUB\n",
        "# ==========================================\n",
        "\n",
        "# Configurar la estrategia de reconciliaci√≥n\n",
        "!git -C /content/{DEST_REPO_NAME} config pull.rebase false\n",
        "\n",
        "# Forzar una limpieza y descarga de la versi√≥n actual de GitHub\n",
        "try:\n",
        "    print(\"üîÑ Sincronizando repositorio...\")\n",
        "    !git -C /content/{DEST_REPO_NAME} fetch origin\n",
        "    !git -C /content/{DEST_REPO_NAME} reset --hard origin/main # O 'master' seg√∫n tu rama\n",
        "    print(\"‚úÖ Repositorio sincronizado y limpio.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al sincronizar: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjCMjl3oZxgm",
        "outputId": "a918d38f-de12-4acb-dfce-1eafa6d620b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Sincronizando repositorio...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (4/4), 1.67 KiB | 214.00 KiB/s, done.\n",
            "From https://github.com/jpmachinelearning/U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J\n",
            "   1156f44..a14135f  main       -> origin/main\n",
            "HEAD is now at a14135f Created using Colab\n",
            "‚úÖ Repositorio sincronizado y limpio.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# 1. Borrar la carpeta local que da problemas\n",
        "if os.path.exists(f\"/content/{DEST_REPO_NAME}\"):\n",
        "    shutil.rmtree(f\"/content/{DEST_REPO_NAME}\")\n",
        "    print(\"üßπ Carpeta local eliminada para evitar conflictos.\")\n",
        "\n",
        "# 2. Volver a inicializar (esto clonar√° el repo limpio de GitHub)\n",
        "inicializar_repositorio()\n",
        "\n",
        "# 3. MUY IMPORTANTE: Aseg√∫rate de guardar los modelos de nuevo\n",
        "# Si no tienes los archivos .joblib en /content, el error seguir√°.\n",
        "# Ejemplo manual (repite esto con tus modelos reales si es necesario):\n",
        "# joblib.dump(vectorizer_unigramas, f\"/content/{DEST_REPO_NAME}/tfidf_vectorizer_unigramas.joblib\")"
      ],
      "metadata": {
        "id": "CGEv8yDpgwJ9",
        "outputId": "4d03ba7c-d6c8-46c2-89fa-892fa8328d9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Carpeta local eliminada para evitar conflictos.\n",
            "/content\n",
            "üöÄ Clonando U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J...\n",
            "Cloning into 'U4_S16_Trabajo_Final_2P_PLAZA_A-PADILLA_J'...\n",
            "remote: Enumerating objects: 206, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (11/11), done.\u001b[K\n",
            "remote: Total 206 (delta 1), reused 3 (delta 0), pack-reused 194 (from 1)\u001b[K\n",
            "Receiving objects: 100% (206/206), 5.51 MiB | 10.18 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ]
        }
      ]
    }
  ]
}